{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "selloutData = pd.read_csv(\"../dataset/kopo_decision_tree_all_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REGIONID</th>\n",
       "      <th>PRODUCTGROUP</th>\n",
       "      <th>PRODUCT</th>\n",
       "      <th>ITEM</th>\n",
       "      <th>YEARWEEK</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>WEEK</th>\n",
       "      <th>QTY</th>\n",
       "      <th>HOLIDAY</th>\n",
       "      <th>HCLUS</th>\n",
       "      <th>PROMOTION</th>\n",
       "      <th>PRO_PERCENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG01</td>\n",
       "      <td>P01</td>\n",
       "      <td>ITEM001</td>\n",
       "      <td>201538</td>\n",
       "      <td>2015</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG01</td>\n",
       "      <td>P01</td>\n",
       "      <td>ITEM001</td>\n",
       "      <td>201548</td>\n",
       "      <td>2015</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG01</td>\n",
       "      <td>P01</td>\n",
       "      <td>ITEM001</td>\n",
       "      <td>201549</td>\n",
       "      <td>2015</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG01</td>\n",
       "      <td>P01</td>\n",
       "      <td>ITEM002</td>\n",
       "      <td>201526</td>\n",
       "      <td>2015</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG01</td>\n",
       "      <td>P01</td>\n",
       "      <td>ITEM002</td>\n",
       "      <td>201532</td>\n",
       "      <td>2015</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  REGIONID PRODUCTGROUP PRODUCT     ITEM  YEARWEEK  YEAR  WEEK  QTY HOLIDAY  \\\n",
       "0      A01         PG01     P01  ITEM001    201538  2015    38    1       N   \n",
       "1      A01         PG01     P01  ITEM001    201548  2015    48    1       Y   \n",
       "2      A01         PG01     P01  ITEM001    201549  2015    49    2       Y   \n",
       "3      A01         PG01     P01  ITEM002    201526  2015    26    1       Y   \n",
       "4      A01         PG01     P01  ITEM002    201532  2015    32    1       N   \n",
       "\n",
       "   HCLUS PROMOTION  PRO_PERCENT  \n",
       "0      4         N          0.0  \n",
       "1      0         N          0.0  \n",
       "2      0         N          0.0  \n",
       "3      1         N          0.0  \n",
       "4      4         N          0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selloutData.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 그룹단위(item) 단위로 데이터 사이즈 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupKey = [\"REGIONID\",\"PRODUCTGROUP\",\"PRODUCT\",\"ITEM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ITEM</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ITEM001</th>\n",
       "      <td>201545.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mean\n",
       "ITEM             \n",
       "ITEM001  201545.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selloutData.groupby(groupKey)[\"YEARWEEK\"].agg([\"size\"])\n",
    "selloutData.groupby(groupKey)[\"YEARWEEK\"].agg([\"mean\"]).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupData = selloutData.groupby(groupKey)[\"YEARWEEK\"].agg([\"size\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupData.rename(columns={\"size\":\"KNOB\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(491, 5)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32415, 12)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selloutData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.원본 데이터와 그룹핑데이터 머지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedData = pd.merge(left=selloutData,\n",
    "         right=groupData,\n",
    "         how=\"left\",\n",
    "         on=groupKey\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32415, 13)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergedData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxKnob = mergedData.KNOB.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 데이터 정제 시작(과거실적 최대치만 남김)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleansedData = mergedData[mergedData.KNOB >=  maxKnob ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7592, 13)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleansedData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleansedData.ITEM.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REGIONID         object\n",
       "PRODUCTGROUP     object\n",
       "PRODUCT          object\n",
       "ITEM             object\n",
       "YEARWEEK          int64\n",
       "YEAR              int64\n",
       "WEEK              int64\n",
       "QTY               int64\n",
       "HOLIDAY          object\n",
       "HCLUS             int64\n",
       "PROMOTION        object\n",
       "PRO_PERCENT     float64\n",
       "KNOB              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleansedData.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_yn = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "cleansedData[\"LE_HOLI\"] = le_yn.fit_transform(cleansedData.HOLIDAY)\n",
    "cleansedData[\"LE_PROMO\"] = le_yn.fit_transform(cleansedData.PROMOTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REGIONID</th>\n",
       "      <th>PRODUCTGROUP</th>\n",
       "      <th>PRODUCT</th>\n",
       "      <th>ITEM</th>\n",
       "      <th>YEARWEEK</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>WEEK</th>\n",
       "      <th>QTY</th>\n",
       "      <th>HOLIDAY</th>\n",
       "      <th>HCLUS</th>\n",
       "      <th>PROMOTION</th>\n",
       "      <th>PRO_PERCENT</th>\n",
       "      <th>KNOB</th>\n",
       "      <th>LE_HOLI</th>\n",
       "      <th>LE_PROMO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG02</td>\n",
       "      <td>P03</td>\n",
       "      <td>ITEM043</td>\n",
       "      <td>201501</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.19759</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG02</td>\n",
       "      <td>P03</td>\n",
       "      <td>ITEM043</td>\n",
       "      <td>201502</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.19759</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG02</td>\n",
       "      <td>P03</td>\n",
       "      <td>ITEM043</td>\n",
       "      <td>201503</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     REGIONID PRODUCTGROUP PRODUCT     ITEM  YEARWEEK  YEAR  WEEK  QTY  \\\n",
       "2352      A01         PG02     P03  ITEM043    201501  2015     1   87   \n",
       "2353      A01         PG02     P03  ITEM043    201502  2015     2   60   \n",
       "2354      A01         PG02     P03  ITEM043    201503  2015     3   51   \n",
       "\n",
       "     HOLIDAY  HCLUS PROMOTION  PRO_PERCENT  KNOB  LE_HOLI  LE_PROMO  \n",
       "2352       Y      1         Y      0.19759   146        1         1  \n",
       "2353       N      4         Y      0.19759   146        0         1  \n",
       "2354       N      4         N      0.00000   146        0         0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleansedData.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7592, 15)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleansedData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1번 풀이 -> groupby 풀이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### item 별로 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictGroup = cleansedData.groupby(groupKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ITEM376'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(predictGroup.groups)[51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "onegroup = predictGroup.get_group(list(predictGroup.groups)[51])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 15)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onegroup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "onegroup = onegroup.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REGIONID</th>\n",
       "      <th>PRODUCTGROUP</th>\n",
       "      <th>PRODUCT</th>\n",
       "      <th>ITEM</th>\n",
       "      <th>YEARWEEK</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>WEEK</th>\n",
       "      <th>QTY</th>\n",
       "      <th>HOLIDAY</th>\n",
       "      <th>HCLUS</th>\n",
       "      <th>PROMOTION</th>\n",
       "      <th>PRO_PERCENT</th>\n",
       "      <th>KNOB</th>\n",
       "      <th>LE_HOLI</th>\n",
       "      <th>LE_PROMO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG05</td>\n",
       "      <td>P15</td>\n",
       "      <td>ITEM376</td>\n",
       "      <td>201501</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.195745</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG05</td>\n",
       "      <td>P15</td>\n",
       "      <td>ITEM376</td>\n",
       "      <td>201502</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>119</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.195745</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  REGIONID PRODUCTGROUP PRODUCT     ITEM  YEARWEEK  YEAR  WEEK  QTY HOLIDAY  \\\n",
       "0      A01         PG05     P15  ITEM376    201501  2015     1  170       Y   \n",
       "1      A01         PG05     P15  ITEM376    201502  2015     2  119       N   \n",
       "\n",
       "   HCLUS PROMOTION  PRO_PERCENT  KNOB  LE_HOLI  LE_PROMO  \n",
       "0      1         Y     0.195745   146        1         1  \n",
       "1      4         Y     0.195745   146        0         1  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onegroup.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2번 풀이->category 풀이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_item = LabelEncoder()\n",
    "le_regionid = LabelEncoder()\n",
    "le_productgroup = LabelEncoder()\n",
    "le_product = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "cleansedData[\"LE_HOLI\"] = le_yn.fit_transform(cleansedData.HOLIDAY)\n",
    "cleansedData[\"LE_PROMO\"] = le_yn.fit_transform(cleansedData.PROMOTION)\n",
    "cleansedData[\"LE_ITEM\"] = le_item.fit_transform(cleansedData.ITEM)\n",
    "cleansedData[\"LE_REGION\"] = le_item.fit_transform(cleansedData.REGIONID)\n",
    "cleansedData[\"LE_PROD_GROUP\"] = le_item.fit_transform(cleansedData.PRODUCTGROUP)\n",
    "cleansedData[\"LE_PROD\"] = le_item.fit_transform(cleansedData.PRODUCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REGIONID</th>\n",
       "      <th>PRODUCTGROUP</th>\n",
       "      <th>PRODUCT</th>\n",
       "      <th>ITEM</th>\n",
       "      <th>YEARWEEK</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>WEEK</th>\n",
       "      <th>QTY</th>\n",
       "      <th>HOLIDAY</th>\n",
       "      <th>HCLUS</th>\n",
       "      <th>PROMOTION</th>\n",
       "      <th>PRO_PERCENT</th>\n",
       "      <th>KNOB</th>\n",
       "      <th>LE_HOLI</th>\n",
       "      <th>LE_PROMO</th>\n",
       "      <th>LE_ITEM</th>\n",
       "      <th>LE_REGION</th>\n",
       "      <th>LE_PROD_GROUP</th>\n",
       "      <th>LE_PROD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG02</td>\n",
       "      <td>P03</td>\n",
       "      <td>ITEM043</td>\n",
       "      <td>201501</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.197590</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG02</td>\n",
       "      <td>P03</td>\n",
       "      <td>ITEM043</td>\n",
       "      <td>201502</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.197590</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG02</td>\n",
       "      <td>P03</td>\n",
       "      <td>ITEM043</td>\n",
       "      <td>201503</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2355</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG02</td>\n",
       "      <td>P03</td>\n",
       "      <td>ITEM043</td>\n",
       "      <td>201504</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>Y</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2356</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG02</td>\n",
       "      <td>P03</td>\n",
       "      <td>ITEM043</td>\n",
       "      <td>201505</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>136</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.201205</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     REGIONID PRODUCTGROUP PRODUCT     ITEM  YEARWEEK  YEAR  WEEK  QTY  \\\n",
       "2352      A01         PG02     P03  ITEM043    201501  2015     1   87   \n",
       "2353      A01         PG02     P03  ITEM043    201502  2015     2   60   \n",
       "2354      A01         PG02     P03  ITEM043    201503  2015     3   51   \n",
       "2355      A01         PG02     P03  ITEM043    201504  2015     4   37   \n",
       "2356      A01         PG02     P03  ITEM043    201505  2015     5  136   \n",
       "\n",
       "     HOLIDAY  HCLUS PROMOTION  PRO_PERCENT  KNOB  LE_HOLI  LE_PROMO  LE_ITEM  \\\n",
       "2352       Y      1         Y     0.197590   146        1         1        0   \n",
       "2353       N      4         Y     0.197590   146        0         1        0   \n",
       "2354       N      4         N     0.000000   146        0         0        0   \n",
       "2355       Y      2         N     0.000000   146        1         0        0   \n",
       "2356       N      4         Y     0.201205   146        0         1        0   \n",
       "\n",
       "      LE_REGION  LE_PROD_GROUP  LE_PROD  \n",
       "2352          0              0        0  \n",
       "2353          0              0        0  \n",
       "2354          0              0        0  \n",
       "2355          0              0        0  \n",
       "2356          0              0        0  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleansedData.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ITEM043', 'ITEM043', 'ITEM043', ..., 'ITEM376', 'ITEM376',\n",
       "       'ITEM376'], dtype=object)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_item.inverse_transform(cleansedData.LE_ITEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REGIONID</th>\n",
       "      <th>PRODUCTGROUP</th>\n",
       "      <th>PRODUCT</th>\n",
       "      <th>ITEM</th>\n",
       "      <th>YEARWEEK</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>WEEK</th>\n",
       "      <th>QTY</th>\n",
       "      <th>HOLIDAY</th>\n",
       "      <th>HCLUS</th>\n",
       "      <th>PROMOTION</th>\n",
       "      <th>PRO_PERCENT</th>\n",
       "      <th>KNOB</th>\n",
       "      <th>LE_HOLI</th>\n",
       "      <th>LE_PROMO</th>\n",
       "      <th>LE_ITEM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG02</td>\n",
       "      <td>P03</td>\n",
       "      <td>ITEM043</td>\n",
       "      <td>201501</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.197590</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG02</td>\n",
       "      <td>P03</td>\n",
       "      <td>ITEM043</td>\n",
       "      <td>201502</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.197590</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG02</td>\n",
       "      <td>P03</td>\n",
       "      <td>ITEM043</td>\n",
       "      <td>201503</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2355</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG02</td>\n",
       "      <td>P03</td>\n",
       "      <td>ITEM043</td>\n",
       "      <td>201504</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>Y</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2356</th>\n",
       "      <td>A01</td>\n",
       "      <td>PG02</td>\n",
       "      <td>P03</td>\n",
       "      <td>ITEM043</td>\n",
       "      <td>201505</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>136</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.201205</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     REGIONID PRODUCTGROUP PRODUCT     ITEM  YEARWEEK  YEAR  WEEK  QTY  \\\n",
       "2352      A01         PG02     P03  ITEM043    201501  2015     1   87   \n",
       "2353      A01         PG02     P03  ITEM043    201502  2015     2   60   \n",
       "2354      A01         PG02     P03  ITEM043    201503  2015     3   51   \n",
       "2355      A01         PG02     P03  ITEM043    201504  2015     4   37   \n",
       "2356      A01         PG02     P03  ITEM043    201505  2015     5  136   \n",
       "\n",
       "     HOLIDAY  HCLUS PROMOTION  PRO_PERCENT  KNOB  LE_HOLI  LE_PROMO  LE_ITEM  \n",
       "2352       Y      1         Y     0.197590   146        1         1        0  \n",
       "2353       N      4         Y     0.197590   146        0         1        0  \n",
       "2354       N      4         N     0.000000   146        0         0        0  \n",
       "2355       Y      2         N     0.000000   146        1         0        0  \n",
       "2356       N      4         Y     0.201205   146        0         1        0  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleansedData.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEARWEEK</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>WEEK</th>\n",
       "      <th>QTY</th>\n",
       "      <th>HCLUS</th>\n",
       "      <th>PRO_PERCENT</th>\n",
       "      <th>KNOB</th>\n",
       "      <th>LE_HOLI</th>\n",
       "      <th>LE_PROMO</th>\n",
       "      <th>LE_ITEM</th>\n",
       "      <th>LE_REGION</th>\n",
       "      <th>LE_PROD_GROUP</th>\n",
       "      <th>LE_PROD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>YEARWEEK</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.832932e-01</td>\n",
       "      <td>2.392348e-02</td>\n",
       "      <td>0.024869</td>\n",
       "      <td>8.534880e-03</td>\n",
       "      <td>0.244636</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.108832e-03</td>\n",
       "      <td>0.094830</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.751344e-18</td>\n",
       "      <td>8.518743e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAR</th>\n",
       "      <td>9.832932e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.584529e-01</td>\n",
       "      <td>-0.002302</td>\n",
       "      <td>6.501308e-02</td>\n",
       "      <td>0.203425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.072250e-02</td>\n",
       "      <td>0.084591</td>\n",
       "      <td>-2.500084e-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.095853e-18</td>\n",
       "      <td>-4.122495e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WEEK</th>\n",
       "      <td>2.392348e-02</td>\n",
       "      <td>-1.584529e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.147539</td>\n",
       "      <td>-3.107608e-01</td>\n",
       "      <td>0.209740</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.513620e-01</td>\n",
       "      <td>0.049804</td>\n",
       "      <td>-1.990940e-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.431800e-17</td>\n",
       "      <td>-1.829053e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QTY</th>\n",
       "      <td>2.486926e-02</td>\n",
       "      <td>-2.302016e-03</td>\n",
       "      <td>1.475395e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.597564e-01</td>\n",
       "      <td>0.347993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.246354e-01</td>\n",
       "      <td>0.112769</td>\n",
       "      <td>1.439177e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.510086e-01</td>\n",
       "      <td>1.747543e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCLUS</th>\n",
       "      <td>8.534880e-03</td>\n",
       "      <td>6.501308e-02</td>\n",
       "      <td>-3.107608e-01</td>\n",
       "      <td>-0.259756</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-0.335141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.768332e-01</td>\n",
       "      <td>-0.093221</td>\n",
       "      <td>-5.490514e-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.220885e-17</td>\n",
       "      <td>-9.760642e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRO_PERCENT</th>\n",
       "      <td>2.446360e-01</td>\n",
       "      <td>2.034249e-01</td>\n",
       "      <td>2.097402e-01</td>\n",
       "      <td>0.347993</td>\n",
       "      <td>-3.351410e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.859627e-01</td>\n",
       "      <td>0.725557</td>\n",
       "      <td>1.843868e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.196276e-01</td>\n",
       "      <td>1.650151e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNOB</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LE_HOLI</th>\n",
       "      <td>5.108832e-03</td>\n",
       "      <td>-4.072250e-02</td>\n",
       "      <td>2.513620e-01</td>\n",
       "      <td>0.224635</td>\n",
       "      <td>-9.768332e-01</td>\n",
       "      <td>0.285963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.081870</td>\n",
       "      <td>-6.071775e-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.832234e-18</td>\n",
       "      <td>-3.832974e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LE_PROMO</th>\n",
       "      <td>9.483034e-02</td>\n",
       "      <td>8.459070e-02</td>\n",
       "      <td>4.980427e-02</td>\n",
       "      <td>0.112769</td>\n",
       "      <td>-9.322107e-02</td>\n",
       "      <td>0.725557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.186992e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.059897e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.412269e-01</td>\n",
       "      <td>7.386776e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LE_ITEM</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.500084e-18</td>\n",
       "      <td>-1.990940e-18</td>\n",
       "      <td>0.143918</td>\n",
       "      <td>-5.490514e-19</td>\n",
       "      <td>0.184387</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-6.071775e-19</td>\n",
       "      <td>0.105990</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.130656e-01</td>\n",
       "      <td>9.437133e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LE_REGION</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LE_PROD_GROUP</th>\n",
       "      <td>4.751344e-18</td>\n",
       "      <td>3.095853e-18</td>\n",
       "      <td>-1.431800e-17</td>\n",
       "      <td>0.151009</td>\n",
       "      <td>4.220885e-17</td>\n",
       "      <td>0.219628</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.832234e-18</td>\n",
       "      <td>0.141227</td>\n",
       "      <td>9.130656e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.606239e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LE_PROD</th>\n",
       "      <td>8.518743e-18</td>\n",
       "      <td>-4.122495e-16</td>\n",
       "      <td>-1.829053e-17</td>\n",
       "      <td>0.174754</td>\n",
       "      <td>-9.760642e-18</td>\n",
       "      <td>0.165015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.832974e-17</td>\n",
       "      <td>0.073868</td>\n",
       "      <td>9.437133e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.606239e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   YEARWEEK          YEAR          WEEK       QTY  \\\n",
       "YEARWEEK       1.000000e+00  9.832932e-01  2.392348e-02  0.024869   \n",
       "YEAR           9.832932e-01  1.000000e+00 -1.584529e-01 -0.002302   \n",
       "WEEK           2.392348e-02 -1.584529e-01  1.000000e+00  0.147539   \n",
       "QTY            2.486926e-02 -2.302016e-03  1.475395e-01  1.000000   \n",
       "HCLUS          8.534880e-03  6.501308e-02 -3.107608e-01 -0.259756   \n",
       "PRO_PERCENT    2.446360e-01  2.034249e-01  2.097402e-01  0.347993   \n",
       "KNOB                    NaN           NaN           NaN       NaN   \n",
       "LE_HOLI        5.108832e-03 -4.072250e-02  2.513620e-01  0.224635   \n",
       "LE_PROMO       9.483034e-02  8.459070e-02  4.980427e-02  0.112769   \n",
       "LE_ITEM        0.000000e+00 -2.500084e-18 -1.990940e-18  0.143918   \n",
       "LE_REGION               NaN           NaN           NaN       NaN   \n",
       "LE_PROD_GROUP  4.751344e-18  3.095853e-18 -1.431800e-17  0.151009   \n",
       "LE_PROD        8.518743e-18 -4.122495e-16 -1.829053e-17  0.174754   \n",
       "\n",
       "                      HCLUS  PRO_PERCENT  KNOB       LE_HOLI  LE_PROMO  \\\n",
       "YEARWEEK       8.534880e-03     0.244636   NaN  5.108832e-03  0.094830   \n",
       "YEAR           6.501308e-02     0.203425   NaN -4.072250e-02  0.084591   \n",
       "WEEK          -3.107608e-01     0.209740   NaN  2.513620e-01  0.049804   \n",
       "QTY           -2.597564e-01     0.347993   NaN  2.246354e-01  0.112769   \n",
       "HCLUS          1.000000e+00    -0.335141   NaN -9.768332e-01 -0.093221   \n",
       "PRO_PERCENT   -3.351410e-01     1.000000   NaN  2.859627e-01  0.725557   \n",
       "KNOB                    NaN          NaN   NaN           NaN       NaN   \n",
       "LE_HOLI       -9.768332e-01     0.285963   NaN  1.000000e+00  0.081870   \n",
       "LE_PROMO      -9.322107e-02     0.725557   NaN  8.186992e-02  1.000000   \n",
       "LE_ITEM       -5.490514e-19     0.184387   NaN -6.071775e-19  0.105990   \n",
       "LE_REGION               NaN          NaN   NaN           NaN       NaN   \n",
       "LE_PROD_GROUP  4.220885e-17     0.219628   NaN  3.832234e-18  0.141227   \n",
       "LE_PROD       -9.760642e-18     0.165015   NaN -3.832974e-17  0.073868   \n",
       "\n",
       "                    LE_ITEM  LE_REGION  LE_PROD_GROUP       LE_PROD  \n",
       "YEARWEEK       0.000000e+00        NaN   4.751344e-18  8.518743e-18  \n",
       "YEAR          -2.500084e-18        NaN   3.095853e-18 -4.122495e-16  \n",
       "WEEK          -1.990940e-18        NaN  -1.431800e-17 -1.829053e-17  \n",
       "QTY            1.439177e-01        NaN   1.510086e-01  1.747543e-01  \n",
       "HCLUS         -5.490514e-19        NaN   4.220885e-17 -9.760642e-18  \n",
       "PRO_PERCENT    1.843868e-01        NaN   2.196276e-01  1.650151e-01  \n",
       "KNOB                    NaN        NaN            NaN           NaN  \n",
       "LE_HOLI       -6.071775e-19        NaN   3.832234e-18 -3.832974e-17  \n",
       "LE_PROMO       1.059897e-01        NaN   1.412269e-01  7.386776e-02  \n",
       "LE_ITEM        1.000000e+00        NaN   9.130656e-01  9.437133e-01  \n",
       "LE_REGION               NaN        NaN            NaN           NaN  \n",
       "LE_PROD_GROUP  9.130656e-01        NaN   1.000000e+00  9.606239e-01  \n",
       "LE_PROD        9.437133e-01        NaN   9.606239e-01  1.000000e+00  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrdf = cleansedData.corr()\n",
    "corrdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HCLUS', 'PRO_PERCENT', 'LE_HOLI', 'LE_PROD_GROUP', 'LE_PROD', 'LE_ITEM']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = []\n",
    "features = list (corrdf[ (abs(corrdf.QTY) > 0.15) & (abs(corrdf.QTY) < 1)].index)\n",
    "features.append('LE_ITEM')\n",
    "label = [\"QTY\"]\n",
    "all_data = features[0:]\n",
    "all_data.append(label[0])\n",
    "all_data\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresData = cleansedData[all_data]\n",
    "featuresData = featuresData.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData_features,testData_features,trainingData_label,testData_label = \\\n",
    "train_test_split(featuresData[features],featuresData[label], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HCLUS</th>\n",
       "      <th>PRO_PERCENT</th>\n",
       "      <th>LE_HOLI</th>\n",
       "      <th>LE_PROD_GROUP</th>\n",
       "      <th>LE_PROD</th>\n",
       "      <th>LE_ITEM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2722</th>\n",
       "      <td>4</td>\n",
       "      <td>0.166537</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4406</th>\n",
       "      <td>1</td>\n",
       "      <td>0.304350</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6159</th>\n",
       "      <td>1</td>\n",
       "      <td>0.318242</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6062</th>\n",
       "      <td>4</td>\n",
       "      <td>0.409013</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6146</th>\n",
       "      <td>4</td>\n",
       "      <td>0.262180</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1</td>\n",
       "      <td>0.249246</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5947</th>\n",
       "      <td>2</td>\n",
       "      <td>0.235609</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>2</td>\n",
       "      <td>0.291705</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7581</th>\n",
       "      <td>4</td>\n",
       "      <td>0.245307</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4464</th>\n",
       "      <td>4</td>\n",
       "      <td>0.217442</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5644</th>\n",
       "      <td>0</td>\n",
       "      <td>0.304350</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>4</td>\n",
       "      <td>0.166537</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6347</th>\n",
       "      <td>4</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>4</td>\n",
       "      <td>0.235609</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7297</th>\n",
       "      <td>4</td>\n",
       "      <td>0.218131</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6735</th>\n",
       "      <td>1</td>\n",
       "      <td>0.206118</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>1</td>\n",
       "      <td>0.240096</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5326</th>\n",
       "      <td>4</td>\n",
       "      <td>0.200233</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>4</td>\n",
       "      <td>0.368388</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>4</td>\n",
       "      <td>0.350325</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4369</th>\n",
       "      <td>4</td>\n",
       "      <td>0.291705</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6218</th>\n",
       "      <td>1</td>\n",
       "      <td>0.429366</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2423</th>\n",
       "      <td>1</td>\n",
       "      <td>0.299933</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5023</th>\n",
       "      <td>1</td>\n",
       "      <td>0.340825</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>2</td>\n",
       "      <td>0.180645</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3168</th>\n",
       "      <td>4</td>\n",
       "      <td>0.276026</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4075</th>\n",
       "      <td>1</td>\n",
       "      <td>0.214483</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>4</td>\n",
       "      <td>0.212249</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>1</td>\n",
       "      <td>0.400100</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7383</th>\n",
       "      <td>4</td>\n",
       "      <td>0.280258</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2628</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>1</td>\n",
       "      <td>0.166221</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6542</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4764</th>\n",
       "      <td>4</td>\n",
       "      <td>0.285852</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3562</th>\n",
       "      <td>4</td>\n",
       "      <td>0.340825</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7051</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>1</td>\n",
       "      <td>0.178485</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>4</td>\n",
       "      <td>0.196632</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4453</th>\n",
       "      <td>1</td>\n",
       "      <td>0.304350</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5374</th>\n",
       "      <td>4</td>\n",
       "      <td>0.333478</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5396</th>\n",
       "      <td>1</td>\n",
       "      <td>0.333478</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>1</td>\n",
       "      <td>0.368388</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7449</th>\n",
       "      <td>2</td>\n",
       "      <td>0.194618</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>1</td>\n",
       "      <td>0.378589</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6652</th>\n",
       "      <td>4</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2797</th>\n",
       "      <td>4</td>\n",
       "      <td>0.168920</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4225</th>\n",
       "      <td>4</td>\n",
       "      <td>0.130535</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>4</td>\n",
       "      <td>0.280723</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5056</th>\n",
       "      <td>4</td>\n",
       "      <td>0.312691</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>4</td>\n",
       "      <td>0.172335</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>4</td>\n",
       "      <td>0.278099</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>4</td>\n",
       "      <td>0.225750</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>4</td>\n",
       "      <td>0.190472</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5157</th>\n",
       "      <td>0</td>\n",
       "      <td>0.477541</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6073 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HCLUS  PRO_PERCENT  LE_HOLI  LE_PROD_GROUP  LE_PROD  LE_ITEM\n",
       "2722      4     0.166537        0              2        3       18\n",
       "4406      1     0.304350        1              2        3       30\n",
       "6997      4     0.000000        0              3        6       47\n",
       "6159      1     0.318242        1              3        5       42\n",
       "6062      4     0.409013        0              3        5       41\n",
       "1615      4     0.000000        0              1        2       11\n",
       "6146      4     0.262180        0              3        5       42\n",
       "181       1     0.249246        1              0        0        1\n",
       "5947      2     0.235609        1              2        4       40\n",
       "5787      2     0.291705        1              2        4       39\n",
       "7581      4     0.245307        0              3        6       51\n",
       "4464      4     0.217442        0              2        3       30\n",
       "5644      0     0.304350        1              2        4       38\n",
       "2641      4     0.166537        0              2        3       18\n",
       "6347      4     0.294869        0              3        5       43\n",
       "1660      4     0.235609        0              1        2       11\n",
       "7297      4     0.218131        0              3        6       49\n",
       "6735      1     0.206118        1              3        6       46\n",
       "2064      1     0.240096        1              1        2       14\n",
       "5326      4     0.200233        0              2        4       36\n",
       "1554      4     0.368388        0              1        2       10\n",
       "3772      4     0.350325        0              2        3       25\n",
       "4369      4     0.291705        0              2        3       29\n",
       "6218      1     0.429366        1              3        5       42\n",
       "2423      1     0.299933        1              2        3       16\n",
       "5023      1     0.340825        1              2        3       34\n",
       "1714      4     0.000000        0              1        2       11\n",
       "385       2     0.180645        1              0        0        2\n",
       "3168      4     0.276026        0              2        3       21\n",
       "4075      1     0.214483        1              2        3       27\n",
       "...     ...          ...      ...            ...      ...      ...\n",
       "753       4     0.212249        0              0        0        5\n",
       "3049      1     0.400100        1              2        3       20\n",
       "7383      4     0.280258        0              3        6       50\n",
       "2628      1     0.000000        1              2        3       18\n",
       "562       1     0.166221        1              0        0        3\n",
       "6542      4     0.000000        0              3        6       44\n",
       "4764      4     0.285852        0              2        3       32\n",
       "3562      4     0.340825        0              2        3       24\n",
       "252       4     0.000000        0              0        0        1\n",
       "7051      4     0.000000        0              3        6       48\n",
       "2516      1     0.178485        1              2        3       17\n",
       "2962      4     0.196632        0              2        3       20\n",
       "4453      1     0.304350        1              2        3       30\n",
       "5374      4     0.333478        0              2        4       36\n",
       "5396      1     0.333478        1              2        4       36\n",
       "1202      1     0.368388        1              1        2        8\n",
       "7449      2     0.194618        1              3        6       51\n",
       "3462      1     0.378589        1              2        3       23\n",
       "6652      4     0.294869        0              3        6       45\n",
       "2797      4     0.168920        0              2        3       19\n",
       "4225      4     0.130535        0              2        3       28\n",
       "144       4     0.280723        0              0        0        0\n",
       "5056      4     0.312691        0              2        3       34\n",
       "2895      4     0.172335        0              2        3       19\n",
       "2763      4     0.278099        0              2        3       18\n",
       "905       4     0.000000        0              1        1        6\n",
       "5192      4     0.225750        0              2        3       35\n",
       "3980      4     0.190472        0              2        3       27\n",
       "235       4     0.000000        0              0        0        1\n",
       "5157      0     0.477541        1              2        3       35\n",
       "\n",
       "[6073 rows x 6 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputDim = trainingData_features.loc[0,:].shape\n",
    "inputDim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import tree\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 8)                 56        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 137\n",
      "Trainable params: 137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=8, activation=\"relu\",input_shape=inputDim))\n",
    "model.add(Dense(units=8, activation=\"relu\"))\n",
    "model.add(Dense(units=1, activation=\"relu\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer = 'adam', metrics=['mean_squared_error','mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 모델 훈련(callback 정의)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callback List를 만들기 위한 선언 및 변수 설정\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "log_folder = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "\n",
    "log_dirs = os.path.join(\"logs\",log_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list = [\n",
    "    TensorBoard(log_dir = log_dirs),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=10), \n",
    "    ModelCheckpoint(filepath=\"./model_r_weights.h5\", monitor=\"val_loss\", verbose=1, save_best_only=True)   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1016 15:52:03.373573  8284 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4858 samples, validate on 1215 samples\n",
      "Epoch 1/2000\n",
      "4384/4858 [==========================>...] - ETA: 0s - loss: 146666.8189 - mean_squared_error: 146666.8125 - mean_absolute_error: 166.6271\n",
      "Epoch 00001: val_loss improved from inf to 132999.97621, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 1s 141us/sample - loss: 146960.4601 - mean_squared_error: 146960.4531 - mean_absolute_error: 167.0422 - val_loss: 132999.9762 - val_mean_squared_error: 132999.9688 - val_mean_absolute_error: 154.1216\n",
      "Epoch 2/2000\n",
      "4416/4858 [==========================>...] - ETA: 0s - loss: 132352.2782 - mean_squared_error: 132352.3281 - mean_absolute_error: 153.8720\n",
      "Epoch 00002: val_loss improved from 132999.97621 to 115273.30388, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 58us/sample - loss: 135553.1392 - mean_squared_error: 135553.1875 - mean_absolute_error: 155.7233 - val_loss: 115273.3039 - val_mean_squared_error: 115273.3203 - val_mean_absolute_error: 153.5338\n",
      "Epoch 3/2000\n",
      "4128/4858 [========================>.....] - ETA: 0s - loss: 128176.7018 - mean_squared_error: 128176.7031 - mean_absolute_error: 169.3420\n",
      "Epoch 00003: val_loss improved from 115273.30388 to 110539.31996, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 54us/sample - loss: 121217.3663 - mean_squared_error: 121217.3672 - mean_absolute_error: 168.6452 - val_loss: 110539.3200 - val_mean_squared_error: 110539.3281 - val_mean_absolute_error: 176.5975\n",
      "Epoch 4/2000\n",
      "4576/4858 [===========================>..] - ETA: 0s - loss: 119830.9361 - mean_squared_error: 119830.8984 - mean_absolute_error: 179.3120\n",
      "Epoch 00004: val_loss improved from 110539.31996 to 109703.78040, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 91us/sample - loss: 119077.1158 - mean_squared_error: 119077.0859 - mean_absolute_error: 178.6486 - val_loss: 109703.7804 - val_mean_squared_error: 109703.7891 - val_mean_absolute_error: 180.2384\n",
      "Epoch 5/2000\n",
      "4384/4858 [==========================>...] - ETA: 0s - loss: 116027.3861 - mean_squared_error: 116027.3828 - mean_absolute_error: 180.4843\n",
      "Epoch 00005: val_loss improved from 109703.78040 to 108787.52137, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 96us/sample - loss: 118240.9024 - mean_squared_error: 118240.9062 - mean_absolute_error: 180.0105 - val_loss: 108787.5214 - val_mean_squared_error: 108787.5312 - val_mean_absolute_error: 179.1330\n",
      "Epoch 6/2000\n",
      "4768/4858 [============================>.] - ETA: 0s - loss: 118605.4213 - mean_squared_error: 118605.4219 - mean_absolute_error: 179.8087\n",
      "Epoch 00006: val_loss improved from 108787.52137 to 107895.83641, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 43us/sample - loss: 117422.1012 - mean_squared_error: 117422.0938 - mean_absolute_error: 179.3663 - val_loss: 107895.8364 - val_mean_squared_error: 107895.8359 - val_mean_absolute_error: 178.8939\n",
      "Epoch 7/2000\n",
      "3456/4858 [====================>.........] - ETA: 0s - loss: 122694.1299 - mean_squared_error: 122694.1484 - mean_absolute_error: 182.0310\n",
      "Epoch 00007: val_loss improved from 107895.83641 to 106948.21080, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 44us/sample - loss: 116531.8274 - mean_squared_error: 116531.8438 - mean_absolute_error: 179.3211 - val_loss: 106948.2108 - val_mean_squared_error: 106948.2109 - val_mean_absolute_error: 177.6794\n",
      "Epoch 8/2000\n",
      "4704/4858 [============================>.] - ETA: 0s - loss: 117415.1311 - mean_squared_error: 117415.1328 - mean_absolute_error: 178.9719\n",
      "Epoch 00008: val_loss improved from 106948.21080 to 106002.88167, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 77us/sample - loss: 115650.5968 - mean_squared_error: 115650.5938 - mean_absolute_error: 178.4856 - val_loss: 106002.8817 - val_mean_squared_error: 106002.8750 - val_mean_absolute_error: 178.5692\n",
      "Epoch 9/2000\n",
      "3968/4858 [=======================>......] - ETA: 0s - loss: 117360.8483 - mean_squared_error: 117360.8359 - mean_absolute_error: 178.5130\n",
      "Epoch 00009: val_loss improved from 106002.88167 to 104814.12589, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 60us/sample - loss: 114711.8137 - mean_squared_error: 114711.8047 - mean_absolute_error: 178.3739 - val_loss: 104814.1259 - val_mean_squared_error: 104814.1406 - val_mean_absolute_error: 177.8881\n",
      "Epoch 10/2000\n",
      "4736/4858 [============================>.] - ETA: 0s - loss: 105068.2852 - mean_squared_error: 105068.2891 - mean_absolute_error: 176.5227\n",
      "Epoch 00010: val_loss improved from 104814.12589 to 103611.30758, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 87us/sample - loss: 113451.0439 - mean_squared_error: 113451.0391 - mean_absolute_error: 178.1010 - val_loss: 103611.3076 - val_mean_squared_error: 103611.2891 - val_mean_absolute_error: 179.3040\n",
      "Epoch 11/2000\n",
      "4832/4858 [============================>.] - ETA: 0s - loss: 112376.3092 - mean_squared_error: 112376.3047 - mean_absolute_error: 178.5594\n",
      "Epoch 00011: val_loss improved from 103611.30758 to 102215.56780, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 83us/sample - loss: 112315.7626 - mean_squared_error: 112315.7578 - mean_absolute_error: 178.6120 - val_loss: 102215.5678 - val_mean_squared_error: 102215.5703 - val_mean_absolute_error: 175.4536\n",
      "Epoch 12/2000\n",
      "3264/4858 [===================>..........] - ETA: 0s - loss: 109180.5937 - mean_squared_error: 109180.6016 - mean_absolute_error: 175.2474\n",
      "Epoch 00012: val_loss improved from 102215.56780 to 101067.36757, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 43us/sample - loss: 111215.7926 - mean_squared_error: 111215.7969 - mean_absolute_error: 177.3918 - val_loss: 101067.3676 - val_mean_squared_error: 101067.3672 - val_mean_absolute_error: 177.7914\n",
      "Epoch 13/2000\n",
      "3232/4858 [==================>...........] - ETA: 0s - loss: 101185.3919 - mean_squared_error: 101185.3828 - mean_absolute_error: 178.5888\n",
      "Epoch 00013: val_loss improved from 101067.36757 to 99845.85179, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 43us/sample - loss: 110136.7925 - mean_squared_error: 110136.7812 - mean_absolute_error: 178.1354 - val_loss: 99845.8518 - val_mean_squared_error: 99845.8516 - val_mean_absolute_error: 172.1755\n",
      "Epoch 14/2000\n",
      "4608/4858 [===========================>..] - ETA: 0s - loss: 111310.2479 - mean_squared_error: 111310.2344 - mean_absolute_error: 176.7533\n",
      "Epoch 00014: val_loss improved from 99845.85179 to 98796.26020, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 44us/sample - loss: 109217.7880 - mean_squared_error: 109217.7812 - mean_absolute_error: 176.2489 - val_loss: 98796.2602 - val_mean_squared_error: 98796.2578 - val_mean_absolute_error: 172.2353\n",
      "Epoch 15/2000\n",
      "3200/4858 [==================>...........] - ETA: 0s - loss: 122233.2202 - mean_squared_error: 122233.2109 - mean_absolute_error: 177.9211\n",
      "Epoch 00015: val_loss improved from 98796.26020 to 97842.69687, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 41us/sample - loss: 108325.5306 - mean_squared_error: 108325.5234 - mean_absolute_error: 175.9276 - val_loss: 97842.6969 - val_mean_squared_error: 97842.6875 - val_mean_absolute_error: 173.9904\n",
      "Epoch 16/2000\n",
      "3392/4858 [===================>..........] - ETA: 0s - loss: 109366.9847 - mean_squared_error: 109366.9609 - mean_absolute_error: 175.7208\n",
      "Epoch 00016: val_loss improved from 97842.69687 to 96917.90365, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 45us/sample - loss: 107519.2472 - mean_squared_error: 107519.2500 - mean_absolute_error: 175.0799 - val_loss: 96917.9037 - val_mean_squared_error: 96917.8906 - val_mean_absolute_error: 172.6005\n",
      "Epoch 17/2000\n",
      "3392/4858 [===================>..........] - ETA: 0s - loss: 111149.9600 - mean_squared_error: 111149.9688 - mean_absolute_error: 178.9086\n",
      "Epoch 00017: val_loss improved from 96917.90365 to 96144.64316, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 42us/sample - loss: 106848.9080 - mean_squared_error: 106848.9062 - mean_absolute_error: 176.4591 - val_loss: 96144.6432 - val_mean_squared_error: 96144.6328 - val_mean_absolute_error: 169.0456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/2000\n",
      "4416/4858 [==========================>...] - ETA: 0s - loss: 105613.1709 - mean_squared_error: 105613.1719 - mean_absolute_error: 174.9903\n",
      "Epoch 00018: val_loss improved from 96144.64316 to 95478.19129, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 57us/sample - loss: 106281.5182 - mean_squared_error: 106281.5234 - mean_absolute_error: 175.1986 - val_loss: 95478.1913 - val_mean_squared_error: 95478.1953 - val_mean_absolute_error: 168.6806\n",
      "Epoch 19/2000\n",
      "3904/4858 [=======================>......] - ETA: 0s - loss: 106800.2583 - mean_squared_error: 106800.2812 - mean_absolute_error: 173.7615\n",
      "Epoch 00019: val_loss improved from 95478.19129 to 94951.01161, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 64us/sample - loss: 105775.7925 - mean_squared_error: 105775.8047 - mean_absolute_error: 174.0852 - val_loss: 94951.0116 - val_mean_squared_error: 94951.0000 - val_mean_absolute_error: 172.4890\n",
      "Epoch 20/2000\n",
      "4064/4858 [========================>.....] - ETA: 0s - loss: 106980.0501 - mean_squared_error: 106980.0391 - mean_absolute_error: 175.3734\n",
      "Epoch 00020: val_loss improved from 94951.01161 to 94388.11606, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 55us/sample - loss: 105368.3152 - mean_squared_error: 105368.3203 - mean_absolute_error: 175.2072 - val_loss: 94388.1161 - val_mean_squared_error: 94388.1250 - val_mean_absolute_error: 170.1115\n",
      "Epoch 21/2000\n",
      "4672/4858 [===========================>..] - ETA: 0s - loss: 107545.0681 - mean_squared_error: 107545.0312 - mean_absolute_error: 176.1304\n",
      "Epoch 00021: val_loss improved from 94388.11606 to 94011.45051, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 57us/sample - loss: 104996.7723 - mean_squared_error: 104996.7344 - mean_absolute_error: 174.7615 - val_loss: 94011.4505 - val_mean_squared_error: 94011.4453 - val_mean_absolute_error: 170.2536\n",
      "Epoch 22/2000\n",
      "4800/4858 [============================>.] - ETA: 0s - loss: 105484.1841 - mean_squared_error: 105484.2109 - mean_absolute_error: 175.3259\n",
      "Epoch 00022: val_loss improved from 94011.45051 to 93615.24310, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 57us/sample - loss: 104702.8906 - mean_squared_error: 104702.9219 - mean_absolute_error: 174.8922 - val_loss: 93615.2431 - val_mean_squared_error: 93615.2422 - val_mean_absolute_error: 167.8793\n",
      "Epoch 23/2000\n",
      "4608/4858 [===========================>..] - ETA: 0s - loss: 106055.3956 - mean_squared_error: 106055.3750 - mean_absolute_error: 174.8476\n",
      "Epoch 00023: val_loss improved from 93615.24310 to 93332.77568, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 90us/sample - loss: 104461.9213 - mean_squared_error: 104461.9062 - mean_absolute_error: 174.3498 - val_loss: 93332.7757 - val_mean_squared_error: 93332.7812 - val_mean_absolute_error: 169.9143\n",
      "Epoch 24/2000\n",
      "4000/4858 [=======================>......] - ETA: 0s - loss: 102810.8311 - mean_squared_error: 102810.7969 - mean_absolute_error: 171.7652\n",
      "Epoch 00024: val_loss improved from 93332.77568 to 93138.81765, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 67us/sample - loss: 104218.2419 - mean_squared_error: 104218.2266 - mean_absolute_error: 173.6511 - val_loss: 93138.8176 - val_mean_squared_error: 93138.8203 - val_mean_absolute_error: 171.5760\n",
      "Epoch 25/2000\n",
      "3872/4858 [======================>.......] - ETA: 0s - loss: 109352.6172 - mean_squared_error: 109352.6250 - mean_absolute_error: 177.3701\n",
      "Epoch 00025: val_loss improved from 93138.81765 to 92858.63765, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 56us/sample - loss: 104054.9197 - mean_squared_error: 104054.9297 - mean_absolute_error: 175.4363 - val_loss: 92858.6377 - val_mean_squared_error: 92858.6406 - val_mean_absolute_error: 167.2880\n",
      "Epoch 26/2000\n",
      "4256/4858 [=========================>....] - ETA: 0s - loss: 94547.0893 - mean_squared_error: 94547.0781 - mean_absolute_error: 170.7128\n",
      "Epoch 00026: val_loss improved from 92858.63765 to 92635.89491, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 83us/sample - loss: 103834.6525 - mean_squared_error: 103834.6484 - mean_absolute_error: 173.5499 - val_loss: 92635.8949 - val_mean_squared_error: 92635.8906 - val_mean_absolute_error: 168.7643\n",
      "Epoch 27/2000\n",
      "4832/4858 [============================>.] - ETA: 0s - loss: 103998.0163 - mean_squared_error: 103998.0234 - mean_absolute_error: 173.6402\n",
      "Epoch 00027: val_loss improved from 92635.89491 to 92403.61363, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 86us/sample - loss: 103625.9734 - mean_squared_error: 103625.9766 - mean_absolute_error: 173.4892 - val_loss: 92403.6136 - val_mean_squared_error: 92403.6016 - val_mean_absolute_error: 168.4545\n",
      "Epoch 28/2000\n",
      "4736/4858 [============================>.] - ETA: 0s - loss: 104870.3634 - mean_squared_error: 104870.3594 - mean_absolute_error: 175.2601\n",
      "Epoch 00028: val_loss improved from 92403.61363 to 92272.29101, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 87us/sample - loss: 103492.0156 - mean_squared_error: 103492.0078 - mean_absolute_error: 174.4823 - val_loss: 92272.2910 - val_mean_squared_error: 92272.2969 - val_mean_absolute_error: 169.1891\n",
      "Epoch 29/2000\n",
      "4320/4858 [=========================>....] - ETA: 0s - loss: 99827.3360 - mean_squared_error: 99827.3594 - mean_absolute_error: 172.2483  \n",
      "Epoch 00029: val_loss improved from 92272.29101 to 92084.57187, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 60us/sample - loss: 103331.2018 - mean_squared_error: 103331.2266 - mean_absolute_error: 173.8485 - val_loss: 92084.5719 - val_mean_squared_error: 92084.5703 - val_mean_absolute_error: 168.6109\n",
      "Epoch 30/2000\n",
      "3648/4858 [=====================>........] - ETA: 0s - loss: 98947.2316 - mean_squared_error: 98947.2266 - mean_absolute_error: 176.0736  \n",
      "Epoch 00030: val_loss improved from 92084.57187 to 91898.17029, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 62us/sample - loss: 103135.7999 - mean_squared_error: 103135.7891 - mean_absolute_error: 174.9364 - val_loss: 91898.1703 - val_mean_squared_error: 91898.1641 - val_mean_absolute_error: 165.9130\n",
      "Epoch 31/2000\n",
      "4000/4858 [=======================>......] - ETA: 0s - loss: 103131.6454 - mean_squared_error: 103131.6562 - mean_absolute_error: 170.9424\n",
      "Epoch 00031: val_loss improved from 91898.17029 to 91737.19120, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 61us/sample - loss: 103049.1193 - mean_squared_error: 103049.1172 - mean_absolute_error: 173.1158 - val_loss: 91737.1912 - val_mean_squared_error: 91737.1953 - val_mean_absolute_error: 169.0913\n",
      "Epoch 32/2000\n",
      "4224/4858 [=========================>....] - ETA: 0s - loss: 103734.7587 - mean_squared_error: 103734.7734 - mean_absolute_error: 173.8071\n",
      "Epoch 00032: val_loss improved from 91737.19120 to 91705.71041, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 62us/sample - loss: 102888.2655 - mean_squared_error: 102888.2656 - mean_absolute_error: 173.7380 - val_loss: 91705.7104 - val_mean_squared_error: 91705.7031 - val_mean_absolute_error: 169.0707\n",
      "Epoch 33/2000\n",
      "4672/4858 [===========================>..] - ETA: 0s - loss: 103331.6957 - mean_squared_error: 103331.6719 - mean_absolute_error: 174.1727\n",
      "Epoch 00033: val_loss improved from 91705.71041 to 91528.95484, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 86us/sample - loss: 102699.7358 - mean_squared_error: 102699.7109 - mean_absolute_error: 174.0012 - val_loss: 91528.9548 - val_mean_squared_error: 91528.9453 - val_mean_absolute_error: 169.2389\n",
      "Epoch 34/2000\n",
      "4640/4858 [===========================>..] - ETA: 0s - loss: 103192.4460 - mean_squared_error: 103192.4375 - mean_absolute_error: 173.4840\n",
      "Epoch 00034: val_loss improved from 91528.95484 to 91390.62014, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 68us/sample - loss: 102567.6690 - mean_squared_error: 102567.6641 - mean_absolute_error: 173.7008 - val_loss: 91390.6201 - val_mean_squared_error: 91390.6250 - val_mean_absolute_error: 167.7418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/2000\n",
      "4608/4858 [===========================>..] - ETA: 0s - loss: 101375.2562 - mean_squared_error: 101375.2422 - mean_absolute_error: 172.7732\n",
      "Epoch 00035: val_loss improved from 91390.62014 to 91216.22498, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 89us/sample - loss: 102455.7858 - mean_squared_error: 102455.7734 - mean_absolute_error: 173.7456 - val_loss: 91216.2250 - val_mean_squared_error: 91216.2109 - val_mean_absolute_error: 168.1683\n",
      "Epoch 36/2000\n",
      "4256/4858 [=========================>....] - ETA: 0s - loss: 107283.0384 - mean_squared_error: 107283.0078 - mean_absolute_error: 178.0132\n",
      "Epoch 00036: val_loss improved from 91216.22498 to 91082.37449, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 61us/sample - loss: 102300.3046 - mean_squared_error: 102300.2812 - mean_absolute_error: 174.7452 - val_loss: 91082.3745 - val_mean_squared_error: 91082.3750 - val_mean_absolute_error: 165.7242\n",
      "Epoch 37/2000\n",
      "4672/4858 [===========================>..] - ETA: 0s - loss: 103033.9486 - mean_squared_error: 103033.9609 - mean_absolute_error: 171.8626\n",
      "Epoch 00037: val_loss improved from 91082.37449 to 90989.88965, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 88us/sample - loss: 102186.6213 - mean_squared_error: 102186.6328 - mean_absolute_error: 171.9533 - val_loss: 90989.8896 - val_mean_squared_error: 90989.8984 - val_mean_absolute_error: 168.3079\n",
      "Epoch 38/2000\n",
      "4800/4858 [============================>.] - ETA: 0s - loss: 102645.0559 - mean_squared_error: 102645.0625 - mean_absolute_error: 173.6315\n",
      "Epoch 00038: val_loss improved from 90989.88965 to 90842.24241, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 86us/sample - loss: 101984.1010 - mean_squared_error: 101984.1016 - mean_absolute_error: 173.4020 - val_loss: 90842.2424 - val_mean_squared_error: 90842.2578 - val_mean_absolute_error: 168.3805\n",
      "Epoch 39/2000\n",
      "4576/4858 [===========================>..] - ETA: 0s - loss: 100416.5979 - mean_squared_error: 100416.6016 - mean_absolute_error: 170.1205\n",
      "Epoch 00039: val_loss improved from 90842.24241 to 90762.69666, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 89us/sample - loss: 101811.1600 - mean_squared_error: 101811.1641 - mean_absolute_error: 171.7325 - val_loss: 90762.6967 - val_mean_squared_error: 90762.7031 - val_mean_absolute_error: 169.7019\n",
      "Epoch 40/2000\n",
      "4192/4858 [========================>.....] - ETA: 0s - loss: 101445.6864 - mean_squared_error: 101445.6953 - mean_absolute_error: 173.9483\n",
      "Epoch 00040: val_loss improved from 90762.69666 to 90536.73465, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 81us/sample - loss: 101747.3954 - mean_squared_error: 101747.4219 - mean_absolute_error: 173.5555 - val_loss: 90536.7346 - val_mean_squared_error: 90536.7344 - val_mean_absolute_error: 168.3166\n",
      "Epoch 41/2000\n",
      "4832/4858 [============================>.] - ETA: 0s - loss: 101796.2827 - mean_squared_error: 101796.2500 - mean_absolute_error: 173.1142\n",
      "Epoch 00041: val_loss improved from 90536.73465 to 90460.31732, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 86us/sample - loss: 101570.8229 - mean_squared_error: 101570.7891 - mean_absolute_error: 173.0890 - val_loss: 90460.3173 - val_mean_squared_error: 90460.3281 - val_mean_absolute_error: 168.7521\n",
      "Epoch 42/2000\n",
      "4544/4858 [===========================>..] - ETA: 0s - loss: 100272.5400 - mean_squared_error: 100272.5312 - mean_absolute_error: 173.1792\n",
      "Epoch 00042: val_loss improved from 90460.31732 to 90249.22106, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 89us/sample - loss: 101375.2594 - mean_squared_error: 101375.2500 - mean_absolute_error: 173.6983 - val_loss: 90249.2211 - val_mean_squared_error: 90249.2266 - val_mean_absolute_error: 164.6753\n",
      "Epoch 43/2000\n",
      "4832/4858 [============================>.] - ETA: 0s - loss: 101207.1949 - mean_squared_error: 101207.1953 - mean_absolute_error: 171.5781\n",
      "Epoch 00043: val_loss improved from 90249.22106 to 90114.46920, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 74us/sample - loss: 101233.6044 - mean_squared_error: 101233.6016 - mean_absolute_error: 171.6437 - val_loss: 90114.4692 - val_mean_squared_error: 90114.4688 - val_mean_absolute_error: 166.1264\n",
      "Epoch 44/2000\n",
      "4192/4858 [========================>.....] - ETA: 0s - loss: 94159.5615 - mean_squared_error: 94159.5547 - mean_absolute_error: 169.3014\n",
      "Epoch 00044: val_loss improved from 90114.46920 to 90097.06665, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 60us/sample - loss: 101066.0483 - mean_squared_error: 101066.0703 - mean_absolute_error: 171.4793 - val_loss: 90097.0667 - val_mean_squared_error: 90097.0625 - val_mean_absolute_error: 169.1919\n",
      "Epoch 45/2000\n",
      "3936/4858 [=======================>......] - ETA: 0s - loss: 103777.9988 - mean_squared_error: 103778.0078 - mean_absolute_error: 173.9118\n",
      "Epoch 00045: val_loss improved from 90097.06665 to 89829.90048, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 62us/sample - loss: 100979.4905 - mean_squared_error: 100979.4922 - mean_absolute_error: 173.1557 - val_loss: 89829.9005 - val_mean_squared_error: 89829.9141 - val_mean_absolute_error: 165.4935\n",
      "Epoch 46/2000\n",
      "4032/4858 [=======================>......] - ETA: 0s - loss: 106606.0773 - mean_squared_error: 106606.0625 - mean_absolute_error: 174.7037\n",
      "Epoch 00046: val_loss improved from 89829.90048 to 89681.80107, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 52us/sample - loss: 100751.3314 - mean_squared_error: 100751.3125 - mean_absolute_error: 172.8788 - val_loss: 89681.8011 - val_mean_squared_error: 89681.7891 - val_mean_absolute_error: 164.6106\n",
      "Epoch 47/2000\n",
      "4640/4858 [===========================>..] - ETA: 0s - loss: 98274.9856 - mean_squared_error: 98274.9688 - mean_absolute_error: 170.3855\n",
      "Epoch 00047: val_loss improved from 89681.80107 to 89631.23261, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 82us/sample - loss: 100611.5167 - mean_squared_error: 100611.4922 - mean_absolute_error: 171.2327 - val_loss: 89631.2326 - val_mean_squared_error: 89631.2266 - val_mean_absolute_error: 164.0026\n",
      "Epoch 48/2000\n",
      "4448/4858 [==========================>...] - ETA: 0s - loss: 103228.8014 - mean_squared_error: 103228.7734 - mean_absolute_error: 172.2734\n",
      "Epoch 00048: val_loss improved from 89631.23261 to 89508.77088, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 66us/sample - loss: 100486.0555 - mean_squared_error: 100486.0391 - mean_absolute_error: 172.4659 - val_loss: 89508.7709 - val_mean_squared_error: 89508.7656 - val_mean_absolute_error: 163.5051\n",
      "Epoch 49/2000\n",
      "3328/4858 [===================>..........] - ETA: 0s - loss: 108776.3682 - mean_squared_error: 108776.3594 - mean_absolute_error: 173.8646\n",
      "Epoch 00049: val_loss improved from 89508.77088 to 89469.07425, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 43us/sample - loss: 100304.3869 - mean_squared_error: 100304.3672 - mean_absolute_error: 171.5242 - val_loss: 89469.0743 - val_mean_squared_error: 89469.0859 - val_mean_absolute_error: 165.8579\n",
      "Epoch 50/2000\n",
      "4512/4858 [==========================>...] - ETA: 0s - loss: 95767.7703 - mean_squared_error: 95767.7656 - mean_absolute_error: 172.0618  \n",
      "Epoch 00050: val_loss improved from 89469.07425 to 89329.85173, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 83us/sample - loss: 100113.3210 - mean_squared_error: 100113.3125 - mean_absolute_error: 172.7070 - val_loss: 89329.8517 - val_mean_squared_error: 89329.8516 - val_mean_absolute_error: 163.1372\n",
      "Epoch 51/2000\n",
      "4704/4858 [============================>.] - ETA: 0s - loss: 101819.7229 - mean_squared_error: 101819.7188 - mean_absolute_error: 173.6835\n",
      "Epoch 00051: val_loss did not improve from 89329.85173\n",
      "4858/4858 [==============================] - 0s 83us/sample - loss: 99992.0863 - mean_squared_error: 99992.0859 - mean_absolute_error: 172.8189 - val_loss: 89468.1385 - val_mean_squared_error: 89468.1328 - val_mean_absolute_error: 159.1390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/2000\n",
      "4736/4858 [============================>.] - ETA: 0s - loss: 98064.1883 - mean_squared_error: 98064.1484 - mean_absolute_error: 168.9024  \n",
      "Epoch 00052: val_loss improved from 89329.85173 to 89159.90775, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 56us/sample - loss: 99922.7886 - mean_squared_error: 99922.7500 - mean_absolute_error: 169.6481 - val_loss: 89159.9078 - val_mean_squared_error: 89159.9062 - val_mean_absolute_error: 166.8476\n",
      "Epoch 53/2000\n",
      "4032/4858 [=======================>......] - ETA: 0s - loss: 107588.3470 - mean_squared_error: 107588.3281 - mean_absolute_error: 176.7632\n",
      "Epoch 00053: val_loss improved from 89159.90775 to 89011.42614, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 69us/sample - loss: 99627.3330 - mean_squared_error: 99627.3203 - mean_absolute_error: 172.9415 - val_loss: 89011.4261 - val_mean_squared_error: 89011.4219 - val_mean_absolute_error: 161.9789\n",
      "Epoch 54/2000\n",
      "4160/4858 [========================>.....] - ETA: 0s - loss: 102369.9302 - mean_squared_error: 102369.9297 - mean_absolute_error: 171.7483\n",
      "Epoch 00054: val_loss improved from 89011.42614 to 88920.99023, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 68us/sample - loss: 99579.2396 - mean_squared_error: 99579.2500 - mean_absolute_error: 170.4477 - val_loss: 88920.9902 - val_mean_squared_error: 88920.9922 - val_mean_absolute_error: 163.9871\n",
      "Epoch 55/2000\n",
      "4672/4858 [===========================>..] - ETA: 0s - loss: 98912.4578 - mean_squared_error: 98912.4297 - mean_absolute_error: 170.0791\n",
      "Epoch 00055: val_loss improved from 88920.99023 to 88853.05360, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 88us/sample - loss: 99394.4826 - mean_squared_error: 99394.4609 - mean_absolute_error: 170.1957 - val_loss: 88853.0536 - val_mean_squared_error: 88853.0469 - val_mean_absolute_error: 166.2850\n",
      "Epoch 56/2000\n",
      "4448/4858 [==========================>...] - ETA: 0s - loss: 102235.8028 - mean_squared_error: 102235.7812 - mean_absolute_error: 173.8635\n",
      "Epoch 00056: val_loss improved from 88853.05360 to 88700.70240, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 92us/sample - loss: 99235.7329 - mean_squared_error: 99235.7031 - mean_absolute_error: 172.7207 - val_loss: 88700.7024 - val_mean_squared_error: 88700.6953 - val_mean_absolute_error: 162.2779\n",
      "Epoch 57/2000\n",
      "4576/4858 [===========================>..] - ETA: 0s - loss: 95695.4335 - mean_squared_error: 95695.4531 - mean_absolute_error: 170.2390\n",
      "Epoch 00057: val_loss improved from 88700.70240 to 88618.28672, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 92us/sample - loss: 99164.7882 - mean_squared_error: 99164.8047 - mean_absolute_error: 170.3385 - val_loss: 88618.2867 - val_mean_squared_error: 88618.2969 - val_mean_absolute_error: 165.2740\n",
      "Epoch 58/2000\n",
      "4224/4858 [=========================>....] - ETA: 0s - loss: 100238.4820 - mean_squared_error: 100238.5078 - mean_absolute_error: 169.7836\n",
      "Epoch 00058: val_loss improved from 88618.28672 to 88448.36483, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 88us/sample - loss: 99067.7043 - mean_squared_error: 99067.7188 - mean_absolute_error: 170.7957 - val_loss: 88448.3648 - val_mean_squared_error: 88448.3672 - val_mean_absolute_error: 163.4628\n",
      "Epoch 59/2000\n",
      "3776/4858 [======================>.......] - ETA: 0s - loss: 101683.6935 - mean_squared_error: 101683.6797 - mean_absolute_error: 172.6640\n",
      "Epoch 00059: val_loss improved from 88448.36483 to 88326.62549, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 52us/sample - loss: 98761.5331 - mean_squared_error: 98761.5312 - mean_absolute_error: 171.7111 - val_loss: 88326.6255 - val_mean_squared_error: 88326.6250 - val_mean_absolute_error: 161.7448\n",
      "Epoch 60/2000\n",
      "4512/4858 [==========================>...] - ETA: 0s - loss: 95425.1342 - mean_squared_error: 95425.1406 - mean_absolute_error: 168.4443\n",
      "Epoch 00060: val_loss improved from 88326.62549 to 88231.26050, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 91us/sample - loss: 98684.1310 - mean_squared_error: 98684.1406 - mean_absolute_error: 169.5501 - val_loss: 88231.2605 - val_mean_squared_error: 88231.2656 - val_mean_absolute_error: 166.5519\n",
      "Epoch 61/2000\n",
      "3936/4858 [=======================>......] - ETA: 0s - loss: 100777.7114 - mean_squared_error: 100777.7188 - mean_absolute_error: 171.7102\n",
      "Epoch 00061: val_loss improved from 88231.26050 to 88055.74899, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 51us/sample - loss: 98491.4975 - mean_squared_error: 98491.5156 - mean_absolute_error: 171.6353 - val_loss: 88055.7490 - val_mean_squared_error: 88055.7500 - val_mean_absolute_error: 162.6114\n",
      "Epoch 62/2000\n",
      "4512/4858 [==========================>...] - ETA: 0s - loss: 94206.5965 - mean_squared_error: 94206.5938 - mean_absolute_error: 169.3919\n",
      "Epoch 00062: val_loss improved from 88055.74899 to 87984.52379, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 89us/sample - loss: 98373.0627 - mean_squared_error: 98373.0625 - mean_absolute_error: 170.2632 - val_loss: 87984.5238 - val_mean_squared_error: 87984.5391 - val_mean_absolute_error: 163.4661\n",
      "Epoch 63/2000\n",
      "4160/4858 [========================>.....] - ETA: 0s - loss: 89938.9230 - mean_squared_error: 89938.9297 - mean_absolute_error: 168.2748\n",
      "Epoch 00063: val_loss improved from 87984.52379 to 87810.20604, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 77us/sample - loss: 98181.9290 - mean_squared_error: 98181.9297 - mean_absolute_error: 170.5179 - val_loss: 87810.2060 - val_mean_squared_error: 87810.2031 - val_mean_absolute_error: 162.9986\n",
      "Epoch 64/2000\n",
      "4384/4858 [==========================>...] - ETA: 0s - loss: 98211.9822 - mean_squared_error: 98211.9844 - mean_absolute_error: 166.6733\n",
      "Epoch 00064: val_loss improved from 87810.20604 to 87743.55601, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 58us/sample - loss: 98012.6494 - mean_squared_error: 98012.6562 - mean_absolute_error: 168.6141 - val_loss: 87743.5560 - val_mean_squared_error: 87743.5469 - val_mean_absolute_error: 166.6340\n",
      "Epoch 65/2000\n",
      "4736/4858 [============================>.] - ETA: 0s - loss: 96470.0107 - mean_squared_error: 96469.9922 - mean_absolute_error: 171.4055\n",
      "Epoch 00065: val_loss improved from 87743.55601 to 87556.19084, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 79us/sample - loss: 97938.7554 - mean_squared_error: 97938.7422 - mean_absolute_error: 171.8393 - val_loss: 87556.1908 - val_mean_squared_error: 87556.1953 - val_mean_absolute_error: 162.6311\n",
      "Epoch 66/2000\n",
      "4352/4858 [=========================>....] - ETA: 0s - loss: 98451.1734 - mean_squared_error: 98451.1406 - mean_absolute_error: 170.6901  \n",
      "Epoch 00066: val_loss improved from 87556.19084 to 87484.46426, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 80us/sample - loss: 97757.0570 - mean_squared_error: 97757.0312 - mean_absolute_error: 169.6680 - val_loss: 87484.4643 - val_mean_squared_error: 87484.4609 - val_mean_absolute_error: 163.4964\n",
      "Epoch 67/2000\n",
      "4128/4858 [========================>.....] - ETA: 0s - loss: 104725.8673 - mean_squared_error: 104725.8750 - mean_absolute_error: 173.6709\n",
      "Epoch 00067: val_loss improved from 87484.46426 to 87371.93207, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 84us/sample - loss: 97665.4693 - mean_squared_error: 97665.4609 - mean_absolute_error: 170.9935 - val_loss: 87371.9321 - val_mean_squared_error: 87371.9141 - val_mean_absolute_error: 161.3631\n",
      "Epoch 68/2000\n",
      "4768/4858 [============================>.] - ETA: 0s - loss: 98560.4643 - mean_squared_error: 98560.4688 - mean_absolute_error: 170.1476\n",
      "Epoch 00068: val_loss improved from 87371.93207 to 87270.85757, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 86us/sample - loss: 97514.4287 - mean_squared_error: 97514.4375 - mean_absolute_error: 169.5830 - val_loss: 87270.8576 - val_mean_squared_error: 87270.8594 - val_mean_absolute_error: 162.2265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/2000\n",
      "4320/4858 [=========================>....] - ETA: 0s - loss: 88601.6819 - mean_squared_error: 88601.6875 - mean_absolute_error: 166.8348\n",
      "Epoch 00069: val_loss improved from 87270.85757 to 87133.87195, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 63us/sample - loss: 97368.4268 - mean_squared_error: 97368.4141 - mean_absolute_error: 168.6925 - val_loss: 87133.8720 - val_mean_squared_error: 87133.8672 - val_mean_absolute_error: 163.9437\n",
      "Epoch 70/2000\n",
      "4448/4858 [==========================>...] - ETA: 0s - loss: 94564.8758 - mean_squared_error: 94564.8828 - mean_absolute_error: 167.9614\n",
      "Epoch 00070: val_loss improved from 87133.87195 to 86943.22514, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 76us/sample - loss: 97145.3584 - mean_squared_error: 97145.3750 - mean_absolute_error: 169.7867 - val_loss: 86943.2251 - val_mean_squared_error: 86943.2266 - val_mean_absolute_error: 163.8552\n",
      "Epoch 71/2000\n",
      "4448/4858 [==========================>...] - ETA: 0s - loss: 97341.5057 - mean_squared_error: 97341.4766 - mean_absolute_error: 169.7946  \n",
      "Epoch 00071: val_loss improved from 86943.22514 to 86806.41181, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 79us/sample - loss: 97072.5543 - mean_squared_error: 97072.5156 - mean_absolute_error: 169.8907 - val_loss: 86806.4118 - val_mean_squared_error: 86806.4219 - val_mean_absolute_error: 163.1849\n",
      "Epoch 72/2000\n",
      "4736/4858 [============================>.] - ETA: 0s - loss: 98105.6418 - mean_squared_error: 98105.6562 - mean_absolute_error: 169.3983\n",
      "Epoch 00072: val_loss improved from 86806.41181 to 86771.29120, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 79us/sample - loss: 96873.0448 - mean_squared_error: 96873.0547 - mean_absolute_error: 168.6342 - val_loss: 86771.2912 - val_mean_squared_error: 86771.2969 - val_mean_absolute_error: 165.3687\n",
      "Epoch 73/2000\n",
      "4800/4858 [============================>.] - ETA: 0s - loss: 97251.3826 - mean_squared_error: 97251.3828 - mean_absolute_error: 169.4019\n",
      "Epoch 00073: val_loss improved from 86771.29120 to 86606.88850, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 81us/sample - loss: 96707.9975 - mean_squared_error: 96708.0000 - mean_absolute_error: 169.0901 - val_loss: 86606.8885 - val_mean_squared_error: 86606.8906 - val_mean_absolute_error: 164.1131\n",
      "Epoch 74/2000\n",
      "4608/4858 [===========================>..] - ETA: 0s - loss: 90032.3900 - mean_squared_error: 90032.3906 - mean_absolute_error: 167.5271\n",
      "Epoch 00074: val_loss improved from 86606.88850 to 86537.61033, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 79us/sample - loss: 96537.4246 - mean_squared_error: 96537.4297 - mean_absolute_error: 168.2874 - val_loss: 86537.6103 - val_mean_squared_error: 86537.6016 - val_mean_absolute_error: 164.5477\n",
      "Epoch 75/2000\n",
      "4800/4858 [============================>.] - ETA: 0s - loss: 97127.8139 - mean_squared_error: 97127.8281 - mean_absolute_error: 169.7796\n",
      "Epoch 00075: val_loss improved from 86537.61033 to 86262.05289, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 85us/sample - loss: 96461.9821 - mean_squared_error: 96461.9922 - mean_absolute_error: 169.2714 - val_loss: 86262.0529 - val_mean_squared_error: 86262.0547 - val_mean_absolute_error: 164.2441\n",
      "Epoch 76/2000\n",
      "4096/4858 [========================>.....] - ETA: 0s - loss: 99911.9826 - mean_squared_error: 99911.9922 - mean_absolute_error: 170.6805  \n",
      "Epoch 00076: val_loss improved from 86262.05289 to 86115.53146, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 75us/sample - loss: 96238.7060 - mean_squared_error: 96238.7188 - mean_absolute_error: 169.2983 - val_loss: 86115.5315 - val_mean_squared_error: 86115.5156 - val_mean_absolute_error: 161.7326\n",
      "Epoch 77/2000\n",
      "4704/4858 [============================>.] - ETA: 0s - loss: 96003.5721 - mean_squared_error: 96003.5859 - mean_absolute_error: 167.2723\n",
      "Epoch 00077: val_loss improved from 86115.53146 to 86050.84918, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 76us/sample - loss: 96041.5786 - mean_squared_error: 96041.5859 - mean_absolute_error: 167.9570 - val_loss: 86050.8492 - val_mean_squared_error: 86050.8516 - val_mean_absolute_error: 164.0135\n",
      "Epoch 78/2000\n",
      "3968/4858 [=======================>......] - ETA: 0s - loss: 98805.7427 - mean_squared_error: 98805.7500 - mean_absolute_error: 170.3777  \n",
      "Epoch 00078: val_loss improved from 86050.84918 to 85817.24241, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 75us/sample - loss: 95940.2368 - mean_squared_error: 95940.2344 - mean_absolute_error: 169.4476 - val_loss: 85817.2424 - val_mean_squared_error: 85817.2344 - val_mean_absolute_error: 159.2042\n",
      "Epoch 79/2000\n",
      "4480/4858 [==========================>...] - ETA: 0s - loss: 97115.1005 - mean_squared_error: 97115.0938 - mean_absolute_error: 167.3584\n",
      "Epoch 00079: val_loss improved from 85817.24241 to 85650.72803, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 90us/sample - loss: 95818.0911 - mean_squared_error: 95818.0781 - mean_absolute_error: 167.5096 - val_loss: 85650.7280 - val_mean_squared_error: 85650.7031 - val_mean_absolute_error: 161.3603\n",
      "Epoch 80/2000\n",
      "3808/4858 [======================>.......] - ETA: 0s - loss: 85272.5225 - mean_squared_error: 85272.5234 - mean_absolute_error: 165.8525\n",
      "Epoch 00080: val_loss improved from 85650.72803 to 85499.37081, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 56us/sample - loss: 95591.9292 - mean_squared_error: 95591.9297 - mean_absolute_error: 167.7795 - val_loss: 85499.3708 - val_mean_squared_error: 85499.3594 - val_mean_absolute_error: 161.5231\n",
      "Epoch 81/2000\n",
      "4608/4858 [===========================>..] - ETA: 0s - loss: 95594.2351 - mean_squared_error: 95594.2188 - mean_absolute_error: 167.9445\n",
      "Epoch 00081: val_loss improved from 85499.37081 to 85395.26754, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 90us/sample - loss: 95424.0338 - mean_squared_error: 95424.0234 - mean_absolute_error: 167.8175 - val_loss: 85395.2675 - val_mean_squared_error: 85395.2656 - val_mean_absolute_error: 159.1821\n",
      "Epoch 82/2000\n",
      "4640/4858 [===========================>..] - ETA: 0s - loss: 94665.6522 - mean_squared_error: 94665.6484 - mean_absolute_error: 167.5423\n",
      "Epoch 00082: val_loss improved from 85395.26754 to 85231.68384, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 90us/sample - loss: 95275.9502 - mean_squared_error: 95275.9453 - mean_absolute_error: 167.7129 - val_loss: 85231.6838 - val_mean_squared_error: 85231.6797 - val_mean_absolute_error: 159.0116\n",
      "Epoch 83/2000\n",
      "4416/4858 [==========================>...] - ETA: 0s - loss: 91517.8885 - mean_squared_error: 91517.8984 - mean_absolute_error: 166.5220\n",
      "Epoch 00083: val_loss improved from 85231.68384 to 85087.70991, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 93us/sample - loss: 95094.5980 - mean_squared_error: 95094.6094 - mean_absolute_error: 167.2367 - val_loss: 85087.7099 - val_mean_squared_error: 85087.6953 - val_mean_absolute_error: 158.3211\n",
      "Epoch 84/2000\n",
      "4672/4858 [===========================>..] - ETA: 0s - loss: 96691.7608 - mean_squared_error: 96691.7812 - mean_absolute_error: 168.1269\n",
      "Epoch 00084: val_loss improved from 85087.70991 to 84939.34843, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 85us/sample - loss: 94953.8378 - mean_squared_error: 94953.8516 - mean_absolute_error: 167.6704 - val_loss: 84939.3484 - val_mean_squared_error: 84939.3672 - val_mean_absolute_error: 161.0323\n",
      "Epoch 85/2000\n",
      "3520/4858 [====================>.........] - ETA: 0s - loss: 91940.9930 - mean_squared_error: 91941.0078 - mean_absolute_error: 165.6077\n",
      "Epoch 00085: val_loss improved from 84939.34843 to 84763.43469, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 52us/sample - loss: 94787.6518 - mean_squared_error: 94787.6484 - mean_absolute_error: 166.2525 - val_loss: 84763.4347 - val_mean_squared_error: 84763.4453 - val_mean_absolute_error: 162.5368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/2000\n",
      "4576/4858 [===========================>..] - ETA: 0s - loss: 96155.3274 - mean_squared_error: 96155.3516 - mean_absolute_error: 166.5653\n",
      "Epoch 00086: val_loss improved from 84763.43469 to 84545.14941, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 90us/sample - loss: 94638.4637 - mean_squared_error: 94638.4844 - mean_absolute_error: 166.6401 - val_loss: 84545.1494 - val_mean_squared_error: 84545.1484 - val_mean_absolute_error: 160.1263\n",
      "Epoch 87/2000\n",
      "4672/4858 [===========================>..] - ETA: 0s - loss: 90602.7406 - mean_squared_error: 90602.7578 - mean_absolute_error: 166.7841\n",
      "Epoch 00087: val_loss improved from 84545.14941 to 84457.52522, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 85us/sample - loss: 94501.4631 - mean_squared_error: 94501.4766 - mean_absolute_error: 167.1349 - val_loss: 84457.5252 - val_mean_squared_error: 84457.5078 - val_mean_absolute_error: 158.6472\n",
      "Epoch 88/2000\n",
      "3904/4858 [=======================>......] - ETA: 0s - loss: 88661.8698 - mean_squared_error: 88661.8828 - mean_absolute_error: 165.8219\n",
      "Epoch 00088: val_loss improved from 84457.52522 to 84268.18135, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 69us/sample - loss: 94346.0029 - mean_squared_error: 94346.0156 - mean_absolute_error: 166.4140 - val_loss: 84268.1814 - val_mean_squared_error: 84268.1797 - val_mean_absolute_error: 157.5715\n",
      "Epoch 89/2000\n",
      "4512/4858 [==========================>...] - ETA: 0s - loss: 94966.0402 - mean_squared_error: 94966.0391 - mean_absolute_error: 165.1333\n",
      "Epoch 00089: val_loss improved from 84268.18135 to 84159.36934, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 76us/sample - loss: 94177.5588 - mean_squared_error: 94177.5703 - mean_absolute_error: 165.6912 - val_loss: 84159.3693 - val_mean_squared_error: 84159.3594 - val_mean_absolute_error: 161.4883\n",
      "Epoch 90/2000\n",
      "4608/4858 [===========================>..] - ETA: 0s - loss: 93193.0309 - mean_squared_error: 93193.0156 - mean_absolute_error: 166.8388\n",
      "Epoch 00090: val_loss improved from 84159.36934 to 83966.56430, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 84us/sample - loss: 93934.9260 - mean_squared_error: 93934.9141 - mean_absolute_error: 166.6970 - val_loss: 83966.5643 - val_mean_squared_error: 83966.5547 - val_mean_absolute_error: 157.1552\n",
      "Epoch 91/2000\n",
      "4352/4858 [=========================>....] - ETA: 0s - loss: 91293.7498 - mean_squared_error: 91293.7266 - mean_absolute_error: 165.8352\n",
      "Epoch 00091: val_loss improved from 83966.56430 to 83780.46206, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 83us/sample - loss: 93776.0382 - mean_squared_error: 93776.0234 - mean_absolute_error: 165.6315 - val_loss: 83780.4621 - val_mean_squared_error: 83780.4688 - val_mean_absolute_error: 158.2481\n",
      "Epoch 92/2000\n",
      "3712/4858 [=====================>........] - ETA: 0s - loss: 96666.8173 - mean_squared_error: 96666.7969 - mean_absolute_error: 169.0014\n",
      "Epoch 00092: val_loss improved from 83780.46206 to 83602.52672, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 69us/sample - loss: 93570.6712 - mean_squared_error: 93570.6562 - mean_absolute_error: 166.2408 - val_loss: 83602.5267 - val_mean_squared_error: 83602.5312 - val_mean_absolute_error: 157.4216\n",
      "Epoch 93/2000\n",
      "4160/4858 [========================>.....] - ETA: 0s - loss: 92302.2003 - mean_squared_error: 92302.1719 - mean_absolute_error: 162.8971\n",
      "Epoch 00093: val_loss did not improve from 83602.52672\n",
      "4858/4858 [==============================] - 0s 77us/sample - loss: 93507.8146 - mean_squared_error: 93507.7891 - mean_absolute_error: 164.2379 - val_loss: 83664.0369 - val_mean_squared_error: 83664.0234 - val_mean_absolute_error: 162.4654\n",
      "Epoch 94/2000\n",
      "4416/4858 [==========================>...] - ETA: 0s - loss: 94616.6099 - mean_squared_error: 94616.6094 - mean_absolute_error: 166.2634\n",
      "Epoch 00094: val_loss improved from 83602.52672 to 83240.17086, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 72us/sample - loss: 93273.5318 - mean_squared_error: 93273.5312 - mean_absolute_error: 166.1252 - val_loss: 83240.1709 - val_mean_squared_error: 83240.1719 - val_mean_absolute_error: 158.8030\n",
      "Epoch 95/2000\n",
      "4640/4858 [===========================>..] - ETA: 0s - loss: 86809.4137 - mean_squared_error: 86809.3828 - mean_absolute_error: 163.4072\n",
      "Epoch 00095: val_loss improved from 83240.17086 to 83163.71828, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 89us/sample - loss: 93159.3665 - mean_squared_error: 93159.3359 - mean_absolute_error: 164.9911 - val_loss: 83163.7183 - val_mean_squared_error: 83163.7188 - val_mean_absolute_error: 156.7822\n",
      "Epoch 96/2000\n",
      "4672/4858 [===========================>..] - ETA: 0s - loss: 93442.3334 - mean_squared_error: 93442.3047 - mean_absolute_error: 165.5297\n",
      "Epoch 00096: val_loss improved from 83163.71828 to 82996.10825, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 57us/sample - loss: 92899.3141 - mean_squared_error: 92899.2812 - mean_absolute_error: 164.9288 - val_loss: 82996.1083 - val_mean_squared_error: 82996.1172 - val_mean_absolute_error: 154.9265\n",
      "Epoch 97/2000\n",
      "4416/4858 [==========================>...] - ETA: 0s - loss: 93819.4031 - mean_squared_error: 93819.3906 - mean_absolute_error: 165.0587\n",
      "Epoch 00097: val_loss improved from 82996.10825 to 82790.77844, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 92us/sample - loss: 92747.4099 - mean_squared_error: 92747.3906 - mean_absolute_error: 165.4181 - val_loss: 82790.7784 - val_mean_squared_error: 82790.7734 - val_mean_absolute_error: 157.7381\n",
      "Epoch 98/2000\n",
      "4544/4858 [===========================>..] - ETA: 0s - loss: 88565.2484 - mean_squared_error: 88565.2266 - mean_absolute_error: 163.7895\n",
      "Epoch 00098: val_loss improved from 82790.77844 to 82780.88490, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 75us/sample - loss: 92574.1464 - mean_squared_error: 92574.1328 - mean_absolute_error: 164.2751 - val_loss: 82780.8849 - val_mean_squared_error: 82780.8750 - val_mean_absolute_error: 152.8664\n",
      "Epoch 99/2000\n",
      "4544/4858 [===========================>..] - ETA: 0s - loss: 86238.1381 - mean_squared_error: 86238.1094 - mean_absolute_error: 162.1886\n",
      "Epoch 00099: val_loss improved from 82780.88490 to 82431.18272, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 58us/sample - loss: 92385.4221 - mean_squared_error: 92385.3906 - mean_absolute_error: 163.7350 - val_loss: 82431.1827 - val_mean_squared_error: 82431.1797 - val_mean_absolute_error: 157.1609\n",
      "Epoch 100/2000\n",
      "4384/4858 [==========================>...] - ETA: 0s - loss: 93332.9289 - mean_squared_error: 93332.9375 - mean_absolute_error: 162.8970\n",
      "Epoch 00100: val_loss improved from 82431.18272 to 82207.98401, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 90us/sample - loss: 92271.6578 - mean_squared_error: 92271.6641 - mean_absolute_error: 164.2211 - val_loss: 82207.9840 - val_mean_squared_error: 82207.9844 - val_mean_absolute_error: 158.4100\n",
      "Epoch 101/2000\n",
      "4256/4858 [=========================>....] - ETA: 0s - loss: 95491.5487 - mean_squared_error: 95491.5625 - mean_absolute_error: 164.6397\n",
      "Epoch 00101: val_loss improved from 82207.98401 to 82064.86902, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 78us/sample - loss: 91982.9121 - mean_squared_error: 91982.9141 - mean_absolute_error: 163.7450 - val_loss: 82064.8690 - val_mean_squared_error: 82064.8906 - val_mean_absolute_error: 157.7655\n",
      "Epoch 102/2000\n",
      "4672/4858 [===========================>..] - ETA: 0s - loss: 92414.8887 - mean_squared_error: 92414.8750 - mean_absolute_error: 164.3878  \n",
      "Epoch 00102: val_loss improved from 82064.86902 to 81889.70892, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 43us/sample - loss: 91853.1398 - mean_squared_error: 91853.1250 - mean_absolute_error: 164.1380 - val_loss: 81889.7089 - val_mean_squared_error: 81889.6953 - val_mean_absolute_error: 155.0771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/2000\n",
      "4288/4858 [=========================>....] - ETA: 0s - loss: 90393.5256 - mean_squared_error: 90393.5469 - mean_absolute_error: 160.8369\n",
      "Epoch 00103: val_loss improved from 81889.70892 to 81734.37186, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 50us/sample - loss: 91599.3559 - mean_squared_error: 91599.3828 - mean_absolute_error: 162.6653 - val_loss: 81734.3719 - val_mean_squared_error: 81734.3594 - val_mean_absolute_error: 158.1593\n",
      "Epoch 104/2000\n",
      "4320/4858 [=========================>....] - ETA: 0s - loss: 94608.0241 - mean_squared_error: 94608.0391 - mean_absolute_error: 164.9385\n",
      "Epoch 00104: val_loss improved from 81734.37186 to 81476.80983, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 61us/sample - loss: 91489.8935 - mean_squared_error: 91489.9062 - mean_absolute_error: 163.7153 - val_loss: 81476.8098 - val_mean_squared_error: 81476.7969 - val_mean_absolute_error: 153.6550\n",
      "Epoch 105/2000\n",
      "4096/4858 [========================>.....] - ETA: 0s - loss: 95356.0487 - mean_squared_error: 95356.0547 - mean_absolute_error: 164.5513\n",
      "Epoch 00105: val_loss improved from 81476.80983 to 81440.33560, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 74us/sample - loss: 91158.9313 - mean_squared_error: 91158.9531 - mean_absolute_error: 163.0567 - val_loss: 81440.3356 - val_mean_squared_error: 81440.3516 - val_mean_absolute_error: 150.6109\n",
      "Epoch 106/2000\n",
      "4032/4858 [=======================>......] - ETA: 0s - loss: 96684.9735 - mean_squared_error: 96684.9531 - mean_absolute_error: 162.2456\n",
      "Epoch 00106: val_loss improved from 81440.33560 to 81151.29343, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 76us/sample - loss: 91088.9526 - mean_squared_error: 91088.9297 - mean_absolute_error: 161.8071 - val_loss: 81151.2934 - val_mean_squared_error: 81151.3047 - val_mean_absolute_error: 152.3784\n",
      "Epoch 107/2000\n",
      "4416/4858 [==========================>...] - ETA: 0s - loss: 90894.5454 - mean_squared_error: 90894.5312 - mean_absolute_error: 162.6040\n",
      "Epoch 00107: val_loss improved from 81151.29343 to 81033.31476, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 74us/sample - loss: 90870.4433 - mean_squared_error: 90870.4375 - mean_absolute_error: 162.5572 - val_loss: 81033.3148 - val_mean_squared_error: 81033.3203 - val_mean_absolute_error: 153.3528\n",
      "Epoch 108/2000\n",
      "4768/4858 [============================>.] - ETA: 0s - loss: 90563.3836 - mean_squared_error: 90563.3828 - mean_absolute_error: 160.5465\n",
      "Epoch 00108: val_loss improved from 81033.31476 to 80868.10814, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 43us/sample - loss: 90622.6891 - mean_squared_error: 90622.6875 - mean_absolute_error: 160.9481 - val_loss: 80868.1081 - val_mean_squared_error: 80868.1094 - val_mean_absolute_error: 153.9953\n",
      "Epoch 109/2000\n",
      "4704/4858 [============================>.] - ETA: 0s - loss: 91241.3139 - mean_squared_error: 91241.3281 - mean_absolute_error: 161.9574\n",
      "Epoch 00109: val_loss improved from 80868.10814 to 80680.76157, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 46us/sample - loss: 90393.4467 - mean_squared_error: 90393.4609 - mean_absolute_error: 161.7993 - val_loss: 80680.7616 - val_mean_squared_error: 80680.7656 - val_mean_absolute_error: 153.7183\n",
      "Epoch 110/2000\n",
      "4768/4858 [============================>.] - ETA: 0s - loss: 90191.3688 - mean_squared_error: 90191.3594 - mean_absolute_error: 160.1673\n",
      "Epoch 00110: val_loss improved from 80680.76157 to 80573.89270, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 60us/sample - loss: 90284.8061 - mean_squared_error: 90284.7891 - mean_absolute_error: 160.7289 - val_loss: 80573.8927 - val_mean_squared_error: 80573.9141 - val_mean_absolute_error: 153.4513\n",
      "Epoch 111/2000\n",
      "4768/4858 [============================>.] - ETA: 0s - loss: 90857.0149 - mean_squared_error: 90857.0547 - mean_absolute_error: 161.4739\n",
      "Epoch 00111: val_loss improved from 80573.89270 to 80461.78476, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 74us/sample - loss: 90033.9170 - mean_squared_error: 90033.9531 - mean_absolute_error: 160.9576 - val_loss: 80461.7848 - val_mean_squared_error: 80461.7734 - val_mean_absolute_error: 152.7551\n",
      "Epoch 112/2000\n",
      "4192/4858 [========================>.....] - ETA: 0s - loss: 87693.3903 - mean_squared_error: 87693.3828 - mean_absolute_error: 160.0501\n",
      "Epoch 00112: val_loss improved from 80461.78476 to 80413.39198, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 63us/sample - loss: 89923.7800 - mean_squared_error: 89923.7969 - mean_absolute_error: 160.1594 - val_loss: 80413.3920 - val_mean_squared_error: 80413.3828 - val_mean_absolute_error: 154.1880\n",
      "Epoch 113/2000\n",
      "4640/4858 [===========================>..] - ETA: 0s - loss: 90329.1017 - mean_squared_error: 90329.1328 - mean_absolute_error: 160.0566\n",
      "Epoch 00113: val_loss improved from 80413.39198 to 80341.36820, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 79us/sample - loss: 89699.1637 - mean_squared_error: 89699.1953 - mean_absolute_error: 160.4608 - val_loss: 80341.3682 - val_mean_squared_error: 80341.3828 - val_mean_absolute_error: 156.0847\n",
      "Epoch 114/2000\n",
      "4288/4858 [=========================>....] - ETA: 0s - loss: 93700.1701 - mean_squared_error: 93700.1562 - mean_absolute_error: 162.4150\n",
      "Epoch 00114: val_loss improved from 80341.36820 to 80139.63822, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 81us/sample - loss: 89431.3967 - mean_squared_error: 89431.3984 - mean_absolute_error: 160.4139 - val_loss: 80139.6382 - val_mean_squared_error: 80139.6250 - val_mean_absolute_error: 150.8194\n",
      "Epoch 115/2000\n",
      "4128/4858 [========================>.....] - ETA: 0s - loss: 85668.3921 - mean_squared_error: 85668.4141 - mean_absolute_error: 156.0284\n",
      "Epoch 00115: val_loss improved from 80139.63822 to 80001.45737, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 69us/sample - loss: 89246.1155 - mean_squared_error: 89246.1406 - mean_absolute_error: 158.3067 - val_loss: 80001.4574 - val_mean_squared_error: 80001.4609 - val_mean_absolute_error: 154.0558\n",
      "Epoch 116/2000\n",
      "4608/4858 [===========================>..] - ETA: 0s - loss: 89366.4012 - mean_squared_error: 89366.3750 - mean_absolute_error: 159.4031\n",
      "Epoch 00116: val_loss improved from 80001.45737 to 79903.70584, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 59us/sample - loss: 89139.2519 - mean_squared_error: 89139.2344 - mean_absolute_error: 159.5577 - val_loss: 79903.7058 - val_mean_squared_error: 79903.6953 - val_mean_absolute_error: 153.8077\n",
      "Epoch 117/2000\n",
      "4256/4858 [=========================>....] - ETA: 0s - loss: 92155.7789 - mean_squared_error: 92155.7734 - mean_absolute_error: 160.6446\n",
      "Epoch 00117: val_loss improved from 79903.70584 to 79724.11419, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 89us/sample - loss: 88936.6353 - mean_squared_error: 88936.6172 - mean_absolute_error: 159.8055 - val_loss: 79724.1142 - val_mean_squared_error: 79724.1250 - val_mean_absolute_error: 149.7813\n",
      "Epoch 118/2000\n",
      "4320/4858 [=========================>....] - ETA: 0s - loss: 88071.7235 - mean_squared_error: 88071.7266 - mean_absolute_error: 157.9199\n",
      "Epoch 00118: val_loss did not improve from 79724.11419\n",
      "4858/4858 [==============================] - 0s 42us/sample - loss: 88733.8660 - mean_squared_error: 88733.8672 - mean_absolute_error: 157.7795 - val_loss: 79759.6267 - val_mean_squared_error: 79759.6250 - val_mean_absolute_error: 154.4579\n",
      "Epoch 119/2000\n",
      "4608/4858 [===========================>..] - ETA: 0s - loss: 89082.1780 - mean_squared_error: 89082.1953 - mean_absolute_error: 159.8705\n",
      "Epoch 00119: val_loss improved from 79724.11419 to 79450.89156, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 88us/sample - loss: 88613.6453 - mean_squared_error: 88613.6641 - mean_absolute_error: 159.2069 - val_loss: 79450.8916 - val_mean_squared_error: 79450.8906 - val_mean_absolute_error: 148.5167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/2000\n",
      "4576/4858 [===========================>..] - ETA: 0s - loss: 86914.8266 - mean_squared_error: 86914.8516 - mean_absolute_error: 155.6330\n",
      "Epoch 00120: val_loss did not improve from 79450.89156\n",
      "4858/4858 [==============================] - 0s 81us/sample - loss: 88410.3221 - mean_squared_error: 88410.3516 - mean_absolute_error: 156.8019 - val_loss: 79514.7664 - val_mean_squared_error: 79514.7734 - val_mean_absolute_error: 155.4586\n",
      "Epoch 121/2000\n",
      "4256/4858 [=========================>....] - ETA: 0s - loss: 88707.3766 - mean_squared_error: 88707.3750 - mean_absolute_error: 157.8843\n",
      "Epoch 00121: val_loss improved from 79450.89156 to 79171.25517, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 74us/sample - loss: 88231.9902 - mean_squared_error: 88231.9844 - mean_absolute_error: 158.7868 - val_loss: 79171.2552 - val_mean_squared_error: 79171.2422 - val_mean_absolute_error: 150.9454\n",
      "Epoch 122/2000\n",
      "4160/4858 [========================>.....] - ETA: 0s - loss: 88994.5991 - mean_squared_error: 88994.5781 - mean_absolute_error: 156.9383\n",
      "Epoch 00122: val_loss improved from 79171.25517 to 79075.47024, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 76us/sample - loss: 88076.0326 - mean_squared_error: 88076.0234 - mean_absolute_error: 157.5680 - val_loss: 79075.4702 - val_mean_squared_error: 79075.4609 - val_mean_absolute_error: 149.9150\n",
      "Epoch 123/2000\n",
      "4096/4858 [========================>.....] - ETA: 0s - loss: 83509.3442 - mean_squared_error: 83509.3594 - mean_absolute_error: 157.2956\n",
      "Epoch 00123: val_loss improved from 79075.47024 to 78898.37823, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 86us/sample - loss: 87874.1972 - mean_squared_error: 87874.2188 - mean_absolute_error: 157.1524 - val_loss: 78898.3782 - val_mean_squared_error: 78898.3672 - val_mean_absolute_error: 150.9007\n",
      "Epoch 124/2000\n",
      "4640/4858 [===========================>..] - ETA: 0s - loss: 86993.3017 - mean_squared_error: 86993.3047 - mean_absolute_error: 156.6734\n",
      "Epoch 00124: val_loss improved from 78898.37823 to 78807.24505, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 88us/sample - loss: 87831.6699 - mean_squared_error: 87831.6719 - mean_absolute_error: 157.3766 - val_loss: 78807.2450 - val_mean_squared_error: 78807.2344 - val_mean_absolute_error: 150.2794\n",
      "Epoch 125/2000\n",
      "4160/4858 [========================>.....] - ETA: 0s - loss: 86336.5223 - mean_squared_error: 86336.5391 - mean_absolute_error: 154.9114\n",
      "Epoch 00125: val_loss did not improve from 78807.24505\n",
      "4858/4858 [==============================] - 0s 80us/sample - loss: 87505.2706 - mean_squared_error: 87505.2969 - mean_absolute_error: 156.3948 - val_loss: 78886.3794 - val_mean_squared_error: 78886.3828 - val_mean_absolute_error: 155.7077\n",
      "Epoch 126/2000\n",
      "4384/4858 [==========================>...] - ETA: 0s - loss: 88057.7297 - mean_squared_error: 88057.7188 - mean_absolute_error: 158.2989\n",
      "Epoch 00126: val_loss improved from 78807.24505 to 78470.85829, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 81us/sample - loss: 87470.5540 - mean_squared_error: 87470.5547 - mean_absolute_error: 157.4713 - val_loss: 78470.8583 - val_mean_squared_error: 78470.8594 - val_mean_absolute_error: 148.7885\n",
      "Epoch 127/2000\n",
      "4384/4858 [==========================>...] - ETA: 0s - loss: 85493.6250 - mean_squared_error: 85493.6172 - mean_absolute_error: 154.4635\n",
      "Epoch 00127: val_loss improved from 78470.85829 to 78239.30369, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 83us/sample - loss: 87255.9614 - mean_squared_error: 87255.9531 - mean_absolute_error: 156.2781 - val_loss: 78239.3037 - val_mean_squared_error: 78239.3125 - val_mean_absolute_error: 149.9794\n",
      "Epoch 128/2000\n",
      "4832/4858 [============================>.] - ETA: 0s - loss: 86711.8644 - mean_squared_error: 86711.8672 - mean_absolute_error: 156.8392\n",
      "Epoch 00128: val_loss improved from 78239.30369 to 78222.85651, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 85us/sample - loss: 87174.9842 - mean_squared_error: 87174.9922 - mean_absolute_error: 157.0218 - val_loss: 78222.8565 - val_mean_squared_error: 78222.8672 - val_mean_absolute_error: 148.8386\n",
      "Epoch 129/2000\n",
      "4000/4858 [=======================>......] - ETA: 0s - loss: 80479.6224 - mean_squared_error: 80479.6172 - mean_absolute_error: 154.4302\n",
      "Epoch 00129: val_loss improved from 78222.85651 to 78016.73424, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 79us/sample - loss: 86943.4279 - mean_squared_error: 86943.4219 - mean_absolute_error: 155.7206 - val_loss: 78016.7342 - val_mean_squared_error: 78016.7266 - val_mean_absolute_error: 152.3270\n",
      "Epoch 130/2000\n",
      "4800/4858 [============================>.] - ETA: 0s - loss: 86880.0927 - mean_squared_error: 86880.0781 - mean_absolute_error: 155.1536\n",
      "Epoch 00130: val_loss did not improve from 78016.73424\n",
      "4858/4858 [==============================] - 0s 80us/sample - loss: 86804.4963 - mean_squared_error: 86804.4766 - mean_absolute_error: 155.6266 - val_loss: 78135.9462 - val_mean_squared_error: 78135.9375 - val_mean_absolute_error: 154.9062\n",
      "Epoch 131/2000\n",
      "4672/4858 [===========================>..] - ETA: 0s - loss: 86084.8480 - mean_squared_error: 86084.8438 - mean_absolute_error: 156.3645\n",
      "Epoch 00131: val_loss improved from 78016.73424 to 77797.90223, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 88us/sample - loss: 86706.0715 - mean_squared_error: 86706.0703 - mean_absolute_error: 156.1656 - val_loss: 77797.9022 - val_mean_squared_error: 77797.9141 - val_mean_absolute_error: 151.8401\n",
      "Epoch 132/2000\n",
      "4704/4858 [============================>.] - ETA: 0s - loss: 86365.3378 - mean_squared_error: 86365.3594 - mean_absolute_error: 154.2222\n",
      "Epoch 00132: val_loss improved from 77797.90223 to 77746.10978, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 89us/sample - loss: 86625.7285 - mean_squared_error: 86625.7500 - mean_absolute_error: 155.0614 - val_loss: 77746.1098 - val_mean_squared_error: 77746.1094 - val_mean_absolute_error: 151.3607\n",
      "Epoch 133/2000\n",
      "4576/4858 [===========================>..] - ETA: 0s - loss: 88371.9831 - mean_squared_error: 88371.9688 - mean_absolute_error: 156.6152\n",
      "Epoch 00133: val_loss improved from 77746.10978 to 77475.53438, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 88us/sample - loss: 86487.6824 - mean_squared_error: 86487.6719 - mean_absolute_error: 156.3919 - val_loss: 77475.5344 - val_mean_squared_error: 77475.5312 - val_mean_absolute_error: 149.6691\n",
      "Epoch 134/2000\n",
      "3744/4858 [======================>.......] - ETA: 0s - loss: 76624.1119 - mean_squared_error: 76624.1172 - mean_absolute_error: 149.0522\n",
      "Epoch 00134: val_loss did not improve from 77475.53438\n",
      "4858/4858 [==============================] - 0s 57us/sample - loss: 86286.7999 - mean_squared_error: 86286.7891 - mean_absolute_error: 154.6815 - val_loss: 77573.3464 - val_mean_squared_error: 77573.3438 - val_mean_absolute_error: 153.0387\n",
      "Epoch 135/2000\n",
      "4800/4858 [============================>.] - ETA: 0s - loss: 86697.6864 - mean_squared_error: 86697.6719 - mean_absolute_error: 156.1756\n",
      "Epoch 00135: val_loss improved from 77475.53438 to 77233.68962, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 73us/sample - loss: 86262.1675 - mean_squared_error: 86262.1484 - mean_absolute_error: 155.8260 - val_loss: 77233.6896 - val_mean_squared_error: 77233.6797 - val_mean_absolute_error: 145.4527\n",
      "Epoch 136/2000\n",
      "4448/4858 [==========================>...] - ETA: 0s - loss: 89468.0322 - mean_squared_error: 89468.0469 - mean_absolute_error: 155.7332\n",
      "Epoch 00136: val_loss improved from 77233.68962 to 77072.76415, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 79us/sample - loss: 86198.3922 - mean_squared_error: 86198.4062 - mean_absolute_error: 155.3943 - val_loss: 77072.7641 - val_mean_squared_error: 77072.7500 - val_mean_absolute_error: 148.6015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/2000\n",
      "4384/4858 [==========================>...] - ETA: 0s - loss: 87906.7208 - mean_squared_error: 87906.7188 - mean_absolute_error: 155.8301\n",
      "Epoch 00137: val_loss improved from 77072.76415 to 76922.12512, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 79us/sample - loss: 85938.0805 - mean_squared_error: 85938.0781 - mean_absolute_error: 154.4649 - val_loss: 76922.1251 - val_mean_squared_error: 76922.1172 - val_mean_absolute_error: 147.8637\n",
      "Epoch 138/2000\n",
      "4800/4858 [============================>.] - ETA: 0s - loss: 86144.1120 - mean_squared_error: 86144.1328 - mean_absolute_error: 153.7181\n",
      "Epoch 00138: val_loss did not improve from 76922.12512\n",
      "4858/4858 [==============================] - 0s 80us/sample - loss: 85713.8352 - mean_squared_error: 85713.8594 - mean_absolute_error: 153.7601 - val_loss: 77340.9830 - val_mean_squared_error: 77340.9688 - val_mean_absolute_error: 154.2962\n",
      "Epoch 139/2000\n",
      "4480/4858 [==========================>...] - ETA: 0s - loss: 89256.7642 - mean_squared_error: 89256.7344 - mean_absolute_error: 155.3982\n",
      "Epoch 00139: val_loss improved from 76922.12512 to 76782.60900, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 82us/sample - loss: 85733.8739 - mean_squared_error: 85733.8516 - mean_absolute_error: 154.5813 - val_loss: 76782.6090 - val_mean_squared_error: 76782.6172 - val_mean_absolute_error: 149.5532\n",
      "Epoch 140/2000\n",
      "4096/4858 [========================>.....] - ETA: 0s - loss: 80940.6351 - mean_squared_error: 80940.6172 - mean_absolute_error: 152.7439\n",
      "Epoch 00140: val_loss improved from 76782.60900 to 76720.55977, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 82us/sample - loss: 85587.0994 - mean_squared_error: 85587.0859 - mean_absolute_error: 153.7755 - val_loss: 76720.5598 - val_mean_squared_error: 76720.5703 - val_mean_absolute_error: 148.1881\n",
      "Epoch 141/2000\n",
      "4576/4858 [===========================>..] - ETA: 0s - loss: 87855.3388 - mean_squared_error: 87855.3359 - mean_absolute_error: 154.6339\n",
      "Epoch 00141: val_loss did not improve from 76720.55977\n",
      "4858/4858 [==============================] - 0s 72us/sample - loss: 85645.9916 - mean_squared_error: 85645.9922 - mean_absolute_error: 154.1914 - val_loss: 76806.7195 - val_mean_squared_error: 76806.7188 - val_mean_absolute_error: 152.3431\n",
      "Epoch 142/2000\n",
      "4608/4858 [===========================>..] - ETA: 0s - loss: 85435.1593 - mean_squared_error: 85435.1484 - mean_absolute_error: 155.1322\n",
      "Epoch 00142: val_loss improved from 76720.55977 to 76485.02384, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 66us/sample - loss: 85360.1556 - mean_squared_error: 85360.1484 - mean_absolute_error: 154.4057 - val_loss: 76485.0238 - val_mean_squared_error: 76485.0078 - val_mean_absolute_error: 143.6245\n",
      "Epoch 143/2000\n",
      "4256/4858 [=========================>....] - ETA: 0s - loss: 77613.7056 - mean_squared_error: 77613.7266 - mean_absolute_error: 150.9982\n",
      "Epoch 00143: val_loss improved from 76485.02384 to 76368.99556, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 84us/sample - loss: 85259.2778 - mean_squared_error: 85259.2969 - mean_absolute_error: 152.6893 - val_loss: 76368.9956 - val_mean_squared_error: 76368.9922 - val_mean_absolute_error: 144.9219\n",
      "Epoch 144/2000\n",
      "4192/4858 [========================>.....] - ETA: 0s - loss: 83170.6129 - mean_squared_error: 83170.6328 - mean_absolute_error: 151.1615\n",
      "Epoch 00144: val_loss improved from 76368.99556 to 76349.98281, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 81us/sample - loss: 85090.1759 - mean_squared_error: 85090.1953 - mean_absolute_error: 152.9629 - val_loss: 76349.9828 - val_mean_squared_error: 76349.9766 - val_mean_absolute_error: 150.7405\n",
      "Epoch 145/2000\n",
      "4320/4858 [=========================>....] - ETA: 0s - loss: 85528.3278 - mean_squared_error: 85528.3203 - mean_absolute_error: 153.2451\n",
      "Epoch 00145: val_loss improved from 76349.98281 to 76143.50411, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 70us/sample - loss: 85137.8570 - mean_squared_error: 85137.8516 - mean_absolute_error: 153.2804 - val_loss: 76143.5041 - val_mean_squared_error: 76143.5156 - val_mean_absolute_error: 143.4781\n",
      "Epoch 146/2000\n",
      "4608/4858 [===========================>..] - ETA: 0s - loss: 86511.3464 - mean_squared_error: 86511.3438 - mean_absolute_error: 153.2094\n",
      "Epoch 00146: val_loss improved from 76143.50411 to 75961.22031, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 89us/sample - loss: 84995.3988 - mean_squared_error: 84995.3984 - mean_absolute_error: 152.8641 - val_loss: 75961.2203 - val_mean_squared_error: 75961.2266 - val_mean_absolute_error: 146.6291\n",
      "Epoch 147/2000\n",
      "4640/4858 [===========================>..] - ETA: 0s - loss: 85108.7842 - mean_squared_error: 85108.7422 - mean_absolute_error: 151.7364\n",
      "Epoch 00147: val_loss did not improve from 75961.22031\n",
      "4858/4858 [==============================] - 0s 86us/sample - loss: 84875.2752 - mean_squared_error: 84875.2344 - mean_absolute_error: 152.4501 - val_loss: 76342.8803 - val_mean_squared_error: 76342.8828 - val_mean_absolute_error: 153.2720\n",
      "Epoch 148/2000\n",
      "4672/4858 [===========================>..] - ETA: 0s - loss: 85371.2320 - mean_squared_error: 85371.2266 - mean_absolute_error: 152.1745\n",
      "Epoch 00148: val_loss did not improve from 75961.22031\n",
      "4858/4858 [==============================] - 0s 64us/sample - loss: 84997.1069 - mean_squared_error: 84997.1016 - mean_absolute_error: 152.5264 - val_loss: 76102.0814 - val_mean_squared_error: 76102.0859 - val_mean_absolute_error: 151.0107\n",
      "Epoch 149/2000\n",
      "4576/4858 [===========================>..] - ETA: 0s - loss: 84866.8455 - mean_squared_error: 84866.8438 - mean_absolute_error: 152.4210\n",
      "Epoch 00149: val_loss did not improve from 75961.22031\n",
      "4858/4858 [==============================] - 0s 85us/sample - loss: 84774.1803 - mean_squared_error: 84774.1797 - mean_absolute_error: 152.8417 - val_loss: 76344.2094 - val_mean_squared_error: 76344.2109 - val_mean_absolute_error: 153.6659\n",
      "Epoch 150/2000\n",
      "4064/4858 [========================>.....] - ETA: 0s - loss: 82304.1171 - mean_squared_error: 82304.1328 - mean_absolute_error: 153.3533\n",
      "Epoch 00150: val_loss improved from 75961.22031 to 75606.83677, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 76us/sample - loss: 84753.6807 - mean_squared_error: 84753.6953 - mean_absolute_error: 152.9433 - val_loss: 75606.8368 - val_mean_squared_error: 75606.8359 - val_mean_absolute_error: 146.0748\n",
      "Epoch 151/2000\n",
      "4096/4858 [========================>.....] - ETA: 0s - loss: 89401.9429 - mean_squared_error: 89401.9375 - mean_absolute_error: 155.2392\n",
      "Epoch 00151: val_loss did not improve from 75606.83677\n",
      "4858/4858 [==============================] - 0s 79us/sample - loss: 84617.4065 - mean_squared_error: 84617.3828 - mean_absolute_error: 153.6454 - val_loss: 75811.8279 - val_mean_squared_error: 75811.8203 - val_mean_absolute_error: 140.5430\n",
      "Epoch 152/2000\n",
      "4480/4858 [==========================>...] - ETA: 0s - loss: 81381.6069 - mean_squared_error: 81381.6250 - mean_absolute_error: 148.3536\n",
      "Epoch 00152: val_loss improved from 75606.83677 to 75543.38747, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 93us/sample - loss: 84482.4044 - mean_squared_error: 84482.4141 - mean_absolute_error: 150.3880 - val_loss: 75543.3875 - val_mean_squared_error: 75543.3984 - val_mean_absolute_error: 145.7986\n",
      "Epoch 153/2000\n",
      "4736/4858 [============================>.] - ETA: 0s - loss: 85146.0537 - mean_squared_error: 85146.0391 - mean_absolute_error: 153.0182\n",
      "Epoch 00153: val_loss improved from 75543.38747 to 75462.50546, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 79us/sample - loss: 84461.4496 - mean_squared_error: 84461.4453 - mean_absolute_error: 153.2298 - val_loss: 75462.5055 - val_mean_squared_error: 75462.5156 - val_mean_absolute_error: 146.1439\n",
      "Epoch 154/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4608/4858 [===========================>..] - ETA: 0s - loss: 85811.4620 - mean_squared_error: 85811.4688 - mean_absolute_error: 151.7368\n",
      "Epoch 00154: val_loss improved from 75462.50546 to 75414.62474, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 73us/sample - loss: 84293.3190 - mean_squared_error: 84293.3203 - mean_absolute_error: 151.8005 - val_loss: 75414.6247 - val_mean_squared_error: 75414.6094 - val_mean_absolute_error: 149.0346\n",
      "Epoch 155/2000\n",
      "4224/4858 [=========================>....] - ETA: 0s - loss: 84367.5610 - mean_squared_error: 84367.5781 - mean_absolute_error: 151.0265\n",
      "Epoch 00155: val_loss improved from 75414.62474 to 75359.74026, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 82us/sample - loss: 84244.0014 - mean_squared_error: 84244.0234 - mean_absolute_error: 151.3932 - val_loss: 75359.7403 - val_mean_squared_error: 75359.7344 - val_mean_absolute_error: 147.4402\n",
      "Epoch 156/2000\n",
      "4544/4858 [===========================>..] - ETA: 0s - loss: 86235.9499 - mean_squared_error: 86235.9453 - mean_absolute_error: 152.3702\n",
      "Epoch 00156: val_loss did not improve from 75359.74026\n",
      "4858/4858 [==============================] - 0s 72us/sample - loss: 84228.6234 - mean_squared_error: 84228.6250 - mean_absolute_error: 151.7401 - val_loss: 75411.5943 - val_mean_squared_error: 75411.5938 - val_mean_absolute_error: 149.4829\n",
      "Epoch 157/2000\n",
      "3744/4858 [======================>.......] - ETA: 0s - loss: 69895.5551 - mean_squared_error: 69895.5547 - mean_absolute_error: 146.4489\n",
      "Epoch 00157: val_loss did not improve from 75359.74026\n",
      "4858/4858 [==============================] - 0s 61us/sample - loss: 83991.3456 - mean_squared_error: 83991.3359 - mean_absolute_error: 150.7261 - val_loss: 75660.7346 - val_mean_squared_error: 75660.7266 - val_mean_absolute_error: 152.7136\n",
      "Epoch 158/2000\n",
      "4800/4858 [============================>.] - ETA: 0s - loss: 84540.6557 - mean_squared_error: 84540.6328 - mean_absolute_error: 152.2345\n",
      "Epoch 00158: val_loss improved from 75359.74026 to 75040.50854, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 73us/sample - loss: 83991.3746 - mean_squared_error: 83991.3516 - mean_absolute_error: 151.8300 - val_loss: 75040.5085 - val_mean_squared_error: 75040.5078 - val_mean_absolute_error: 143.5056\n",
      "Epoch 159/2000\n",
      "4064/4858 [========================>.....] - ETA: 0s - loss: 87202.2674 - mean_squared_error: 87202.2578 - mean_absolute_error: 151.9553\n",
      "Epoch 00159: val_loss improved from 75040.50854 to 74816.83366, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 83us/sample - loss: 83962.8314 - mean_squared_error: 83962.8359 - mean_absolute_error: 151.6734 - val_loss: 74816.8337 - val_mean_squared_error: 74816.8281 - val_mean_absolute_error: 143.4574\n",
      "Epoch 160/2000\n",
      "4096/4858 [========================>.....] - ETA: 0s - loss: 84033.3590 - mean_squared_error: 84033.3516 - mean_absolute_error: 149.8392\n",
      "Epoch 00160: val_loss did not improve from 74816.83366\n",
      "4858/4858 [==============================] - 0s 79us/sample - loss: 83858.9591 - mean_squared_error: 83858.9609 - mean_absolute_error: 150.0013 - val_loss: 75307.1063 - val_mean_squared_error: 75307.1016 - val_mean_absolute_error: 151.1361\n",
      "Epoch 161/2000\n",
      "4032/4858 [=======================>......] - ETA: 0s - loss: 84572.2076 - mean_squared_error: 84572.2188 - mean_absolute_error: 152.6131\n",
      "Epoch 00161: val_loss improved from 74816.83366 to 74797.56445, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 87us/sample - loss: 83928.8931 - mean_squared_error: 83928.9062 - mean_absolute_error: 152.0796 - val_loss: 74797.5644 - val_mean_squared_error: 74797.5703 - val_mean_absolute_error: 143.8510\n",
      "Epoch 162/2000\n",
      "4640/4858 [===========================>..] - ETA: 0s - loss: 84182.0613 - mean_squared_error: 84182.0234 - mean_absolute_error: 149.8920\n",
      "Epoch 00162: val_loss did not improve from 74797.56445\n",
      "4858/4858 [==============================] - 0s 85us/sample - loss: 83665.2599 - mean_squared_error: 83665.2109 - mean_absolute_error: 150.3341 - val_loss: 75043.2577 - val_mean_squared_error: 75043.2656 - val_mean_absolute_error: 149.0844\n",
      "Epoch 163/2000\n",
      "4416/4858 [==========================>...] - ETA: 0s - loss: 84488.8617 - mean_squared_error: 84488.8594 - mean_absolute_error: 152.8976\n",
      "Epoch 00163: val_loss did not improve from 74797.56445\n",
      "4858/4858 [==============================] - 0s 74us/sample - loss: 83738.1809 - mean_squared_error: 83738.1875 - mean_absolute_error: 151.8139 - val_loss: 75047.7003 - val_mean_squared_error: 75047.6875 - val_mean_absolute_error: 138.6261\n",
      "Epoch 164/2000\n",
      "3968/4858 [=======================>......] - ETA: 0s - loss: 77000.0699 - mean_squared_error: 77000.0625 - mean_absolute_error: 147.2889\n",
      "Epoch 00164: val_loss improved from 74797.56445 to 74576.13800, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 69us/sample - loss: 83629.2398 - mean_squared_error: 83629.2422 - mean_absolute_error: 150.5168 - val_loss: 74576.1380 - val_mean_squared_error: 74576.1250 - val_mean_absolute_error: 143.0606\n",
      "Epoch 165/2000\n",
      "4480/4858 [==========================>...] - ETA: 0s - loss: 84552.1545 - mean_squared_error: 84552.1250 - mean_absolute_error: 149.3145\n",
      "Epoch 00165: val_loss did not improve from 74576.13800\n",
      "4858/4858 [==============================] - 0s 72us/sample - loss: 83494.8314 - mean_squared_error: 83494.8047 - mean_absolute_error: 149.9053 - val_loss: 74670.6711 - val_mean_squared_error: 74670.6797 - val_mean_absolute_error: 146.1742\n",
      "Epoch 166/2000\n",
      "3872/4858 [======================>.......] - ETA: 0s - loss: 80148.5315 - mean_squared_error: 80148.5312 - mean_absolute_error: 149.6556\n",
      "Epoch 00166: val_loss improved from 74576.13800 to 74448.68208, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 59us/sample - loss: 83529.5293 - mean_squared_error: 83529.5156 - mean_absolute_error: 151.0306 - val_loss: 74448.6821 - val_mean_squared_error: 74448.6797 - val_mean_absolute_error: 142.1404\n",
      "Epoch 167/2000\n",
      "4544/4858 [===========================>..] - ETA: 0s - loss: 83373.7361 - mean_squared_error: 83373.7188 - mean_absolute_error: 149.6330\n",
      "Epoch 00167: val_loss did not improve from 74448.68208\n",
      "4858/4858 [==============================] - 0s 71us/sample - loss: 83481.9335 - mean_squared_error: 83481.9297 - mean_absolute_error: 149.7130 - val_loss: 74568.2434 - val_mean_squared_error: 74568.2422 - val_mean_absolute_error: 139.8203\n",
      "Epoch 168/2000\n",
      "4224/4858 [=========================>....] - ETA: 0s - loss: 86801.5285 - mean_squared_error: 86801.5391 - mean_absolute_error: 151.6130\n",
      "Epoch 00168: val_loss improved from 74448.68208 to 74448.50676, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 85us/sample - loss: 83388.8071 - mean_squared_error: 83388.8047 - mean_absolute_error: 150.2217 - val_loss: 74448.5068 - val_mean_squared_error: 74448.5156 - val_mean_absolute_error: 144.8014\n",
      "Epoch 169/2000\n",
      "4736/4858 [============================>.] - ETA: 0s - loss: 84248.8493 - mean_squared_error: 84248.8281 - mean_absolute_error: 150.1747\n",
      "Epoch 00169: val_loss did not improve from 74448.50676\n",
      "4858/4858 [==============================] - 0s 93us/sample - loss: 83269.5344 - mean_squared_error: 83269.5156 - mean_absolute_error: 149.7776 - val_loss: 74507.3282 - val_mean_squared_error: 74507.3203 - val_mean_absolute_error: 146.3638\n",
      "Epoch 170/2000\n",
      "3360/4858 [===================>..........] - ETA: 0s - loss: 84382.9638 - mean_squared_error: 84382.9844 - mean_absolute_error: 148.1406\n",
      "Epoch 00170: val_loss improved from 74448.50676 to 74426.46649, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 43us/sample - loss: 83231.0773 - mean_squared_error: 83231.0938 - mean_absolute_error: 150.0406 - val_loss: 74426.4665 - val_mean_squared_error: 74426.4688 - val_mean_absolute_error: 143.6225\n",
      "Epoch 171/2000\n",
      "4736/4858 [============================>.] - ETA: 0s - loss: 80482.2331 - mean_squared_error: 80482.2031 - mean_absolute_error: 148.6182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00171: val_loss improved from 74426.46649 to 74386.05002, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 75us/sample - loss: 83195.0273 - mean_squared_error: 83195.0000 - mean_absolute_error: 149.6472 - val_loss: 74386.0500 - val_mean_squared_error: 74386.0469 - val_mean_absolute_error: 145.7936\n",
      "Epoch 172/2000\n",
      "3648/4858 [=====================>........] - ETA: 0s - loss: 77051.3650 - mean_squared_error: 77051.3594 - mean_absolute_error: 146.1241\n",
      "Epoch 00172: val_loss improved from 74386.05002 to 74319.41078, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 65us/sample - loss: 83114.7975 - mean_squared_error: 83114.7734 - mean_absolute_error: 149.2325 - val_loss: 74319.4108 - val_mean_squared_error: 74319.4062 - val_mean_absolute_error: 142.8736\n",
      "Epoch 173/2000\n",
      "4800/4858 [============================>.] - ETA: 0s - loss: 81352.2722 - mean_squared_error: 81352.2812 - mean_absolute_error: 148.7955\n",
      "Epoch 00173: val_loss did not improve from 74319.41078\n",
      "4858/4858 [==============================] - 0s 69us/sample - loss: 82997.6678 - mean_squared_error: 82997.6719 - mean_absolute_error: 149.1224 - val_loss: 74813.6036 - val_mean_squared_error: 74813.6172 - val_mean_absolute_error: 148.3918\n",
      "Epoch 174/2000\n",
      "4256/4858 [=========================>....] - ETA: 0s - loss: 78619.2705 - mean_squared_error: 78619.2812 - mean_absolute_error: 149.2081\n",
      "Epoch 00174: val_loss did not improve from 74319.41078\n",
      "4858/4858 [==============================] - 0s 76us/sample - loss: 82997.8170 - mean_squared_error: 82997.8125 - mean_absolute_error: 149.9500 - val_loss: 74527.0421 - val_mean_squared_error: 74527.0391 - val_mean_absolute_error: 146.4527\n",
      "Epoch 175/2000\n",
      "4832/4858 [============================>.] - ETA: 0s - loss: 82802.7222 - mean_squared_error: 82802.7266 - mean_absolute_error: 149.4689\n",
      "Epoch 00175: val_loss did not improve from 74319.41078\n",
      "4858/4858 [==============================] - 0s 80us/sample - loss: 82911.1689 - mean_squared_error: 82911.1797 - mean_absolute_error: 149.5781 - val_loss: 74628.4800 - val_mean_squared_error: 74628.4844 - val_mean_absolute_error: 137.8614\n",
      "Epoch 176/2000\n",
      "4832/4858 [============================>.] - ETA: 0s - loss: 78040.2071 - mean_squared_error: 78040.2266 - mean_absolute_error: 147.3726\n",
      "Epoch 00176: val_loss did not improve from 74319.41078\n",
      "4858/4858 [==============================] - 0s 80us/sample - loss: 82868.4212 - mean_squared_error: 82868.4375 - mean_absolute_error: 148.2679 - val_loss: 74527.0931 - val_mean_squared_error: 74527.0938 - val_mean_absolute_error: 143.5145\n",
      "Epoch 177/2000\n",
      "4000/4858 [=======================>......] - ETA: 0s - loss: 84180.1969 - mean_squared_error: 84180.2031 - mean_absolute_error: 150.7083\n",
      "Epoch 00177: val_loss did not improve from 74319.41078\n",
      "4858/4858 [==============================] - 0s 65us/sample - loss: 82896.5484 - mean_squared_error: 82896.5469 - mean_absolute_error: 150.1251 - val_loss: 74328.4774 - val_mean_squared_error: 74328.4844 - val_mean_absolute_error: 142.1745\n",
      "Epoch 178/2000\n",
      "4576/4858 [===========================>..] - ETA: 0s - loss: 79975.5772 - mean_squared_error: 79975.5859 - mean_absolute_error: 148.8752\n",
      "Epoch 00178: val_loss did not improve from 74319.41078\n",
      "4858/4858 [==============================] - 0s 83us/sample - loss: 82845.6739 - mean_squared_error: 82845.6719 - mean_absolute_error: 149.1987 - val_loss: 74394.0819 - val_mean_squared_error: 74394.0703 - val_mean_absolute_error: 144.3905\n",
      "Epoch 179/2000\n",
      "4288/4858 [=========================>....] - ETA: 0s - loss: 82743.7685 - mean_squared_error: 82743.7734 - mean_absolute_error: 148.1265\n",
      "Epoch 00179: val_loss improved from 74319.41078 to 74169.73188, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 82us/sample - loss: 82671.5045 - mean_squared_error: 82671.5000 - mean_absolute_error: 148.7473 - val_loss: 74169.7319 - val_mean_squared_error: 74169.7344 - val_mean_absolute_error: 142.6713\n",
      "Epoch 180/2000\n",
      "4128/4858 [========================>.....] - ETA: 0s - loss: 79514.0867 - mean_squared_error: 79514.0859 - mean_absolute_error: 147.3953\n",
      "Epoch 00180: val_loss improved from 74169.73188 to 74064.07370, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 84us/sample - loss: 82655.5695 - mean_squared_error: 82655.5625 - mean_absolute_error: 149.0878 - val_loss: 74064.0737 - val_mean_squared_error: 74064.0547 - val_mean_absolute_error: 142.4587\n",
      "Epoch 181/2000\n",
      "4512/4858 [==========================>...] - ETA: 0s - loss: 82843.0722 - mean_squared_error: 82843.0625 - mean_absolute_error: 147.5083\n",
      "Epoch 00181: val_loss did not improve from 74064.07370\n",
      "4858/4858 [==============================] - 0s 54us/sample - loss: 82544.1568 - mean_squared_error: 82544.1484 - mean_absolute_error: 148.4352 - val_loss: 74314.2436 - val_mean_squared_error: 74314.2422 - val_mean_absolute_error: 145.5030\n",
      "Epoch 182/2000\n",
      "4768/4858 [============================>.] - ETA: 0s - loss: 82284.4026 - mean_squared_error: 82284.4062 - mean_absolute_error: 147.9960\n",
      "Epoch 00182: val_loss did not improve from 74064.07370\n",
      "4858/4858 [==============================] - 0s 80us/sample - loss: 82562.1977 - mean_squared_error: 82562.2031 - mean_absolute_error: 148.4619 - val_loss: 74421.5538 - val_mean_squared_error: 74421.5625 - val_mean_absolute_error: 147.3272\n",
      "Epoch 183/2000\n",
      "4160/4858 [========================>.....] - ETA: 0s - loss: 79706.1633 - mean_squared_error: 79706.1562 - mean_absolute_error: 148.1772\n",
      "Epoch 00183: val_loss did not improve from 74064.07370\n",
      "4858/4858 [==============================] - 0s 77us/sample - loss: 82478.8327 - mean_squared_error: 82478.8125 - mean_absolute_error: 149.4721 - val_loss: 74200.3624 - val_mean_squared_error: 74200.3516 - val_mean_absolute_error: 138.0863\n",
      "Epoch 184/2000\n",
      "4128/4858 [========================>.....] - ETA: 0s - loss: 84582.2881 - mean_squared_error: 84582.3125 - mean_absolute_error: 150.6937\n",
      "Epoch 00184: val_loss did not improve from 74064.07370\n",
      "4858/4858 [==============================] - 0s 46us/sample - loss: 82408.3426 - mean_squared_error: 82408.3516 - mean_absolute_error: 149.2051 - val_loss: 74139.9255 - val_mean_squared_error: 74139.9141 - val_mean_absolute_error: 141.5811\n",
      "Epoch 185/2000\n",
      "3744/4858 [======================>.......] - ETA: 0s - loss: 81786.5352 - mean_squared_error: 81786.5391 - mean_absolute_error: 145.1118\n",
      "Epoch 00185: val_loss did not improve from 74064.07370\n",
      "4858/4858 [==============================] - 0s 61us/sample - loss: 82333.8806 - mean_squared_error: 82333.8828 - mean_absolute_error: 147.0137 - val_loss: 75159.1499 - val_mean_squared_error: 75159.1484 - val_mean_absolute_error: 151.6964\n",
      "Epoch 186/2000\n",
      "4576/4858 [===========================>..] - ETA: 0s - loss: 84520.9227 - mean_squared_error: 84520.8984 - mean_absolute_error: 150.5497\n",
      "Epoch 00186: val_loss did not improve from 74064.07370\n",
      "4858/4858 [==============================] - 0s 85us/sample - loss: 82605.1709 - mean_squared_error: 82605.1406 - mean_absolute_error: 150.4554 - val_loss: 74248.1701 - val_mean_squared_error: 74248.1797 - val_mean_absolute_error: 145.6559\n",
      "Epoch 187/2000\n",
      "4672/4858 [===========================>..] - ETA: 0s - loss: 77876.9392 - mean_squared_error: 77876.9297 - mean_absolute_error: 146.5512\n",
      "Epoch 00187: val_loss improved from 74064.07370 to 73958.58625, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 80us/sample - loss: 82380.4837 - mean_squared_error: 82380.4766 - mean_absolute_error: 148.3872 - val_loss: 73958.5863 - val_mean_squared_error: 73958.5938 - val_mean_absolute_error: 141.6759\n",
      "Epoch 188/2000\n",
      "4672/4858 [===========================>..] - ETA: 0s - loss: 83636.4261 - mean_squared_error: 83636.4219 - mean_absolute_error: 149.4254\n",
      "Epoch 00188: val_loss did not improve from 73958.58625\n",
      "4858/4858 [==============================] - 0s 72us/sample - loss: 82411.2354 - mean_squared_error: 82411.2266 - mean_absolute_error: 148.5966 - val_loss: 74073.2409 - val_mean_squared_error: 74073.2422 - val_mean_absolute_error: 138.7531\n",
      "Epoch 189/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4768/4858 [============================>.] - ETA: 0s - loss: 82388.8784 - mean_squared_error: 82388.8672 - mean_absolute_error: 147.6465\n",
      "Epoch 00189: val_loss improved from 73958.58625 to 73838.92310, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 71us/sample - loss: 82169.7159 - mean_squared_error: 82169.7031 - mean_absolute_error: 147.9520 - val_loss: 73838.9231 - val_mean_squared_error: 73838.9141 - val_mean_absolute_error: 141.7462\n",
      "Epoch 190/2000\n",
      "4480/4858 [==========================>...] - ETA: 0s - loss: 80280.0744 - mean_squared_error: 80280.0469 - mean_absolute_error: 145.9535\n",
      "Epoch 00190: val_loss did not improve from 73838.92310\n",
      "4858/4858 [==============================] - 0s 72us/sample - loss: 82173.4706 - mean_squared_error: 82173.4609 - mean_absolute_error: 147.7563 - val_loss: 74335.8553 - val_mean_squared_error: 74335.8516 - val_mean_absolute_error: 147.6225\n",
      "Epoch 191/2000\n",
      "4320/4858 [=========================>....] - ETA: 0s - loss: 82371.7825 - mean_squared_error: 82371.7578 - mean_absolute_error: 149.2559\n",
      "Epoch 00191: val_loss did not improve from 73838.92310\n",
      "4858/4858 [==============================] - 0s 61us/sample - loss: 82327.7516 - mean_squared_error: 82327.7109 - mean_absolute_error: 149.5234 - val_loss: 73914.5331 - val_mean_squared_error: 73914.5312 - val_mean_absolute_error: 139.4826\n",
      "Epoch 192/2000\n",
      "4128/4858 [========================>.....] - ETA: 0s - loss: 82852.8292 - mean_squared_error: 82852.8281 - mean_absolute_error: 146.0851\n",
      "Epoch 00192: val_loss did not improve from 73838.92310\n",
      "4858/4858 [==============================] - 0s 56us/sample - loss: 82163.1832 - mean_squared_error: 82163.1953 - mean_absolute_error: 147.3621 - val_loss: 73863.2035 - val_mean_squared_error: 73863.1953 - val_mean_absolute_error: 143.1646\n",
      "Epoch 193/2000\n",
      "4352/4858 [=========================>....] - ETA: 0s - loss: 81768.2595 - mean_squared_error: 81768.2734 - mean_absolute_error: 147.9765\n",
      "Epoch 00193: val_loss improved from 73838.92310 to 73781.13827, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 89us/sample - loss: 82084.4984 - mean_squared_error: 82084.5156 - mean_absolute_error: 148.1485 - val_loss: 73781.1383 - val_mean_squared_error: 73781.1328 - val_mean_absolute_error: 141.1382\n",
      "Epoch 194/2000\n",
      "4320/4858 [=========================>....] - ETA: 0s - loss: 81572.4548 - mean_squared_error: 81572.4375 - mean_absolute_error: 149.1771\n",
      "Epoch 00194: val_loss did not improve from 73781.13827\n",
      "4858/4858 [==============================] - 0s 55us/sample - loss: 82145.0420 - mean_squared_error: 82145.0469 - mean_absolute_error: 148.8117 - val_loss: 73874.7989 - val_mean_squared_error: 73874.8047 - val_mean_absolute_error: 137.8919\n",
      "Epoch 195/2000\n",
      "4128/4858 [========================>.....] - ETA: 0s - loss: 81380.8110 - mean_squared_error: 81380.7969 - mean_absolute_error: 146.2519\n",
      "Epoch 00195: val_loss did not improve from 73781.13827\n",
      "4858/4858 [==============================] - 0s 57us/sample - loss: 82162.0661 - mean_squared_error: 82162.0391 - mean_absolute_error: 147.5743 - val_loss: 73812.4418 - val_mean_squared_error: 73812.4453 - val_mean_absolute_error: 143.0943\n",
      "Epoch 196/2000\n",
      "4576/4858 [===========================>..] - ETA: 0s - loss: 84084.4877 - mean_squared_error: 84084.4766 - mean_absolute_error: 149.1010\n",
      "Epoch 00196: val_loss did not improve from 73781.13827\n",
      "4858/4858 [==============================] - 0s 65us/sample - loss: 82001.0292 - mean_squared_error: 82001.0156 - mean_absolute_error: 147.6880 - val_loss: 73908.7206 - val_mean_squared_error: 73908.7109 - val_mean_absolute_error: 144.4921\n",
      "Epoch 197/2000\n",
      "4512/4858 [==========================>...] - ETA: 0s - loss: 79924.7624 - mean_squared_error: 79924.7344 - mean_absolute_error: 146.9777\n",
      "Epoch 00197: val_loss improved from 73781.13827 to 73750.33816, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 44us/sample - loss: 81943.5680 - mean_squared_error: 81943.5391 - mean_absolute_error: 148.1588 - val_loss: 73750.3382 - val_mean_squared_error: 73750.3281 - val_mean_absolute_error: 138.4945\n",
      "Epoch 198/2000\n",
      "3840/4858 [======================>.......] - ETA: 0s - loss: 83961.7002 - mean_squared_error: 83961.6797 - mean_absolute_error: 146.8037\n",
      "Epoch 00198: val_loss did not improve from 73750.33816\n",
      "4858/4858 [==============================] - 0s 62us/sample - loss: 82037.1410 - mean_squared_error: 82037.1328 - mean_absolute_error: 148.4386 - val_loss: 73761.6139 - val_mean_squared_error: 73761.6094 - val_mean_absolute_error: 141.9369\n",
      "Epoch 199/2000\n",
      "4576/4858 [===========================>..] - ETA: 0s - loss: 83035.2359 - mean_squared_error: 83035.2344 - mean_absolute_error: 146.3079\n",
      "Epoch 00199: val_loss improved from 73750.33816 to 73587.99647, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 85us/sample - loss: 82126.4317 - mean_squared_error: 82126.4219 - mean_absolute_error: 147.5101 - val_loss: 73587.9965 - val_mean_squared_error: 73588.0078 - val_mean_absolute_error: 142.6660\n",
      "Epoch 200/2000\n",
      "3360/4858 [===================>..........] - ETA: 0s - loss: 87293.2413 - mean_squared_error: 87293.2188 - mean_absolute_error: 151.7367\n",
      "Epoch 00200: val_loss did not improve from 73587.99647\n",
      "4858/4858 [==============================] - 0s 38us/sample - loss: 82048.0480 - mean_squared_error: 82048.0312 - mean_absolute_error: 147.4931 - val_loss: 73690.5411 - val_mean_squared_error: 73690.5391 - val_mean_absolute_error: 143.0922\n",
      "Epoch 201/2000\n",
      "4448/4858 [==========================>...] - ETA: 0s - loss: 77873.8845 - mean_squared_error: 77873.8672 - mean_absolute_error: 145.4713\n",
      "Epoch 00201: val_loss did not improve from 73587.99647\n",
      "4858/4858 [==============================] - 0s 86us/sample - loss: 81861.6377 - mean_squared_error: 81861.6250 - mean_absolute_error: 147.3824 - val_loss: 73654.0104 - val_mean_squared_error: 73654.0234 - val_mean_absolute_error: 139.8553\n",
      "Epoch 202/2000\n",
      "4480/4858 [==========================>...] - ETA: 0s - loss: 85071.4521 - mean_squared_error: 85071.4531 - mean_absolute_error: 151.5682\n",
      "Epoch 00202: val_loss did not improve from 73587.99647\n",
      "4858/4858 [==============================] - 0s 85us/sample - loss: 81570.7327 - mean_squared_error: 81570.7266 - mean_absolute_error: 148.7322 - val_loss: 74264.7585 - val_mean_squared_error: 74264.7656 - val_mean_absolute_error: 134.6642\n",
      "Epoch 203/2000\n",
      "4832/4858 [============================>.] - ETA: 0s - loss: 81793.0653 - mean_squared_error: 81793.0625 - mean_absolute_error: 146.3784\n",
      "Epoch 00203: val_loss did not improve from 73587.99647\n",
      "4858/4858 [==============================] - 0s 73us/sample - loss: 81838.3598 - mean_squared_error: 81838.3516 - mean_absolute_error: 146.4498 - val_loss: 73651.2355 - val_mean_squared_error: 73651.2344 - val_mean_absolute_error: 141.1173\n",
      "Epoch 204/2000\n",
      "3200/4858 [==================>...........] - ETA: 0s - loss: 84882.4685 - mean_squared_error: 84882.4609 - mean_absolute_error: 149.8724\n",
      "Epoch 00204: val_loss improved from 73587.99647 to 73527.90775, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 51us/sample - loss: 81716.1206 - mean_squared_error: 81716.1328 - mean_absolute_error: 147.5610 - val_loss: 73527.9078 - val_mean_squared_error: 73527.8984 - val_mean_absolute_error: 141.0533\n",
      "Epoch 205/2000\n",
      "4544/4858 [===========================>..] - ETA: 0s - loss: 83243.1575 - mean_squared_error: 83243.1484 - mean_absolute_error: 147.7852\n",
      "Epoch 00205: val_loss improved from 73527.90775 to 73496.75078, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 92us/sample - loss: 81728.3168 - mean_squared_error: 81728.3125 - mean_absolute_error: 147.8417 - val_loss: 73496.7508 - val_mean_squared_error: 73496.7500 - val_mean_absolute_error: 142.0998\n",
      "Epoch 206/2000\n",
      "4320/4858 [=========================>....] - ETA: 0s - loss: 84789.5754 - mean_squared_error: 84789.5469 - mean_absolute_error: 148.3235\n",
      "Epoch 00206: val_loss did not improve from 73496.75078\n",
      "4858/4858 [==============================] - 0s 76us/sample - loss: 81627.9028 - mean_squared_error: 81627.8750 - mean_absolute_error: 147.5562 - val_loss: 73523.8516 - val_mean_squared_error: 73523.8516 - val_mean_absolute_error: 140.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207/2000\n",
      "3360/4858 [===================>..........] - ETA: 0s - loss: 76700.7891 - mean_squared_error: 76700.7891 - mean_absolute_error: 146.8966\n",
      "Epoch 00207: val_loss improved from 73496.75078 to 73404.26248, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 54us/sample - loss: 81699.9764 - mean_squared_error: 81699.9844 - mean_absolute_error: 146.8464 - val_loss: 73404.2625 - val_mean_squared_error: 73404.2500 - val_mean_absolute_error: 140.9479\n",
      "Epoch 208/2000\n",
      "4064/4858 [========================>.....] - ETA: 0s - loss: 76272.7274 - mean_squared_error: 76272.7344 - mean_absolute_error: 144.7044\n",
      "Epoch 00208: val_loss improved from 73404.26248 to 73373.81035, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 80us/sample - loss: 81499.2341 - mean_squared_error: 81499.2422 - mean_absolute_error: 146.9009 - val_loss: 73373.8103 - val_mean_squared_error: 73373.8047 - val_mean_absolute_error: 141.2697\n",
      "Epoch 209/2000\n",
      "3488/4858 [====================>.........] - ETA: 0s - loss: 86378.8052 - mean_squared_error: 86378.7969 - mean_absolute_error: 150.2876\n",
      "Epoch 00209: val_loss did not improve from 73373.81035\n",
      "4858/4858 [==============================] - 0s 55us/sample - loss: 81591.5587 - mean_squared_error: 81591.5469 - mean_absolute_error: 148.1800 - val_loss: 73434.0941 - val_mean_squared_error: 73434.0938 - val_mean_absolute_error: 138.0787\n",
      "Epoch 210/2000\n",
      "4224/4858 [=========================>....] - ETA: 0s - loss: 85342.6907 - mean_squared_error: 85342.7031 - mean_absolute_error: 150.0815\n",
      "Epoch 00210: val_loss did not improve from 73373.81035\n",
      "4858/4858 [==============================] - 0s 74us/sample - loss: 81379.0072 - mean_squared_error: 81379.0078 - mean_absolute_error: 147.2859 - val_loss: 73673.5043 - val_mean_squared_error: 73673.5078 - val_mean_absolute_error: 136.4625\n",
      "Epoch 211/2000\n",
      "3872/4858 [======================>.......] - ETA: 0s - loss: 70594.2576 - mean_squared_error: 70594.2422 - mean_absolute_error: 143.9153\n",
      "Epoch 00211: val_loss did not improve from 73373.81035\n",
      "4858/4858 [==============================] - 0s 62us/sample - loss: 81442.2119 - mean_squared_error: 81442.1797 - mean_absolute_error: 146.1456 - val_loss: 73969.0626 - val_mean_squared_error: 73969.0703 - val_mean_absolute_error: 146.9734\n",
      "Epoch 212/2000\n",
      "4128/4858 [========================>.....] - ETA: 0s - loss: 82867.5905 - mean_squared_error: 82867.5938 - mean_absolute_error: 148.4446\n",
      "Epoch 00212: val_loss did not improve from 73373.81035\n",
      "4858/4858 [==============================] - 0s 78us/sample - loss: 81456.5553 - mean_squared_error: 81456.5547 - mean_absolute_error: 147.2170 - val_loss: 73384.6192 - val_mean_squared_error: 73384.6094 - val_mean_absolute_error: 140.8890\n",
      "Epoch 213/2000\n",
      "4128/4858 [========================>.....] - ETA: 0s - loss: 83973.5358 - mean_squared_error: 83973.5312 - mean_absolute_error: 148.2423\n",
      "Epoch 00213: val_loss did not improve from 73373.81035\n",
      "4858/4858 [==============================] - 0s 66us/sample - loss: 81272.0470 - mean_squared_error: 81272.0391 - mean_absolute_error: 146.7629 - val_loss: 73552.7076 - val_mean_squared_error: 73552.7031 - val_mean_absolute_error: 140.9949\n",
      "Epoch 214/2000\n",
      "4672/4858 [===========================>..] - ETA: 0s - loss: 81771.1895 - mean_squared_error: 81771.1953 - mean_absolute_error: 147.5506\n",
      "Epoch 00214: val_loss did not improve from 73373.81035\n",
      "4858/4858 [==============================] - 0s 61us/sample - loss: 81401.9816 - mean_squared_error: 81401.9766 - mean_absolute_error: 147.1930 - val_loss: 73447.2078 - val_mean_squared_error: 73447.2031 - val_mean_absolute_error: 139.2610\n",
      "Epoch 215/2000\n",
      "4032/4858 [=======================>......] - ETA: 0s - loss: 80839.1829 - mean_squared_error: 80839.1719 - mean_absolute_error: 145.5785\n",
      "Epoch 00215: val_loss improved from 73373.81035 to 73367.84993, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 78us/sample - loss: 81211.6087 - mean_squared_error: 81211.6094 - mean_absolute_error: 146.3620 - val_loss: 73367.8499 - val_mean_squared_error: 73367.8438 - val_mean_absolute_error: 140.1732\n",
      "Epoch 216/2000\n",
      "4800/4858 [============================>.] - ETA: 0s - loss: 81179.8387 - mean_squared_error: 81179.8281 - mean_absolute_error: 146.6909\n",
      "Epoch 00216: val_loss did not improve from 73367.84993\n",
      "4858/4858 [==============================] - 0s 65us/sample - loss: 81381.7486 - mean_squared_error: 81381.7344 - mean_absolute_error: 146.7951 - val_loss: 73368.7254 - val_mean_squared_error: 73368.7266 - val_mean_absolute_error: 142.2523\n",
      "Epoch 217/2000\n",
      "4704/4858 [============================>.] - ETA: 0s - loss: 81639.3290 - mean_squared_error: 81639.3203 - mean_absolute_error: 146.0132\n",
      "Epoch 00217: val_loss improved from 73367.84993 to 73356.79399, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 89us/sample - loss: 81317.0313 - mean_squared_error: 81317.0156 - mean_absolute_error: 146.3712 - val_loss: 73356.7940 - val_mean_squared_error: 73356.7969 - val_mean_absolute_error: 141.5140\n",
      "Epoch 218/2000\n",
      "4416/4858 [==========================>...] - ETA: 0s - loss: 81836.1001 - mean_squared_error: 81836.0938 - mean_absolute_error: 146.9349\n",
      "Epoch 00218: val_loss improved from 73356.79399 to 73237.17828, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 83us/sample - loss: 81263.3413 - mean_squared_error: 81263.3359 - mean_absolute_error: 147.1666 - val_loss: 73237.1783 - val_mean_squared_error: 73237.1719 - val_mean_absolute_error: 140.7567\n",
      "Epoch 219/2000\n",
      "4800/4858 [============================>.] - ETA: 0s - loss: 81487.8709 - mean_squared_error: 81487.8516 - mean_absolute_error: 146.5690\n",
      "Epoch 00219: val_loss improved from 73237.17828 to 73216.36387, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 87us/sample - loss: 81205.4836 - mean_squared_error: 81205.4688 - mean_absolute_error: 146.6716 - val_loss: 73216.3639 - val_mean_squared_error: 73216.3750 - val_mean_absolute_error: 138.4321\n",
      "Epoch 220/2000\n",
      "4672/4858 [===========================>..] - ETA: 0s - loss: 81032.5823 - mean_squared_error: 81032.5938 - mean_absolute_error: 145.9136\n",
      "Epoch 00220: val_loss did not improve from 73216.36387\n",
      "4858/4858 [==============================] - 0s 81us/sample - loss: 81227.2034 - mean_squared_error: 81227.2109 - mean_absolute_error: 146.2915 - val_loss: 73365.1343 - val_mean_squared_error: 73365.1328 - val_mean_absolute_error: 142.1214\n",
      "Epoch 221/2000\n",
      "4640/4858 [===========================>..] - ETA: 0s - loss: 81299.4283 - mean_squared_error: 81299.4297 - mean_absolute_error: 146.7256\n",
      "Epoch 00221: val_loss did not improve from 73216.36387\n",
      "4858/4858 [==============================] - 0s 66us/sample - loss: 81097.2615 - mean_squared_error: 81097.2656 - mean_absolute_error: 146.5744 - val_loss: 73341.5947 - val_mean_squared_error: 73341.5938 - val_mean_absolute_error: 142.5376\n",
      "Epoch 222/2000\n",
      "4160/4858 [========================>.....] - ETA: 0s - loss: 85667.6161 - mean_squared_error: 85667.6250 - mean_absolute_error: 148.4661\n",
      "Epoch 00222: val_loss did not improve from 73216.36387\n",
      "4858/4858 [==============================] - 0s 77us/sample - loss: 81026.5177 - mean_squared_error: 81026.5234 - mean_absolute_error: 146.9737 - val_loss: 73783.6478 - val_mean_squared_error: 73783.6406 - val_mean_absolute_error: 134.7353\n",
      "Epoch 223/2000\n",
      "4704/4858 [============================>.] - ETA: 0s - loss: 81626.1155 - mean_squared_error: 81626.1250 - mean_absolute_error: 146.5776\n",
      "Epoch 00223: val_loss improved from 73216.36387 to 73178.08210, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 87us/sample - loss: 81097.2329 - mean_squared_error: 81097.2422 - mean_absolute_error: 146.1668 - val_loss: 73178.0821 - val_mean_squared_error: 73178.0781 - val_mean_absolute_error: 139.0804\n",
      "Epoch 224/2000\n",
      "4256/4858 [=========================>....] - ETA: 0s - loss: 76164.4089 - mean_squared_error: 76164.4141 - mean_absolute_error: 144.1578\n",
      "Epoch 00224: val_loss did not improve from 73178.08210\n",
      "4858/4858 [==============================] - 0s 75us/sample - loss: 81062.6143 - mean_squared_error: 81062.6172 - mean_absolute_error: 145.9409 - val_loss: 73256.9573 - val_mean_squared_error: 73256.9688 - val_mean_absolute_error: 141.5067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/2000\n",
      "4768/4858 [============================>.] - ETA: 0s - loss: 81481.8612 - mean_squared_error: 81481.8594 - mean_absolute_error: 146.3449\n",
      "Epoch 00225: val_loss did not improve from 73178.08210\n",
      "4858/4858 [==============================] - 0s 79us/sample - loss: 80920.3458 - mean_squared_error: 80920.3359 - mean_absolute_error: 146.2164 - val_loss: 74344.4945 - val_mean_squared_error: 74344.4922 - val_mean_absolute_error: 149.2294\n",
      "Epoch 226/2000\n",
      "4448/4858 [==========================>...] - ETA: 0s - loss: 82571.6312 - mean_squared_error: 82571.6172 - mean_absolute_error: 147.0172\n",
      "Epoch 00226: val_loss did not improve from 73178.08210\n",
      "4858/4858 [==============================] - 0s 75us/sample - loss: 81034.2984 - mean_squared_error: 81034.2891 - mean_absolute_error: 147.0382 - val_loss: 73325.6727 - val_mean_squared_error: 73325.6719 - val_mean_absolute_error: 136.8662\n",
      "Epoch 227/2000\n",
      "4352/4858 [=========================>....] - ETA: 0s - loss: 82133.0661 - mean_squared_error: 82133.0703 - mean_absolute_error: 146.5551\n",
      "Epoch 00227: val_loss did not improve from 73178.08210\n",
      "4858/4858 [==============================] - 0s 71us/sample - loss: 80909.4635 - mean_squared_error: 80909.4688 - mean_absolute_error: 146.4698 - val_loss: 73347.1156 - val_mean_squared_error: 73347.1172 - val_mean_absolute_error: 136.8432\n",
      "Epoch 228/2000\n",
      "4736/4858 [============================>.] - ETA: 0s - loss: 80381.1380 - mean_squared_error: 80381.1328 - mean_absolute_error: 145.0771\n",
      "Epoch 00228: val_loss did not improve from 73178.08210\n",
      "4858/4858 [==============================] - 0s 82us/sample - loss: 80936.3869 - mean_squared_error: 80936.3750 - mean_absolute_error: 145.6807 - val_loss: 73289.4998 - val_mean_squared_error: 73289.5000 - val_mean_absolute_error: 138.0811\n",
      "Epoch 229/2000\n",
      "4672/4858 [===========================>..] - ETA: 0s - loss: 82042.4124 - mean_squared_error: 82042.4141 - mean_absolute_error: 147.0768\n",
      "Epoch 00229: val_loss did not improve from 73178.08210\n",
      "4858/4858 [==============================] - 0s 45us/sample - loss: 80828.5407 - mean_squared_error: 80828.5469 - mean_absolute_error: 146.5081 - val_loss: 73275.2321 - val_mean_squared_error: 73275.2344 - val_mean_absolute_error: 137.1284\n",
      "Epoch 230/2000\n",
      "4704/4858 [============================>.] - ETA: 0s - loss: 80838.2999 - mean_squared_error: 80838.3125 - mean_absolute_error: 145.6301\n",
      "Epoch 00230: val_loss did not improve from 73178.08210\n",
      "4858/4858 [==============================] - 0s 82us/sample - loss: 80821.3554 - mean_squared_error: 80821.3594 - mean_absolute_error: 146.0076 - val_loss: 73293.2798 - val_mean_squared_error: 73293.2891 - val_mean_absolute_error: 136.8861\n",
      "Epoch 231/2000\n",
      "4320/4858 [=========================>....] - ETA: 0s - loss: 78172.2213 - mean_squared_error: 78172.2344 - mean_absolute_error: 146.2051\n",
      "Epoch 00231: val_loss improved from 73178.08210 to 73171.76071, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 80us/sample - loss: 80876.1525 - mean_squared_error: 80876.1641 - mean_absolute_error: 146.1492 - val_loss: 73171.7607 - val_mean_squared_error: 73171.7578 - val_mean_absolute_error: 138.3376\n",
      "Epoch 232/2000\n",
      "3488/4858 [====================>.........] - ETA: 0s - loss: 91827.1242 - mean_squared_error: 91827.1406 - mean_absolute_error: 151.5805\n",
      "Epoch 00232: val_loss did not improve from 73171.76071\n",
      "4858/4858 [==============================] - 0s 60us/sample - loss: 80772.3278 - mean_squared_error: 80772.3516 - mean_absolute_error: 146.2771 - val_loss: 73322.6013 - val_mean_squared_error: 73322.6016 - val_mean_absolute_error: 136.7026\n",
      "Epoch 233/2000\n",
      "4032/4858 [=======================>......] - ETA: 0s - loss: 75582.0835 - mean_squared_error: 75582.0859 - mean_absolute_error: 141.5912\n",
      "Epoch 00233: val_loss did not improve from 73171.76071\n",
      "4858/4858 [==============================] - 0s 56us/sample - loss: 80779.3273 - mean_squared_error: 80779.3281 - mean_absolute_error: 144.9628 - val_loss: 73413.1012 - val_mean_squared_error: 73413.1016 - val_mean_absolute_error: 143.7804\n",
      "Epoch 234/2000\n",
      "4512/4858 [==========================>...] - ETA: 0s - loss: 79554.6404 - mean_squared_error: 79554.6250 - mean_absolute_error: 145.2522\n",
      "Epoch 00234: val_loss improved from 73171.76071 to 73159.83160, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 60us/sample - loss: 80852.4092 - mean_squared_error: 80852.3906 - mean_absolute_error: 146.4033 - val_loss: 73159.8316 - val_mean_squared_error: 73159.8359 - val_mean_absolute_error: 138.2332\n",
      "Epoch 235/2000\n",
      "4384/4858 [==========================>...] - ETA: 0s - loss: 76599.3238 - mean_squared_error: 76599.3047 - mean_absolute_error: 144.2875\n",
      "Epoch 00235: val_loss did not improve from 73159.83160\n",
      "4858/4858 [==============================] - 0s 71us/sample - loss: 80653.6260 - mean_squared_error: 80653.6250 - mean_absolute_error: 145.2788 - val_loss: 73214.7904 - val_mean_squared_error: 73214.7891 - val_mean_absolute_error: 136.7083\n",
      "Epoch 236/2000\n",
      "4320/4858 [=========================>....] - ETA: 0s - loss: 83046.2704 - mean_squared_error: 83046.2812 - mean_absolute_error: 147.5461\n",
      "Epoch 00236: val_loss improved from 73159.83160 to 73099.21776, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 56us/sample - loss: 80668.0618 - mean_squared_error: 80668.0703 - mean_absolute_error: 145.9610 - val_loss: 73099.2178 - val_mean_squared_error: 73099.2109 - val_mean_absolute_error: 137.5458\n",
      "Epoch 237/2000\n",
      "4768/4858 [============================>.] - ETA: 0s - loss: 80602.7003 - mean_squared_error: 80602.7109 - mean_absolute_error: 144.8497\n",
      "Epoch 00237: val_loss did not improve from 73099.21776\n",
      "4858/4858 [==============================] - 0s 59us/sample - loss: 80639.3629 - mean_squared_error: 80639.3750 - mean_absolute_error: 145.2446 - val_loss: 73160.2543 - val_mean_squared_error: 73160.2578 - val_mean_absolute_error: 140.2950\n",
      "Epoch 238/2000\n",
      "4640/4858 [===========================>..] - ETA: 0s - loss: 81704.9865 - mean_squared_error: 81704.9688 - mean_absolute_error: 145.9814\n",
      "Epoch 00238: val_loss did not improve from 73099.21776\n",
      "4858/4858 [==============================] - 0s 71us/sample - loss: 80569.2440 - mean_squared_error: 80569.2266 - mean_absolute_error: 145.7072 - val_loss: 73163.6366 - val_mean_squared_error: 73163.6328 - val_mean_absolute_error: 141.1383\n",
      "Epoch 239/2000\n",
      "4448/4858 [==========================>...] - ETA: 0s - loss: 82846.0841 - mean_squared_error: 82846.0781 - mean_absolute_error: 146.5955\n",
      "Epoch 00239: val_loss improved from 73099.21776 to 73025.37488, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 80us/sample - loss: 80752.2010 - mean_squared_error: 80752.1875 - mean_absolute_error: 145.5154 - val_loss: 73025.3749 - val_mean_squared_error: 73025.3906 - val_mean_absolute_error: 137.2281\n",
      "Epoch 240/2000\n",
      "3616/4858 [=====================>........] - ETA: 0s - loss: 83161.3706 - mean_squared_error: 83161.3750 - mean_absolute_error: 146.3488\n",
      "Epoch 00240: val_loss did not improve from 73025.37488\n",
      "4858/4858 [==============================] - 0s 56us/sample - loss: 80708.5729 - mean_squared_error: 80708.5703 - mean_absolute_error: 145.3202 - val_loss: 73264.6895 - val_mean_squared_error: 73264.6875 - val_mean_absolute_error: 141.3391\n",
      "Epoch 241/2000\n",
      "4544/4858 [===========================>..] - ETA: 0s - loss: 79925.8539 - mean_squared_error: 79925.8203 - mean_absolute_error: 145.8543\n",
      "Epoch 00241: val_loss did not improve from 73025.37488\n",
      "4858/4858 [==============================] - 0s 78us/sample - loss: 80537.3253 - mean_squared_error: 80537.2969 - mean_absolute_error: 145.6770 - val_loss: 73141.6527 - val_mean_squared_error: 73141.6406 - val_mean_absolute_error: 135.6816\n",
      "Epoch 242/2000\n",
      "4704/4858 [============================>.] - ETA: 0s - loss: 81994.5266 - mean_squared_error: 81994.5312 - mean_absolute_error: 146.6929\n",
      "Epoch 00242: val_loss improved from 73025.37488 to 72963.40940, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 50us/sample - loss: 80507.5102 - mean_squared_error: 80507.5156 - mean_absolute_error: 145.6278 - val_loss: 72963.4094 - val_mean_squared_error: 72963.4141 - val_mean_absolute_error: 137.0085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 243/2000\n",
      "4384/4858 [==========================>...] - ETA: 0s - loss: 79794.8074 - mean_squared_error: 79794.7969 - mean_absolute_error: 145.4298\n",
      "Epoch 00243: val_loss did not improve from 72963.40940\n",
      "4858/4858 [==============================] - 0s 74us/sample - loss: 80329.5592 - mean_squared_error: 80329.5547 - mean_absolute_error: 145.2390 - val_loss: 73039.9722 - val_mean_squared_error: 73039.9766 - val_mean_absolute_error: 140.2140\n",
      "Epoch 244/2000\n",
      "4544/4858 [===========================>..] - ETA: 0s - loss: 78736.4753 - mean_squared_error: 78736.4766 - mean_absolute_error: 144.6336\n",
      "Epoch 00244: val_loss did not improve from 72963.40940\n",
      "4858/4858 [==============================] - 0s 68us/sample - loss: 80490.8668 - mean_squared_error: 80490.8672 - mean_absolute_error: 144.6180 - val_loss: 73030.4284 - val_mean_squared_error: 73030.4297 - val_mean_absolute_error: 138.6061\n",
      "Epoch 245/2000\n",
      "3648/4858 [=====================>........] - ETA: 0s - loss: 77977.0067 - mean_squared_error: 77976.9922 - mean_absolute_error: 145.0907\n",
      "Epoch 00245: val_loss did not improve from 72963.40940\n",
      "4858/4858 [==============================] - 0s 56us/sample - loss: 80292.7709 - mean_squared_error: 80292.7578 - mean_absolute_error: 145.4813 - val_loss: 73074.0334 - val_mean_squared_error: 73074.0312 - val_mean_absolute_error: 141.5343\n",
      "Epoch 246/2000\n",
      "4480/4858 [==========================>...] - ETA: 0s - loss: 79879.0928 - mean_squared_error: 79879.0859 - mean_absolute_error: 145.4278\n",
      "Epoch 00246: val_loss improved from 72963.40940 to 72895.55031, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 79us/sample - loss: 80349.6330 - mean_squared_error: 80349.6328 - mean_absolute_error: 145.9548 - val_loss: 72895.5503 - val_mean_squared_error: 72895.5547 - val_mean_absolute_error: 139.5121\n",
      "Epoch 247/2000\n",
      "4416/4858 [==========================>...] - ETA: 0s - loss: 82796.5810 - mean_squared_error: 82796.5703 - mean_absolute_error: 146.2841\n",
      "Epoch 00247: val_loss did not improve from 72895.55031\n",
      "4858/4858 [==============================] - 0s 51us/sample - loss: 80348.6619 - mean_squared_error: 80348.6562 - mean_absolute_error: 145.6096 - val_loss: 73031.8384 - val_mean_squared_error: 73031.8281 - val_mean_absolute_error: 138.4038\n",
      "Epoch 248/2000\n",
      "4640/4858 [===========================>..] - ETA: 0s - loss: 81615.0598 - mean_squared_error: 81615.0391 - mean_absolute_error: 145.5010\n",
      "Epoch 00248: val_loss did not improve from 72895.55031\n",
      "4858/4858 [==============================] - 0s 71us/sample - loss: 80264.1166 - mean_squared_error: 80264.0938 - mean_absolute_error: 145.4915 - val_loss: 72924.7834 - val_mean_squared_error: 72924.7656 - val_mean_absolute_error: 136.6590\n",
      "Epoch 249/2000\n",
      "4448/4858 [==========================>...] - ETA: 0s - loss: 83201.4371 - mean_squared_error: 83201.4453 - mean_absolute_error: 146.1709\n",
      "Epoch 00249: val_loss did not improve from 72895.55031\n",
      "4858/4858 [==============================] - 0s 73us/sample - loss: 80223.4579 - mean_squared_error: 80223.4688 - mean_absolute_error: 144.8390 - val_loss: 72902.7585 - val_mean_squared_error: 72902.7656 - val_mean_absolute_error: 136.0522\n",
      "Epoch 250/2000\n",
      "4768/4858 [============================>.] - ETA: 0s - loss: 81031.9142 - mean_squared_error: 81031.9062 - mean_absolute_error: 145.6249\n",
      "Epoch 00250: val_loss did not improve from 72895.55031\n",
      "4858/4858 [==============================] - 0s 69us/sample - loss: 80252.5942 - mean_squared_error: 80252.5938 - mean_absolute_error: 145.4670 - val_loss: 73144.9546 - val_mean_squared_error: 73144.9453 - val_mean_absolute_error: 134.5424\n",
      "Epoch 251/2000\n",
      "4192/4858 [========================>.....] - ETA: 0s - loss: 75662.4122 - mean_squared_error: 75662.4062 - mean_absolute_error: 141.2305\n",
      "Epoch 00251: val_loss did not improve from 72895.55031\n",
      "4858/4858 [==============================] - 0s 78us/sample - loss: 80272.6591 - mean_squared_error: 80272.6406 - mean_absolute_error: 143.9312 - val_loss: 73015.2249 - val_mean_squared_error: 73015.2188 - val_mean_absolute_error: 142.1742\n",
      "Epoch 252/2000\n",
      "4128/4858 [========================>.....] - ETA: 0s - loss: 82675.0511 - mean_squared_error: 82675.0625 - mean_absolute_error: 145.5135\n",
      "Epoch 00252: val_loss did not improve from 72895.55031\n",
      "4858/4858 [==============================] - 0s 79us/sample - loss: 80167.0984 - mean_squared_error: 80167.1094 - mean_absolute_error: 145.5034 - val_loss: 72970.3153 - val_mean_squared_error: 72970.3281 - val_mean_absolute_error: 140.9986\n",
      "Epoch 253/2000\n",
      "4480/4858 [==========================>...] - ETA: 0s - loss: 81499.3341 - mean_squared_error: 81499.3672 - mean_absolute_error: 145.6900\n",
      "Epoch 00253: val_loss did not improve from 72895.55031\n",
      "4858/4858 [==============================] - 0s 71us/sample - loss: 80291.6478 - mean_squared_error: 80291.6719 - mean_absolute_error: 144.9588 - val_loss: 72937.3593 - val_mean_squared_error: 72937.3594 - val_mean_absolute_error: 135.7266\n",
      "Epoch 254/2000\n",
      "4416/4858 [==========================>...] - ETA: 0s - loss: 83980.6143 - mean_squared_error: 83980.6016 - mean_absolute_error: 147.2382\n",
      "Epoch 00254: val_loss improved from 72895.55031 to 72853.94548, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 47us/sample - loss: 80295.7122 - mean_squared_error: 80295.7031 - mean_absolute_error: 145.2045 - val_loss: 72853.9455 - val_mean_squared_error: 72853.9375 - val_mean_absolute_error: 136.4333\n",
      "Epoch 255/2000\n",
      "4288/4858 [=========================>....] - ETA: 0s - loss: 75958.6368 - mean_squared_error: 75958.6328 - mean_absolute_error: 143.5721\n",
      "Epoch 00255: val_loss did not improve from 72853.94548\n",
      "4858/4858 [==============================] - 0s 90us/sample - loss: 80035.4482 - mean_squared_error: 80035.4531 - mean_absolute_error: 144.4203 - val_loss: 72923.6579 - val_mean_squared_error: 72923.6562 - val_mean_absolute_error: 135.8738\n",
      "Epoch 256/2000\n",
      "4448/4858 [==========================>...] - ETA: 0s - loss: 79931.3341 - mean_squared_error: 79931.3594 - mean_absolute_error: 142.8101\n",
      "Epoch 00256: val_loss did not improve from 72853.94548\n",
      "4858/4858 [==============================] - 0s 83us/sample - loss: 79840.1788 - mean_squared_error: 79840.2031 - mean_absolute_error: 144.1010 - val_loss: 73968.8968 - val_mean_squared_error: 73968.8984 - val_mean_absolute_error: 148.3142\n",
      "Epoch 257/2000\n",
      "4384/4858 [==========================>...] - ETA: 0s - loss: 83109.9716 - mean_squared_error: 83109.9922 - mean_absolute_error: 145.6333\n",
      "Epoch 00257: val_loss did not improve from 72853.94548\n",
      "4858/4858 [==============================] - 0s 85us/sample - loss: 79928.9462 - mean_squared_error: 79928.9609 - mean_absolute_error: 144.8022 - val_loss: 72963.4415 - val_mean_squared_error: 72963.4375 - val_mean_absolute_error: 141.3930\n",
      "Epoch 258/2000\n",
      "4576/4858 [===========================>..] - ETA: 0s - loss: 76810.8958 - mean_squared_error: 76810.8672 - mean_absolute_error: 144.1589\n",
      "Epoch 00258: val_loss improved from 72853.94548 to 72744.85910, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 89us/sample - loss: 80002.0087 - mean_squared_error: 80001.9766 - mean_absolute_error: 145.1199 - val_loss: 72744.8591 - val_mean_squared_error: 72744.8672 - val_mean_absolute_error: 137.3686\n",
      "Epoch 259/2000\n",
      "3392/4858 [===================>..........] - ETA: 0s - loss: 76367.3856 - mean_squared_error: 76367.3906 - mean_absolute_error: 142.9387\n",
      "Epoch 00259: val_loss did not improve from 72744.85910\n",
      "4858/4858 [==============================] - 0s 54us/sample - loss: 79876.7353 - mean_squared_error: 79876.7344 - mean_absolute_error: 144.3834 - val_loss: 72808.0966 - val_mean_squared_error: 72808.1016 - val_mean_absolute_error: 139.3632\n",
      "Epoch 260/2000\n",
      "4384/4858 [==========================>...] - ETA: 0s - loss: 81647.1984 - mean_squared_error: 81647.1953 - mean_absolute_error: 145.7254\n",
      "Epoch 00260: val_loss did not improve from 72744.85910\n",
      "4858/4858 [==============================] - 0s 84us/sample - loss: 79876.0303 - mean_squared_error: 79876.0391 - mean_absolute_error: 145.1749 - val_loss: 72811.4962 - val_mean_squared_error: 72811.5000 - val_mean_absolute_error: 136.4941\n",
      "Epoch 261/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4512/4858 [==========================>...] - ETA: 0s - loss: 83756.0201 - mean_squared_error: 83756.0391 - mean_absolute_error: 147.8941\n",
      "Epoch 00261: val_loss did not improve from 72744.85910\n",
      "4858/4858 [==============================] - 0s 42us/sample - loss: 79802.3394 - mean_squared_error: 79802.3516 - mean_absolute_error: 145.0318 - val_loss: 72823.1911 - val_mean_squared_error: 72823.2031 - val_mean_absolute_error: 136.9973\n",
      "Epoch 262/2000\n",
      "4352/4858 [=========================>....] - ETA: 0s - loss: 75295.0246 - mean_squared_error: 75295.0156 - mean_absolute_error: 141.5455\n",
      "Epoch 00262: val_loss did not improve from 72744.85910\n",
      "4858/4858 [==============================] - 0s 55us/sample - loss: 79867.4866 - mean_squared_error: 79867.4844 - mean_absolute_error: 143.8691 - val_loss: 72933.5498 - val_mean_squared_error: 72933.5469 - val_mean_absolute_error: 141.1456\n",
      "Epoch 263/2000\n",
      "4128/4858 [========================>.....] - ETA: 0s - loss: 77715.7789 - mean_squared_error: 77715.7891 - mean_absolute_error: 144.8086\n",
      "Epoch 00263: val_loss did not improve from 72744.85910\n",
      "4858/4858 [==============================] - 0s 80us/sample - loss: 79841.4751 - mean_squared_error: 79841.4844 - mean_absolute_error: 145.1884 - val_loss: 72752.3283 - val_mean_squared_error: 72752.3203 - val_mean_absolute_error: 136.2691\n",
      "Epoch 264/2000\n",
      "4576/4858 [===========================>..] - ETA: 0s - loss: 82147.5267 - mean_squared_error: 82147.5156 - mean_absolute_error: 146.0065\n",
      "Epoch 00264: val_loss did not improve from 72744.85910\n",
      "4858/4858 [==============================] - 0s 75us/sample - loss: 79899.8182 - mean_squared_error: 79899.8203 - mean_absolute_error: 144.6428 - val_loss: 72760.7075 - val_mean_squared_error: 72760.6953 - val_mean_absolute_error: 135.3882\n",
      "Epoch 265/2000\n",
      "4320/4858 [=========================>....] - ETA: 0s - loss: 78776.8183 - mean_squared_error: 78776.7969 - mean_absolute_error: 142.4280\n",
      "Epoch 00265: val_loss did not improve from 72744.85910\n",
      "4858/4858 [==============================] - 0s 75us/sample - loss: 79743.6627 - mean_squared_error: 79743.6406 - mean_absolute_error: 144.2986 - val_loss: 73047.6976 - val_mean_squared_error: 73047.6953 - val_mean_absolute_error: 141.2630\n",
      "Epoch 266/2000\n",
      "4544/4858 [===========================>..] - ETA: 0s - loss: 79549.8601 - mean_squared_error: 79549.8672 - mean_absolute_error: 145.3436\n",
      "Epoch 00266: val_loss improved from 72744.85910 to 72675.59695, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 79us/sample - loss: 79776.9818 - mean_squared_error: 79776.9766 - mean_absolute_error: 145.1134 - val_loss: 72675.5970 - val_mean_squared_error: 72675.5859 - val_mean_absolute_error: 137.4069\n",
      "Epoch 267/2000\n",
      "4448/4858 [==========================>...] - ETA: 0s - loss: 78475.3345 - mean_squared_error: 78475.3438 - mean_absolute_error: 142.0103\n",
      "Epoch 00267: val_loss did not improve from 72675.59695\n",
      "4858/4858 [==============================] - 0s 73us/sample - loss: 79624.4317 - mean_squared_error: 79624.4453 - mean_absolute_error: 144.2268 - val_loss: 72707.4935 - val_mean_squared_error: 72707.4922 - val_mean_absolute_error: 140.3295\n",
      "Epoch 268/2000\n",
      "4608/4858 [===========================>..] - ETA: 0s - loss: 77309.5205 - mean_squared_error: 77309.5078 - mean_absolute_error: 143.3317\n",
      "Epoch 00268: val_loss did not improve from 72675.59695\n",
      "4858/4858 [==============================] - 0s 71us/sample - loss: 79606.3159 - mean_squared_error: 79606.3125 - mean_absolute_error: 144.7621 - val_loss: 72805.9530 - val_mean_squared_error: 72805.9453 - val_mean_absolute_error: 137.1970\n",
      "Epoch 269/2000\n",
      "4672/4858 [===========================>..] - ETA: 0s - loss: 80171.4615 - mean_squared_error: 80171.4766 - mean_absolute_error: 144.3221\n",
      "Epoch 00269: val_loss improved from 72675.59695 to 72667.95004, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 75us/sample - loss: 79606.5054 - mean_squared_error: 79606.5156 - mean_absolute_error: 144.5255 - val_loss: 72667.9500 - val_mean_squared_error: 72667.9609 - val_mean_absolute_error: 135.5748\n",
      "Epoch 270/2000\n",
      "4256/4858 [=========================>....] - ETA: 0s - loss: 79924.3990 - mean_squared_error: 79924.4062 - mean_absolute_error: 144.1984\n",
      "Epoch 00270: val_loss did not improve from 72667.95004\n",
      "4858/4858 [==============================] - 0s 80us/sample - loss: 79555.5379 - mean_squared_error: 79555.5469 - mean_absolute_error: 144.6526 - val_loss: 72979.5048 - val_mean_squared_error: 72979.5078 - val_mean_absolute_error: 141.5678\n",
      "Epoch 271/2000\n",
      "4288/4858 [=========================>....] - ETA: 0s - loss: 80502.6420 - mean_squared_error: 80502.6406 - mean_absolute_error: 144.5192\n",
      "Epoch 00271: val_loss did not improve from 72667.95004\n",
      "4858/4858 [==============================] - 0s 88us/sample - loss: 79604.2940 - mean_squared_error: 79604.3047 - mean_absolute_error: 144.4213 - val_loss: 72806.8970 - val_mean_squared_error: 72806.9062 - val_mean_absolute_error: 136.6350\n",
      "Epoch 272/2000\n",
      "4448/4858 [==========================>...] - ETA: 0s - loss: 79000.2259 - mean_squared_error: 79000.2266 - mean_absolute_error: 143.0613\n",
      "Epoch 00272: val_loss improved from 72667.95004 to 72550.98231, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 92us/sample - loss: 79550.4737 - mean_squared_error: 79550.4766 - mean_absolute_error: 143.8917 - val_loss: 72550.9823 - val_mean_squared_error: 72550.9766 - val_mean_absolute_error: 138.1704\n",
      "Epoch 273/2000\n",
      "4320/4858 [=========================>....] - ETA: 0s - loss: 77047.6160 - mean_squared_error: 77047.6328 - mean_absolute_error: 143.7347\n",
      "Epoch 00273: val_loss did not improve from 72550.98231\n",
      "4858/4858 [==============================] - 0s 44us/sample - loss: 79498.1628 - mean_squared_error: 79498.1719 - mean_absolute_error: 144.4537 - val_loss: 72617.3049 - val_mean_squared_error: 72617.3047 - val_mean_absolute_error: 140.3219\n",
      "Epoch 274/2000\n",
      "4000/4858 [=======================>......] - ETA: 0s - loss: 77210.5207 - mean_squared_error: 77210.5156 - mean_absolute_error: 142.6130\n",
      "Epoch 00274: val_loss did not improve from 72550.98231\n",
      "4858/4858 [==============================] - 0s 53us/sample - loss: 79630.2629 - mean_squared_error: 79630.2578 - mean_absolute_error: 144.4007 - val_loss: 72832.0905 - val_mean_squared_error: 72832.0859 - val_mean_absolute_error: 141.9177\n",
      "Epoch 275/2000\n",
      "4512/4858 [==========================>...] - ETA: 0s - loss: 80205.1337 - mean_squared_error: 80205.1328 - mean_absolute_error: 145.3374\n",
      "Epoch 00275: val_loss did not improve from 72550.98231\n",
      "4858/4858 [==============================] - 0s 53us/sample - loss: 79476.7732 - mean_squared_error: 79476.7734 - mean_absolute_error: 144.8631 - val_loss: 72769.3363 - val_mean_squared_error: 72769.3359 - val_mean_absolute_error: 135.1601\n",
      "Epoch 276/2000\n",
      "4768/4858 [============================>.] - ETA: 0s - loss: 80308.8792 - mean_squared_error: 80308.8828 - mean_absolute_error: 144.9922\n",
      "Epoch 00276: val_loss improved from 72550.98231 to 72537.66390, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 77us/sample - loss: 79483.9334 - mean_squared_error: 79483.9453 - mean_absolute_error: 144.4366 - val_loss: 72537.6639 - val_mean_squared_error: 72537.6641 - val_mean_absolute_error: 139.2201\n",
      "Epoch 277/2000\n",
      "4320/4858 [=========================>....] - ETA: 0s - loss: 79829.0120 - mean_squared_error: 79829.0391 - mean_absolute_error: 144.7276\n",
      "Epoch 00277: val_loss improved from 72537.66390 to 72438.69470, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 93us/sample - loss: 79402.8938 - mean_squared_error: 79402.9297 - mean_absolute_error: 143.8179 - val_loss: 72438.6947 - val_mean_squared_error: 72438.7031 - val_mean_absolute_error: 136.9612\n",
      "Epoch 278/2000\n",
      "4608/4858 [===========================>..] - ETA: 0s - loss: 81455.8916 - mean_squared_error: 81455.8750 - mean_absolute_error: 145.0619\n",
      "Epoch 00278: val_loss did not improve from 72438.69470\n",
      "4858/4858 [==============================] - 0s 67us/sample - loss: 79488.9293 - mean_squared_error: 79488.9062 - mean_absolute_error: 143.9344 - val_loss: 72759.0050 - val_mean_squared_error: 72759.0078 - val_mean_absolute_error: 142.2979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 279/2000\n",
      "4064/4858 [========================>.....] - ETA: 0s - loss: 79467.1153 - mean_squared_error: 79467.1172 - mean_absolute_error: 142.8307\n",
      "Epoch 00279: val_loss did not improve from 72438.69470\n",
      "4858/4858 [==============================] - 0s 48us/sample - loss: 79275.0584 - mean_squared_error: 79275.0547 - mean_absolute_error: 143.5811 - val_loss: 73158.3193 - val_mean_squared_error: 73158.3125 - val_mean_absolute_error: 143.7265\n",
      "Epoch 280/2000\n",
      "4256/4858 [=========================>....] - ETA: 0s - loss: 80369.1403 - mean_squared_error: 80369.1641 - mean_absolute_error: 143.8126\n",
      "Epoch 00280: val_loss improved from 72438.69470 to 72396.43558, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 70us/sample - loss: 79336.7099 - mean_squared_error: 79336.7422 - mean_absolute_error: 144.2766 - val_loss: 72396.4356 - val_mean_squared_error: 72396.4375 - val_mean_absolute_error: 137.4846\n",
      "Epoch 281/2000\n",
      "4544/4858 [===========================>..] - ETA: 0s - loss: 80241.3021 - mean_squared_error: 80241.2969 - mean_absolute_error: 144.1401\n",
      "Epoch 00281: val_loss did not improve from 72396.43558\n",
      "4858/4858 [==============================] - 0s 83us/sample - loss: 79348.0862 - mean_squared_error: 79348.0781 - mean_absolute_error: 144.4434 - val_loss: 72447.2373 - val_mean_squared_error: 72447.2266 - val_mean_absolute_error: 137.7873\n",
      "Epoch 282/2000\n",
      "3360/4858 [===================>..........] - ETA: 0s - loss: 81502.5900 - mean_squared_error: 81502.5938 - mean_absolute_error: 142.1161\n",
      "Epoch 00282: val_loss did not improve from 72396.43558\n",
      "4858/4858 [==============================] - 0s 39us/sample - loss: 79222.6368 - mean_squared_error: 79222.6328 - mean_absolute_error: 143.4023 - val_loss: 72712.1750 - val_mean_squared_error: 72712.1797 - val_mean_absolute_error: 141.9430\n",
      "Epoch 283/2000\n",
      "4736/4858 [============================>.] - ETA: 0s - loss: 79988.6271 - mean_squared_error: 79988.6484 - mean_absolute_error: 144.6728\n",
      "Epoch 00283: val_loss did not improve from 72396.43558\n",
      "4858/4858 [==============================] - 0s 41us/sample - loss: 79275.1367 - mean_squared_error: 79275.1641 - mean_absolute_error: 144.4152 - val_loss: 72636.3500 - val_mean_squared_error: 72636.3438 - val_mean_absolute_error: 140.9774\n",
      "Epoch 284/2000\n",
      "4192/4858 [========================>.....] - ETA: 0s - loss: 81073.2014 - mean_squared_error: 81073.1797 - mean_absolute_error: 143.5301\n",
      "Epoch 00284: val_loss improved from 72396.43558 to 72339.57270, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 87us/sample - loss: 79114.0453 - mean_squared_error: 79114.0234 - mean_absolute_error: 144.2910 - val_loss: 72339.5727 - val_mean_squared_error: 72339.5781 - val_mean_absolute_error: 136.3684\n",
      "Epoch 285/2000\n",
      "4224/4858 [=========================>....] - ETA: 0s - loss: 76433.5193 - mean_squared_error: 76433.5000 - mean_absolute_error: 141.6906\n",
      "Epoch 00285: val_loss did not improve from 72339.57270\n",
      "4858/4858 [==============================] - 0s 91us/sample - loss: 79246.0255 - mean_squared_error: 79246.0078 - mean_absolute_error: 142.8844 - val_loss: 72444.3822 - val_mean_squared_error: 72444.3750 - val_mean_absolute_error: 140.2479\n",
      "Epoch 286/2000\n",
      "4640/4858 [===========================>..] - ETA: 0s - loss: 79382.5589 - mean_squared_error: 79382.5469 - mean_absolute_error: 144.2847\n",
      "Epoch 00286: val_loss did not improve from 72339.57270\n",
      "4858/4858 [==============================] - 0s 85us/sample - loss: 79057.8089 - mean_squared_error: 79057.7969 - mean_absolute_error: 144.2747 - val_loss: 72866.5424 - val_mean_squared_error: 72866.5469 - val_mean_absolute_error: 133.1122\n",
      "Epoch 287/2000\n",
      "4640/4858 [===========================>..] - ETA: 0s - loss: 79792.7125 - mean_squared_error: 79792.7188 - mean_absolute_error: 144.4391\n",
      "Epoch 00287: val_loss did not improve from 72339.57270\n",
      "4858/4858 [==============================] - 0s 82us/sample - loss: 78964.6499 - mean_squared_error: 78964.6484 - mean_absolute_error: 143.7536 - val_loss: 72846.2085 - val_mean_squared_error: 72846.2109 - val_mean_absolute_error: 133.0591\n",
      "Epoch 288/2000\n",
      "4448/4858 [==========================>...] - ETA: 0s - loss: 81898.8081 - mean_squared_error: 81898.8125 - mean_absolute_error: 144.8550\n",
      "Epoch 00288: val_loss improved from 72339.57270 to 72280.23666, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 80us/sample - loss: 79199.5541 - mean_squared_error: 79199.5625 - mean_absolute_error: 143.6868 - val_loss: 72280.2367 - val_mean_squared_error: 72280.2344 - val_mean_absolute_error: 136.9004\n",
      "Epoch 289/2000\n",
      "4352/4858 [=========================>....] - ETA: 0s - loss: 78651.8255 - mean_squared_error: 78651.8281 - mean_absolute_error: 143.9450\n",
      "Epoch 00289: val_loss improved from 72280.23666 to 72244.73387, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 80us/sample - loss: 79161.3242 - mean_squared_error: 79161.3438 - mean_absolute_error: 143.8894 - val_loss: 72244.7339 - val_mean_squared_error: 72244.7344 - val_mean_absolute_error: 137.4627\n",
      "Epoch 290/2000\n",
      "4448/4858 [==========================>...] - ETA: 0s - loss: 80413.3570 - mean_squared_error: 80413.3438 - mean_absolute_error: 143.5390\n",
      "Epoch 00290: val_loss did not improve from 72244.73387\n",
      "4858/4858 [==============================] - 0s 72us/sample - loss: 79008.6027 - mean_squared_error: 79008.5859 - mean_absolute_error: 143.3215 - val_loss: 72318.8150 - val_mean_squared_error: 72318.8203 - val_mean_absolute_error: 140.4525\n",
      "Epoch 291/2000\n",
      "4480/4858 [==========================>...] - ETA: 0s - loss: 77926.9223 - mean_squared_error: 77926.9062 - mean_absolute_error: 142.9708\n",
      "Epoch 00291: val_loss did not improve from 72244.73387\n",
      "4858/4858 [==============================] - 0s 73us/sample - loss: 78998.7047 - mean_squared_error: 78998.6953 - mean_absolute_error: 143.6071 - val_loss: 72526.2328 - val_mean_squared_error: 72526.2344 - val_mean_absolute_error: 141.1559\n",
      "Epoch 292/2000\n",
      "4448/4858 [==========================>...] - ETA: 0s - loss: 80649.0769 - mean_squared_error: 80649.0547 - mean_absolute_error: 145.4018\n",
      "Epoch 00292: val_loss did not improve from 72244.73387\n",
      "4858/4858 [==============================] - 0s 73us/sample - loss: 79084.1451 - mean_squared_error: 79084.1328 - mean_absolute_error: 143.8340 - val_loss: 72300.3510 - val_mean_squared_error: 72300.3438 - val_mean_absolute_error: 135.2275\n",
      "Epoch 293/2000\n",
      "4512/4858 [==========================>...] - ETA: 0s - loss: 80488.5125 - mean_squared_error: 80488.5234 - mean_absolute_error: 144.5492\n",
      "Epoch 00293: val_loss did not improve from 72244.73387\n",
      "4858/4858 [==============================] - 0s 72us/sample - loss: 78844.5817 - mean_squared_error: 78844.5859 - mean_absolute_error: 143.4992 - val_loss: 72390.3447 - val_mean_squared_error: 72390.3438 - val_mean_absolute_error: 140.1026\n",
      "Epoch 294/2000\n",
      "4480/4858 [==========================>...] - ETA: 0s - loss: 79539.6304 - mean_squared_error: 79539.6406 - mean_absolute_error: 143.9063\n",
      "Epoch 00294: val_loss improved from 72244.73387 to 72202.46209, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 79us/sample - loss: 78915.1389 - mean_squared_error: 78915.1484 - mean_absolute_error: 143.0938 - val_loss: 72202.4621 - val_mean_squared_error: 72202.4609 - val_mean_absolute_error: 136.8105\n",
      "Epoch 295/2000\n",
      "4416/4858 [==========================>...] - ETA: 0s - loss: 79830.8136 - mean_squared_error: 79830.8359 - mean_absolute_error: 143.8480\n",
      "Epoch 00295: val_loss did not improve from 72202.46209\n",
      "4858/4858 [==============================] - 0s 74us/sample - loss: 78874.6345 - mean_squared_error: 78874.6484 - mean_absolute_error: 143.3842 - val_loss: 72293.7957 - val_mean_squared_error: 72293.7969 - val_mean_absolute_error: 139.4491\n",
      "Epoch 296/2000\n",
      "4512/4858 [==========================>...] - ETA: 0s - loss: 76762.8253 - mean_squared_error: 76762.8281 - mean_absolute_error: 140.6945\n",
      "Epoch 00296: val_loss did not improve from 72202.46209\n",
      "4858/4858 [==============================] - 0s 72us/sample - loss: 78835.2307 - mean_squared_error: 78835.2266 - mean_absolute_error: 142.7504 - val_loss: 72903.0599 - val_mean_squared_error: 72903.0703 - val_mean_absolute_error: 143.8074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297/2000\n",
      "4480/4858 [==========================>...] - ETA: 0s - loss: 80011.1250 - mean_squared_error: 80011.1250 - mean_absolute_error: 144.7979\n",
      "Epoch 00297: val_loss did not improve from 72202.46209\n",
      "4858/4858 [==============================] - 0s 73us/sample - loss: 79026.4768 - mean_squared_error: 79026.4922 - mean_absolute_error: 143.6576 - val_loss: 72207.2594 - val_mean_squared_error: 72207.2578 - val_mean_absolute_error: 137.6226\n",
      "Epoch 298/2000\n",
      "3808/4858 [======================>.......] - ETA: 0s - loss: 77065.2974 - mean_squared_error: 77065.2969 - mean_absolute_error: 143.6055\n",
      "Epoch 00298: val_loss improved from 72202.46209 to 72118.96077, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 54us/sample - loss: 78807.6421 - mean_squared_error: 78807.6328 - mean_absolute_error: 143.2914 - val_loss: 72118.9608 - val_mean_squared_error: 72118.9609 - val_mean_absolute_error: 138.1645\n",
      "Epoch 299/2000\n",
      "4416/4858 [==========================>...] - ETA: 0s - loss: 77144.0725 - mean_squared_error: 77144.0547 - mean_absolute_error: 144.7421\n",
      "Epoch 00299: val_loss did not improve from 72118.96077\n",
      "4858/4858 [==============================] - 0s 73us/sample - loss: 78903.4394 - mean_squared_error: 78903.4297 - mean_absolute_error: 143.8431 - val_loss: 72178.4507 - val_mean_squared_error: 72178.4609 - val_mean_absolute_error: 134.4345\n",
      "Epoch 300/2000\n",
      "3552/4858 [====================>.........] - ETA: 0s - loss: 78310.6410 - mean_squared_error: 78310.6250 - mean_absolute_error: 143.0564\n",
      "Epoch 00300: val_loss improved from 72118.96077 to 72101.00292, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 62us/sample - loss: 78903.3304 - mean_squared_error: 78903.3281 - mean_absolute_error: 142.9051 - val_loss: 72101.0029 - val_mean_squared_error: 72101.0078 - val_mean_absolute_error: 137.6317\n",
      "Epoch 301/2000\n",
      "4128/4858 [========================>.....] - ETA: 0s - loss: 77206.2708 - mean_squared_error: 77206.2578 - mean_absolute_error: 140.7144\n",
      "Epoch 00301: val_loss did not improve from 72101.00292\n",
      "4858/4858 [==============================] - 0s 66us/sample - loss: 78550.6170 - mean_squared_error: 78550.5938 - mean_absolute_error: 142.6331 - val_loss: 72356.2701 - val_mean_squared_error: 72356.2578 - val_mean_absolute_error: 141.4677\n",
      "Epoch 302/2000\n",
      "4448/4858 [==========================>...] - ETA: 0s - loss: 79175.2571 - mean_squared_error: 79175.2578 - mean_absolute_error: 143.4002\n",
      "Epoch 00302: val_loss improved from 72101.00292 to 72018.65504, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 79us/sample - loss: 78703.3547 - mean_squared_error: 78703.3516 - mean_absolute_error: 143.5700 - val_loss: 72018.6550 - val_mean_squared_error: 72018.6562 - val_mean_absolute_error: 136.5719\n",
      "Epoch 303/2000\n",
      "4384/4858 [==========================>...] - ETA: 0s - loss: 82424.0222 - mean_squared_error: 82424.0156 - mean_absolute_error: 144.2996\n",
      "Epoch 00303: val_loss did not improve from 72018.65504\n",
      "4858/4858 [==============================] - 0s 72us/sample - loss: 78682.8390 - mean_squared_error: 78682.8203 - mean_absolute_error: 142.6572 - val_loss: 72302.7188 - val_mean_squared_error: 72302.7266 - val_mean_absolute_error: 140.5753\n",
      "Epoch 304/2000\n",
      "4768/4858 [============================>.] - ETA: 0s - loss: 79153.4376 - mean_squared_error: 79153.4219 - mean_absolute_error: 143.6608\n",
      "Epoch 00304: val_loss did not improve from 72018.65504\n",
      "4858/4858 [==============================] - 0s 68us/sample - loss: 78612.4490 - mean_squared_error: 78612.4375 - mean_absolute_error: 143.5726 - val_loss: 72055.1933 - val_mean_squared_error: 72055.1953 - val_mean_absolute_error: 134.8206\n",
      "Epoch 305/2000\n",
      "4576/4858 [===========================>..] - ETA: 0s - loss: 80612.3673 - mean_squared_error: 80612.3906 - mean_absolute_error: 144.6419\n",
      "Epoch 00305: val_loss did not improve from 72018.65504\n",
      "4858/4858 [==============================] - 0s 72us/sample - loss: 78766.2675 - mean_squared_error: 78766.2969 - mean_absolute_error: 143.3357 - val_loss: 72947.0655 - val_mean_squared_error: 72947.0781 - val_mean_absolute_error: 131.0812\n",
      "Epoch 306/2000\n",
      "4512/4858 [==========================>...] - ETA: 0s - loss: 77286.4489 - mean_squared_error: 77286.4219 - mean_absolute_error: 140.7856\n",
      "Epoch 00306: val_loss did not improve from 72018.65504\n",
      "4858/4858 [==============================] - 0s 69us/sample - loss: 78642.2932 - mean_squared_error: 78642.2734 - mean_absolute_error: 141.6671 - val_loss: 72453.3729 - val_mean_squared_error: 72453.3828 - val_mean_absolute_error: 142.0068\n",
      "Epoch 307/2000\n",
      "4608/4858 [===========================>..] - ETA: 0s - loss: 80483.9656 - mean_squared_error: 80483.9609 - mean_absolute_error: 144.2535\n",
      "Epoch 00307: val_loss did not improve from 72018.65504\n",
      "4858/4858 [==============================] - 0s 82us/sample - loss: 78679.4567 - mean_squared_error: 78679.4531 - mean_absolute_error: 143.3207 - val_loss: 72161.8460 - val_mean_squared_error: 72161.8359 - val_mean_absolute_error: 139.9433\n",
      "Epoch 308/2000\n",
      "4800/4858 [============================>.] - ETA: 0s - loss: 77848.8253 - mean_squared_error: 77848.8438 - mean_absolute_error: 142.4738\n",
      "Epoch 00308: val_loss improved from 72018.65504 to 71907.58740, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 75us/sample - loss: 78399.9807 - mean_squared_error: 78400.0078 - mean_absolute_error: 142.7363 - val_loss: 71907.5874 - val_mean_squared_error: 71907.5781 - val_mean_absolute_error: 136.3367\n",
      "Epoch 309/2000\n",
      "4576/4858 [===========================>..] - ETA: 0s - loss: 76792.8342 - mean_squared_error: 76792.8125 - mean_absolute_error: 141.2782\n",
      "Epoch 00309: val_loss did not improve from 71907.58740\n",
      "4858/4858 [==============================] - 0s 74us/sample - loss: 78515.6423 - mean_squared_error: 78515.6250 - mean_absolute_error: 142.2050 - val_loss: 72145.3561 - val_mean_squared_error: 72145.3594 - val_mean_absolute_error: 140.9371\n",
      "Epoch 310/2000\n",
      "4736/4858 [============================>.] - ETA: 0s - loss: 77771.2291 - mean_squared_error: 77771.2266 - mean_absolute_error: 143.8683\n",
      "Epoch 00310: val_loss did not improve from 71907.58740\n",
      "4858/4858 [==============================] - 0s 80us/sample - loss: 78494.5868 - mean_squared_error: 78494.5859 - mean_absolute_error: 143.3345 - val_loss: 71938.0318 - val_mean_squared_error: 71938.0312 - val_mean_absolute_error: 135.8718\n",
      "Epoch 311/2000\n",
      "4256/4858 [=========================>....] - ETA: 0s - loss: 79581.4880 - mean_squared_error: 79581.5000 - mean_absolute_error: 141.5698\n",
      "Epoch 00311: val_loss did not improve from 71907.58740\n",
      "4858/4858 [==============================] - 0s 71us/sample - loss: 78477.4704 - mean_squared_error: 78477.5000 - mean_absolute_error: 142.4609 - val_loss: 72036.9107 - val_mean_squared_error: 72036.9219 - val_mean_absolute_error: 138.7792\n",
      "Epoch 312/2000\n",
      "4576/4858 [===========================>..] - ETA: 0s - loss: 78833.4444 - mean_squared_error: 78833.4375 - mean_absolute_error: 142.9128\n",
      "Epoch 00312: val_loss improved from 71907.58740 to 71899.88914, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 78us/sample - loss: 78509.6734 - mean_squared_error: 78509.6641 - mean_absolute_error: 143.0681 - val_loss: 71899.8891 - val_mean_squared_error: 71899.8828 - val_mean_absolute_error: 134.7701\n",
      "Epoch 313/2000\n",
      "4256/4858 [=========================>....] - ETA: 0s - loss: 78618.7934 - mean_squared_error: 78618.7891 - mean_absolute_error: 142.1152\n",
      "Epoch 00313: val_loss did not improve from 71899.88914\n",
      "4858/4858 [==============================] - 0s 70us/sample - loss: 78388.0268 - mean_squared_error: 78388.0078 - mean_absolute_error: 142.1663 - val_loss: 72146.1735 - val_mean_squared_error: 72146.1719 - val_mean_absolute_error: 139.9352\n",
      "Epoch 314/2000\n",
      "4608/4858 [===========================>..] - ETA: 0s - loss: 77909.5110 - mean_squared_error: 77909.5391 - mean_absolute_error: 141.3497\n",
      "Epoch 00314: val_loss did not improve from 71899.88914\n",
      "4858/4858 [==============================] - 0s 55us/sample - loss: 78367.9827 - mean_squared_error: 78368.0156 - mean_absolute_error: 142.5190 - val_loss: 72037.5174 - val_mean_squared_error: 72037.5312 - val_mean_absolute_error: 139.2731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 315/2000\n",
      "4384/4858 [==========================>...] - ETA: 0s - loss: 75280.7061 - mean_squared_error: 75280.7031 - mean_absolute_error: 142.3952\n",
      "Epoch 00315: val_loss improved from 71899.88914 to 71834.87572, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 63us/sample - loss: 78248.1905 - mean_squared_error: 78248.1797 - mean_absolute_error: 143.0312 - val_loss: 71834.8757 - val_mean_squared_error: 71834.8750 - val_mean_absolute_error: 135.5228\n",
      "Epoch 316/2000\n",
      "4672/4858 [===========================>..] - ETA: 0s - loss: 78125.0654 - mean_squared_error: 78125.0703 - mean_absolute_error: 141.9472\n",
      "Epoch 00316: val_loss did not improve from 71834.87572\n",
      "4858/4858 [==============================] - 0s 42us/sample - loss: 78408.8208 - mean_squared_error: 78408.8203 - mean_absolute_error: 142.3495 - val_loss: 72017.5933 - val_mean_squared_error: 72017.6016 - val_mean_absolute_error: 139.7223\n",
      "Epoch 317/2000\n",
      "4448/4858 [==========================>...] - ETA: 0s - loss: 74805.5115 - mean_squared_error: 74805.5078 - mean_absolute_error: 141.6657\n",
      "Epoch 00317: val_loss did not improve from 71834.87572\n",
      "4858/4858 [==============================] - 0s 45us/sample - loss: 78416.2886 - mean_squared_error: 78416.2891 - mean_absolute_error: 142.3823 - val_loss: 71866.0686 - val_mean_squared_error: 71866.0625 - val_mean_absolute_error: 137.0552\n",
      "Epoch 318/2000\n",
      "4160/4858 [========================>.....] - ETA: 0s - loss: 76137.7227 - mean_squared_error: 76137.7031 - mean_absolute_error: 141.2925\n",
      "Epoch 00318: val_loss did not improve from 71834.87572\n",
      "4858/4858 [==============================] - 0s 46us/sample - loss: 78313.3400 - mean_squared_error: 78313.3125 - mean_absolute_error: 142.9160 - val_loss: 71899.0868 - val_mean_squared_error: 71899.0859 - val_mean_absolute_error: 137.3460\n",
      "Epoch 319/2000\n",
      "4000/4858 [=======================>......] - ETA: 0s - loss: 61190.4283 - mean_squared_error: 61190.4375 - mean_absolute_error: 135.4565\n",
      "Epoch 00319: val_loss did not improve from 71834.87572\n",
      "4858/4858 [==============================] - 0s 59us/sample - loss: 78405.6760 - mean_squared_error: 78405.6875 - mean_absolute_error: 142.0663 - val_loss: 71942.7894 - val_mean_squared_error: 71942.7891 - val_mean_absolute_error: 140.4464\n",
      "Epoch 320/2000\n",
      "4256/4858 [=========================>....] - ETA: 0s - loss: 80757.1624 - mean_squared_error: 80757.1562 - mean_absolute_error: 143.2700\n",
      "Epoch 00320: val_loss did not improve from 71834.87572\n",
      "4858/4858 [==============================] - 0s 53us/sample - loss: 78250.7152 - mean_squared_error: 78250.7109 - mean_absolute_error: 142.6323 - val_loss: 72088.0527 - val_mean_squared_error: 72088.0547 - val_mean_absolute_error: 132.4141\n",
      "Epoch 321/2000\n",
      "4768/4858 [============================>.] - ETA: 0s - loss: 78545.0890 - mean_squared_error: 78545.0781 - mean_absolute_error: 142.1718\n",
      "Epoch 00321: val_loss improved from 71834.87572 to 71786.29117, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 56us/sample - loss: 78325.6521 - mean_squared_error: 78325.6484 - mean_absolute_error: 142.0419 - val_loss: 71786.2912 - val_mean_squared_error: 71786.2969 - val_mean_absolute_error: 135.0780\n",
      "Epoch 322/2000\n",
      "4640/4858 [===========================>..] - ETA: 0s - loss: 78298.9155 - mean_squared_error: 78298.9062 - mean_absolute_error: 143.1250\n",
      "Epoch 00322: val_loss did not improve from 71786.29117\n",
      "4858/4858 [==============================] - 0s 42us/sample - loss: 78148.1877 - mean_squared_error: 78148.1797 - mean_absolute_error: 142.4041 - val_loss: 71965.6451 - val_mean_squared_error: 71965.6484 - val_mean_absolute_error: 132.6121\n",
      "Epoch 323/2000\n",
      "4320/4858 [=========================>....] - ETA: 0s - loss: 81071.3427 - mean_squared_error: 81071.3281 - mean_absolute_error: 143.3497\n",
      "Epoch 00323: val_loss did not improve from 71786.29117\n",
      "4858/4858 [==============================] - 0s 45us/sample - loss: 78120.4477 - mean_squared_error: 78120.4297 - mean_absolute_error: 142.3213 - val_loss: 71872.7131 - val_mean_squared_error: 71872.7109 - val_mean_absolute_error: 134.8340\n",
      "Epoch 324/2000\n",
      "4640/4858 [===========================>..] - ETA: 0s - loss: 79751.8231 - mean_squared_error: 79751.8203 - mean_absolute_error: 143.0769\n",
      "Epoch 00324: val_loss did not improve from 71786.29117\n",
      "4858/4858 [==============================] - 0s 41us/sample - loss: 78147.9043 - mean_squared_error: 78147.8906 - mean_absolute_error: 142.1273 - val_loss: 71911.2283 - val_mean_squared_error: 71911.2266 - val_mean_absolute_error: 133.2764\n",
      "Epoch 325/2000\n",
      "4448/4858 [==========================>...] - ETA: 0s - loss: 76561.8556 - mean_squared_error: 76561.8594 - mean_absolute_error: 140.9610\n",
      "Epoch 00325: val_loss did not improve from 71786.29117\n",
      "4858/4858 [==============================] - 0s 53us/sample - loss: 78060.4021 - mean_squared_error: 78060.3984 - mean_absolute_error: 141.7208 - val_loss: 72006.4532 - val_mean_squared_error: 72006.4375 - val_mean_absolute_error: 139.3969\n",
      "Epoch 326/2000\n",
      "4192/4858 [========================>.....] - ETA: 0s - loss: 78672.4760 - mean_squared_error: 78672.4688 - mean_absolute_error: 143.5583\n",
      "Epoch 00326: val_loss improved from 71786.29117 to 71680.34495, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 70us/sample - loss: 78066.4026 - mean_squared_error: 78066.3984 - mean_absolute_error: 142.5749 - val_loss: 71680.3450 - val_mean_squared_error: 71680.3594 - val_mean_absolute_error: 135.5649\n",
      "Epoch 327/2000\n",
      "4256/4858 [=========================>....] - ETA: 0s - loss: 70742.0624 - mean_squared_error: 70742.0625 - mean_absolute_error: 139.3630\n",
      "Epoch 00327: val_loss did not improve from 71680.34495\n",
      "4858/4858 [==============================] - 0s 75us/sample - loss: 78007.0782 - mean_squared_error: 78007.0859 - mean_absolute_error: 141.0693 - val_loss: 71729.2038 - val_mean_squared_error: 71729.2109 - val_mean_absolute_error: 136.9480\n",
      "Epoch 328/2000\n",
      "4192/4858 [========================>.....] - ETA: 0s - loss: 76801.5186 - mean_squared_error: 76801.5078 - mean_absolute_error: 142.5677\n",
      "Epoch 00328: val_loss did not improve from 71680.34495\n",
      "4858/4858 [==============================] - 0s 79us/sample - loss: 78137.6743 - mean_squared_error: 78137.6719 - mean_absolute_error: 142.3407 - val_loss: 71940.4227 - val_mean_squared_error: 71940.4375 - val_mean_absolute_error: 140.3205\n",
      "Epoch 329/2000\n",
      "3488/4858 [====================>.........] - ETA: 0s - loss: 71169.4845 - mean_squared_error: 71169.5078 - mean_absolute_error: 141.1916\n",
      "Epoch 00329: val_loss improved from 71680.34495 to 71612.12887, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 55us/sample - loss: 78050.3466 - mean_squared_error: 78050.3750 - mean_absolute_error: 142.0491 - val_loss: 71612.1289 - val_mean_squared_error: 71612.1328 - val_mean_absolute_error: 135.9904\n",
      "Epoch 330/2000\n",
      "3328/4858 [===================>..........] - ETA: 0s - loss: 68138.2141 - mean_squared_error: 68138.2266 - mean_absolute_error: 138.1331\n",
      "Epoch 00330: val_loss improved from 71612.12887 to 71488.62831, saving model to ./model_r_weights.h5\n",
      "4858/4858 [==============================] - 0s 62us/sample - loss: 78028.7259 - mean_squared_error: 78028.7422 - mean_absolute_error: 141.8349 - val_loss: 71488.6283 - val_mean_squared_error: 71488.6328 - val_mean_absolute_error: 135.1918\n",
      "Epoch 331/2000\n",
      "4000/4858 [=======================>......] - ETA: 0s - loss: 79553.7414 - mean_squared_error: 79553.7422 - mean_absolute_error: 143.6395\n",
      "Epoch 00331: val_loss did not improve from 71488.62831\n",
      "4858/4858 [==============================] - 0s 77us/sample - loss: 77908.1421 - mean_squared_error: 77908.1484 - mean_absolute_error: 141.7171 - val_loss: 71656.1652 - val_mean_squared_error: 71656.1641 - val_mean_absolute_error: 134.6776\n",
      "Epoch 332/2000\n",
      "4736/4858 [============================>.] - ETA: 0s - loss: 79195.6312 - mean_squared_error: 79195.6250 - mean_absolute_error: 143.1077\n",
      "Epoch 00332: val_loss did not improve from 71488.62831\n",
      "4858/4858 [==============================] - 0s 71us/sample - loss: 77977.0778 - mean_squared_error: 77977.0781 - mean_absolute_error: 142.2184 - val_loss: 71559.6939 - val_mean_squared_error: 71559.6875 - val_mean_absolute_error: 135.0157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 333/2000\n",
      "4672/4858 [===========================>..] - ETA: 0s - loss: 78983.0064 - mean_squared_error: 78982.9922 - mean_absolute_error: 142.2566\n",
      "Epoch 00333: val_loss did not improve from 71488.62831\n",
      "4858/4858 [==============================] - 0s 81us/sample - loss: 77828.9951 - mean_squared_error: 77828.9766 - mean_absolute_error: 141.6152 - val_loss: 71597.0320 - val_mean_squared_error: 71597.0234 - val_mean_absolute_error: 134.5893\n",
      "Epoch 334/2000\n",
      "4416/4858 [==========================>...] - ETA: 0s - loss: 80375.9496 - mean_squared_error: 80375.9453 - mean_absolute_error: 143.2997\n",
      "Epoch 00334: val_loss did not improve from 71488.62831\n",
      "4858/4858 [==============================] - 0s 73us/sample - loss: 77910.0418 - mean_squared_error: 77910.0469 - mean_absolute_error: 141.4719 - val_loss: 71495.4056 - val_mean_squared_error: 71495.3984 - val_mean_absolute_error: 134.4475\n",
      "Epoch 335/2000\n",
      "4288/4858 [=========================>....] - ETA: 0s - loss: 70904.8582 - mean_squared_error: 70904.8516 - mean_absolute_error: 138.6227\n",
      "Epoch 00335: val_loss did not improve from 71488.62831\n",
      "4858/4858 [==============================] - 0s 74us/sample - loss: 77886.1194 - mean_squared_error: 77886.1094 - mean_absolute_error: 141.2581 - val_loss: 71545.8998 - val_mean_squared_error: 71545.8984 - val_mean_absolute_error: 136.5047\n",
      "Epoch 336/2000\n",
      "4192/4858 [========================>.....] - ETA: 0s - loss: 80036.0927 - mean_squared_error: 80036.0938 - mean_absolute_error: 143.4983\n",
      "Epoch 00336: val_loss did not improve from 71488.62831\n",
      "4858/4858 [==============================] - 0s 77us/sample - loss: 77811.9959 - mean_squared_error: 77812.0000 - mean_absolute_error: 142.2267 - val_loss: 71542.7745 - val_mean_squared_error: 71542.7656 - val_mean_absolute_error: 133.3411\n",
      "Epoch 337/2000\n",
      "4416/4858 [==========================>...] - ETA: 0s - loss: 77869.4374 - mean_squared_error: 77869.4453 - mean_absolute_error: 140.2126\n",
      "Epoch 00337: val_loss did not improve from 71488.62831\n",
      "4858/4858 [==============================] - 0s 74us/sample - loss: 78113.4942 - mean_squared_error: 78113.5000 - mean_absolute_error: 141.0051 - val_loss: 72010.4016 - val_mean_squared_error: 72010.3906 - val_mean_absolute_error: 140.2753\n",
      "Epoch 338/2000\n",
      "4448/4858 [==========================>...] - ETA: 0s - loss: 73705.4323 - mean_squared_error: 73705.4219 - mean_absolute_error: 141.1546\n",
      "Epoch 00338: val_loss did not improve from 71488.62831\n",
      "4858/4858 [==============================] - 0s 72us/sample - loss: 77952.4058 - mean_squared_error: 77952.3906 - mean_absolute_error: 142.2050 - val_loss: 71598.8744 - val_mean_squared_error: 71598.8594 - val_mean_absolute_error: 136.4328\n",
      "Epoch 339/2000\n",
      "4544/4858 [===========================>..] - ETA: 0s - loss: 77837.2595 - mean_squared_error: 77837.2656 - mean_absolute_error: 141.5150\n",
      "Epoch 00339: val_loss did not improve from 71488.62831\n",
      "4858/4858 [==============================] - 0s 65us/sample - loss: 77940.5074 - mean_squared_error: 77940.5156 - mean_absolute_error: 142.0067 - val_loss: 71506.8840 - val_mean_squared_error: 71506.8750 - val_mean_absolute_error: 135.6374\n",
      "Epoch 340/2000\n",
      "4512/4858 [==========================>...] - ETA: 0s - loss: 76742.7000 - mean_squared_error: 76742.6719 - mean_absolute_error: 140.3745\n",
      "Epoch 00340: val_loss did not improve from 71488.62831\n",
      "4858/4858 [==============================] - 0s 72us/sample - loss: 77833.1136 - mean_squared_error: 77833.0781 - mean_absolute_error: 141.5424 - val_loss: 71517.5487 - val_mean_squared_error: 71517.5469 - val_mean_absolute_error: 135.8038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23bf1486b38>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=trainingData_features,\n",
    "          y=trainingData_label,\n",
    "          batch_size=32,\n",
    "          epochs=2000,\n",
    "          validation_split=0.2,\n",
    "          callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 모델 추론(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1016 15:54:04.194237  8284 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1519/1519 [==============================] - 0s 33us/sample - loss: 60207.4531 - mean_squared_error: 60207.4531 - mean_absolute_error: 134.8324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[60207.45310185566, 60207.453, 134.83241]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=testData_features, y=testData_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1016 15:54:04.267644  8284 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1519/1519 [==============================] - 0s 41us/sample - loss: 60207.4531 - mean_squared_error: 60207.4531 - mean_absolute_error: 134.8324\n"
     ]
    }
   ],
   "source": [
    "lose, mse, mae = model.evaluate(x=testData_features,\n",
    "              y=testData_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1  2  3  4   5\n",
       "0  3  0.6  1  1  3  20"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LE_HOLI', 'LE_PROD_GROUP', 'LE_PROD'\n",
    "HCLUS=3\n",
    "PRO_PERCENT=0.6\n",
    "LE_HOLI=1\n",
    "LE_PROD_GROUP=1\n",
    "LE_PROD=3\n",
    "LE_ITEM=20\n",
    "sampledata = pd.DataFrame([[HCLUS,PRO_PERCENT,LE_HOLI,LE_PROD_GROUP,LE_PROD,LE_ITEM]])\n",
    "sampledata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1016 15:55:03.560076  8284 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[683.62067]], dtype=float32)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(sampledata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
